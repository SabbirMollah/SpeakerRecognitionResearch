{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TrainRawNet2: On BD ASR Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Necessary Imports:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\r\n",
    "from torch.utils import data\r\n",
    "\r\n",
    "import torchaudio\r\n",
    "\r\n",
    "import csv\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "import glob\r\n",
    "from pathlib import Path"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Directories and lcoations:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Directories are assumed to have a trailing '/' or '\\\\' in all the subsequent code\r\n",
    "\r\n",
    "CURRENT_WORKING_DIRECTORY = \"W:/SpeakerRecognitionResearch\"\r\n",
    "\r\n",
    "BANGLA_ASR_DATASET_DIRECTORY = \"data/BanglaASR/WavFiles/\"\r\n",
    "BANGLA_ASR_TSV_LOCATION = \"data/BanglaASR/utt_spk_text.tsv\"\r\n",
    "\r\n",
    "# To avoid file location related errors, we make sure \"SpeakerRecognitionResearch\" root folder is the current working directory.\r\n",
    "os.chdir(CURRENT_WORKING_DIRECTORY)\r\n",
    "os.getcwd()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'W:\\\\SpeakerRecognitionResearch'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Constants:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# If sample_rate = 16K and number_of_samples = 32000, then each tensor will be equivalent to 2 seconds of data\r\n",
    "SAMPLE_RATE = 16000\r\n",
    "NUMBER_OF_SAMPLES = 32000\r\n",
    "\r\n",
    "# Bangla ASR Dataset has around half of second of silence in the beginning\r\n",
    "# This constant will be used to cut samples from the left of the audio\r\n",
    "TRIM_AMOUNT_TIME = 0.5"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Device\r\n",
    "\r\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n",
    "print(\"Using {}.\".format(device))\r\n",
    "if device==\"cuda\": print(torch.cuda.get_device_name(0))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cuda.\n",
      "NVIDIA GeForce GTX 1050\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Custom dataset for Bangla ASR\r\n",
    "\r\n",
    "This custom dataset is written with the assumption that the Dataset has been already converted into wav format. Check evaluate_asr_ds.ipynb notebook for conversion method.\r\n",
    "\r\n",
    "A stem is the name of the file. I.E: \"a/b/c/bat.jpg\" --stem--> bat"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class BanglaAsrDataset(data.Dataset):\r\n",
    "    def __init__(self, dataset_dir, wav_stem_list, tsv_loc, target_sample_rate, target_num_samples, trim_amount_time, device):\r\n",
    "\r\n",
    "        # wav_stem_list will have a list of stems that will be included in this dataset\r\n",
    "\r\n",
    "        tsv_dataframe = pd.read_csv(tsv_loc, quoting=csv.QUOTE_NONE, sep='\\t', header=None)\r\n",
    "\r\n",
    "        # The TSV file contains speech annotations in the third column.\r\n",
    "        # We don't need the annotations, so we drop the column\r\n",
    "        tsv_dataframe = tsv_dataframe.iloc[:,:-1]\r\n",
    "\r\n",
    "        self.dataset_dir = dataset_dir\r\n",
    "        self.stem_to_spk_mapping = dict(sorted(tsv_dataframe.values.tolist()))\r\n",
    "        self.wav_path_list = self._stem_list_to_path_list(wav_stem_list)\r\n",
    "        self.target_sample_rate = target_sample_rate\r\n",
    "        self.target_num_samples = target_num_samples\r\n",
    "        self.trim_amount_time = trim_amount_time\r\n",
    "        self.device = device\r\n",
    "\r\n",
    "        # Set will ensure uniquness\r\n",
    "        # sorted list will make the order consistent\r\n",
    "        # Tuple will make sure the order doesn't change\r\n",
    "        self.speakers_list = tuple(sorted(list(set([self.stem_to_spk_mapping[stem] for stem in wav_stem_list]))))\r\n",
    "        \r\n",
    "    def _get_audio_path_list(self, dataset_dir):\r\n",
    "\r\n",
    "        pattern = '**/*.wav'\r\n",
    "        files = glob.glob(self.dataset_dir + pattern , recursive=True)\r\n",
    "\r\n",
    "        # Normalize the file paths. To get file paths with '/' or '\\\\' consistently depending on OS\r\n",
    "        wav_list = [os.path.normpath(i) for i in files]\r\n",
    "\r\n",
    "        return wav_list\r\n",
    "\r\n",
    "    def _stem_list_to_path_list(self, stem_list):\r\n",
    "        # Sets are faster to search\r\n",
    "        stem_list = set(stem_list)\r\n",
    "        all_path_list = self._get_audio_path_list(self.dataset_dir)\r\n",
    "        path_list = [path for path in all_path_list if Path(path).stem in stem_list]\r\n",
    "        return path_list\r\n",
    "\r\n",
    "\r\n",
    "    def _resample_to_target_sr(self, signal, sample_rate):\r\n",
    "        if sample_rate != self.target_sample_rate:\r\n",
    "            resampler = torchaudio.transforms.Resample(sample_rate, self.target_sample_rate)\r\n",
    "            signal = resampler(signal)\r\n",
    "        return signal\r\n",
    "\r\n",
    "    def _mix_down_to_mono(self, signal):\r\n",
    "        if signal.shape[0] > 1:\r\n",
    "            signal = torch.mean(siggnal, dim=0, keepdim=True)\r\n",
    "        return signal\r\n",
    "\r\n",
    "    def _trim(self, signal):\r\n",
    "        total_samples = signal.shape[-1]\r\n",
    "\r\n",
    "        # We cut a fixed amount on the left side if the signal is big enough\r\n",
    "        trim_samples_amount = int(self.target_sample_rate * self.trim_amount_time)\r\n",
    "\r\n",
    "        if total_samples >= trim_samples_amount + self.target_num_samples:\r\n",
    "            signal = signal[: , trim_samples_amount:]\r\n",
    "            total_samples = signal.shape[-1]\r\n",
    "\r\n",
    "        # We cut from the right side if the signal is too big\r\n",
    "        if total_samples > self.target_num_samples:\r\n",
    "            signal = signal[:, :self.target_num_samples]\r\n",
    "        \r\n",
    "        # We add zero padding on the right if signal is too small\r\n",
    "        if total_samples < self.target_num_samples:\r\n",
    "            num_missing_samples = self.target_num_samples - total_samples\r\n",
    "            last_dim_padding = (0, num_missing_samples)\r\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\r\n",
    "            \r\n",
    "        return signal\r\n",
    "\r\n",
    "    def _normalize_like_sincnet(self, signal):\r\n",
    "        return signal/torch.max(torch.abs(signal))\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.wav_path_list)\r\n",
    "\r\n",
    "    def __getitem__(self, index):\r\n",
    "        wav_path = self.wav_path_list[index]\r\n",
    "        wav_name = Path(wav_path).stem\r\n",
    "        label = self.stem_to_spk_mapping[wav_name]\r\n",
    "\r\n",
    "        signal, sample_rate = torchaudio.load(wav_path)\r\n",
    "\r\n",
    "        # moving to cpu is now done in the training phase\r\n",
    "        # signal = signal.to(self.device)\r\n",
    "\r\n",
    "        signal = self._resample_to_target_sr(signal, sample_rate)\r\n",
    "        signal = self._mix_down_to_mono(signal)\r\n",
    "\r\n",
    "        signal =  self._trim(signal)\r\n",
    "        signal = self._normalize_like_sincnet(signal)\r\n",
    "\r\n",
    "        # signal = signal.squeeze(0)\r\n",
    "\r\n",
    "        label_index = self.speakers_list.index(label)\r\n",
    "\r\n",
    "        return signal, label_index"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "target = torch.randint(0, 10, (10,))\r\n",
    "one_hot = torch.nn.functional.one_hot(target)\r\n",
    "target, one_hot"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([4, 3, 0, 0, 2, 1, 0, 1, 8, 5]),\n",
       " tensor([[0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 1, 0, 0, 0]]))"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "target = torch.randint(0, 10, (1,10))\r\n",
    "squeezed = target.squeeze(0)\r\n",
    "print(target, squeezed)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[6, 1, 2, 5, 0, 1, 2, 1, 7, 5]]) tensor([6, 1, 2, 5, 0, 1, 2, 1, 7, 5])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train-Evaluate-Test SPLITTING\r\n",
    "\r\n",
    "Strategy:\r\n",
    "1. train_set will be strictly used for training.\r\n",
    "2. eval_set will be used for evaluation during training.\r\n",
    "3. test_set will be used to assess the model after training.\r\n",
    "\r\n",
    "Dev indicates data used during the whole training process, which indicates both train and eval datasets.\r\n",
    "\r\n",
    "Each set will have entirely different speakers (classes)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import random\r\n",
    "\r\n",
    "# The official files provided will be generated using seed 999\r\n",
    "# Run this part of the notebooks sequentially\r\n",
    "random.seed(1001)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# There are 508 speakers in the Bangla ASR dataset\r\n",
    "\r\n",
    "# Test dataset will be created from all audio of TEST_CLASS_NUMBERS of speakers\r\n",
    "# These speakers will not appear during training or evaluation\r\n",
    "TEST_CLASS_NUMBERS = 100\r\n",
    "DEV_CLASS_NUMBERS = 408\r\n",
    "\r\n",
    "# Train and eval dataset will be created from the same pool of dataset\r\n",
    "EVAL_TRAIN_RATIO = 0.10"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "DEV_WAV_LIST_LOCATION = 'notebooks/TrainBdAsrOnRawNet2/dev_test_list/dev_list.txt'\r\n",
    "TEST_WAV_LIST_LOCATION = 'notebooks/TrainBdAsrOnRawNet2/dev_test_list/test_list.txt'\r\n",
    "\r\n",
    "DEV_SPEAKER_CLASS_ORDER_LOCATION = 'notebooks/TrainBdAsrOnRawNet2/dev_test_list/dev_spk_class_order.txt'\r\n",
    "TEST_SPEAKER_CLASS_ORDER_LOCATION = 'notebooks/TrainBdAsrOnRawNet2/dev_test_list/test_spk_class_order.txt'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def get_wav_list(dataset_dir):\r\n",
    "    pattern = '**/*.wav'\r\n",
    "    files = glob.glob(dataset_dir + pattern , recursive=True)\r\n",
    "\r\n",
    "    # Normalize the file paths. To get file paths with '/' or '\\\\' consistently depending on OS\r\n",
    "    wav_list = [os.path.normpath(i) for i in files]\r\n",
    "    return wav_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def generate_labels_from_tsv(wav_list, tsv_loc):\r\n",
    "    labels = {}\r\n",
    "\r\n",
    "    with open(tsv_loc, encoding=\"utf-8\") as tsvfile:\r\n",
    "        tsvreader = csv.reader(tsvfile, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\r\n",
    "        for line in tsvreader:\r\n",
    "            wav_file_name = line[0]\r\n",
    "            speaker_id = line[1]\r\n",
    "\r\n",
    "            labels[wav_file_name] = speaker_id\r\n",
    "\r\n",
    "    return labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def get_speaker_to_paths_dict(wav_list, labels):\r\n",
    "    \r\n",
    "    spk_to_path_d = {}\r\n",
    "\r\n",
    "    for wav_path in wav_list:\r\n",
    "        wav_name = Path(wav_path).stem\r\n",
    "        spk_id = labels[wav_name]\r\n",
    "\r\n",
    "        if spk_id in spk_to_path_d.keys():\r\n",
    "            spk_to_path_d[spk_id].append(wav_path)\r\n",
    "        else:\r\n",
    "            spk_to_path_d[spk_id] = [wav_path]\r\n",
    "    \r\n",
    "    return spk_to_path_d"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "wav_list = get_wav_list(BANGLA_ASR_DATASET_DIRECTORY)\r\n",
    "labels = generate_labels_from_tsv(wav_list, BANGLA_ASR_TSV_LOCATION)\r\n",
    "speaker_to_paths_dict = get_speaker_to_paths_dict(wav_list, labels)\r\n",
    "\r\n",
    "print(len(wav_list), len(labels.keys()), len(speaker_to_paths_dict.keys()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "218703 218703 508\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# In this section we divide the whole dataset lists in two different lists\r\n",
    "# One for the dev={train, eval} set, another for the test set\r\n",
    "\r\n",
    "test_speakers_keys = random.sample(speaker_to_paths_dict.keys(), TEST_CLASS_NUMBERS)\r\n",
    "print\r\n",
    "test_speakers_wavs_stems = []\r\n",
    "dev_speakers_wavs_stems = []\r\n",
    "\r\n",
    "total = 0\r\n",
    "# Add test speakers paths to the test speaker dictionary, and\r\n",
    "# Remove the test speakers from speaker_to_paths_dict\r\n",
    "for key in speaker_to_paths_dict.keys():\r\n",
    "    current_speaker_wavs = speaker_to_paths_dict[key]\r\n",
    "    current_speaker_wavs_stems = [Path(x).stem for x in current_speaker_wavs]\r\n",
    "    total += len(current_speaker_wavs_stems)\r\n",
    "\r\n",
    "    if key in test_speakers_keys:\r\n",
    "        test_speakers_wavs_stems += current_speaker_wavs_stems\r\n",
    "    else:\r\n",
    "        dev_speakers_wavs_stems += current_speaker_wavs_stems\r\n",
    "    \r\n",
    "print(total)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "218703\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "print(len(test_speakers_wavs_stems), len(dev_speakers_wavs_stems))\r\n",
    "print(len(test_speakers_wavs_stems) + len(dev_speakers_wavs_stems))\r\n",
    "\r\n",
    "#42236 176467\r\n",
    "#218703"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "42236 176467\n",
      "218703\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "user_choice = input(\"Generate the dev test split files? Enter YES/NO\")\r\n",
    "if user_choice == \"YES\":\r\n",
    "    # Generate the dev list file and the test list file\r\n",
    "    with open(DEV_WAV_LIST_LOCATION, \"w\") as file:\r\n",
    "        for stem in dev_speakers_wavs_stems:\r\n",
    "            file.write(stem + \"\\n\")\r\n",
    "\r\n",
    "    with open(TEST_WAV_LIST_LOCATION, \"w\") as file:\r\n",
    "        for stem in test_speakers_wavs_stems:\r\n",
    "            file.write(stem + \"\\n\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get DevSet and TestSet\r\n",
    "Devset and test sets will be generated following the splitted lists generated in the previous sections. Devset will then be split into trainset and evalset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "def get_stem_list_from_file(file_path):\r\n",
    "    stem_list = []\r\n",
    "    # The file should have one stem per line\r\n",
    "    with open(file_path, \"r\") as file:\r\n",
    "        for line in file.readlines():\r\n",
    "            line = line.strip()\r\n",
    "            stem_list.append(line)\r\n",
    "    \r\n",
    "    return stem_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "dev_stem_list = get_stem_list_from_file(DEV_WAV_LIST_LOCATION)\r\n",
    "test_stem_list = get_stem_list_from_file(TEST_WAV_LIST_LOCATION)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "devset = BanglaAsrDataset(\r\n",
    "    dataset_dir=BANGLA_ASR_DATASET_DIRECTORY,\r\n",
    "    wav_stem_list=dev_stem_list,\r\n",
    "    tsv_loc = BANGLA_ASR_TSV_LOCATION,\r\n",
    "    target_sample_rate=SAMPLE_RATE,\r\n",
    "    target_num_samples = NUMBER_OF_SAMPLES,\r\n",
    "    trim_amount_time = TRIM_AMOUNT_TIME,\r\n",
    "    device = device\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "testset = BanglaAsrDataset(\r\n",
    "    dataset_dir=BANGLA_ASR_DATASET_DIRECTORY,\r\n",
    "    wav_stem_list=test_stem_list,\r\n",
    "    tsv_loc = BANGLA_ASR_TSV_LOCATION,\r\n",
    "    target_sample_rate=SAMPLE_RATE,\r\n",
    "    target_num_samples = NUMBER_OF_SAMPLES,\r\n",
    "    trim_amount_time = TRIM_AMOUNT_TIME,\r\n",
    "    device = device\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "print(len(devset.speakers_list))\r\n",
    "print(len(testset.speakers_list))\r\n",
    "\r\n",
    "print(devset.speakers_list[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "408\n",
      "100\n",
      "01e91\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "devset.speakers_list"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('01e91',\n",
       " '03284',\n",
       " '05901',\n",
       " '0606d',\n",
       " '0633a',\n",
       " '06527',\n",
       " '06736',\n",
       " '0696e',\n",
       " '06bb7',\n",
       " '06e62',\n",
       " '06ea3',\n",
       " '0744c',\n",
       " '07660',\n",
       " '086dd',\n",
       " '0895a',\n",
       " '089ef',\n",
       " '09b32',\n",
       " '09f0d',\n",
       " '0a7a0',\n",
       " '0a87a',\n",
       " '0b7ca',\n",
       " '0bace',\n",
       " '0d8c1',\n",
       " '0d9f1',\n",
       " '0e637',\n",
       " '0edef',\n",
       " '0f047',\n",
       " '10a9f',\n",
       " '11915',\n",
       " '11bd2',\n",
       " '11de3',\n",
       " '121bd',\n",
       " '12702',\n",
       " '1279e',\n",
       " '13b6a',\n",
       " '13b70',\n",
       " '14816',\n",
       " '14d25',\n",
       " '15a75',\n",
       " '1652d',\n",
       " '16877',\n",
       " '16a17',\n",
       " '16cfb',\n",
       " '16e96',\n",
       " '17ca1',\n",
       " '17dcd',\n",
       " '194f5',\n",
       " '195f7',\n",
       " '1a2a1',\n",
       " '1b359',\n",
       " '1cb75',\n",
       " '1e7c6',\n",
       " '1e7c7',\n",
       " '1f733',\n",
       " '1f974',\n",
       " '21e54',\n",
       " '22096',\n",
       " '23291',\n",
       " '239f7',\n",
       " '24aed',\n",
       " '25117',\n",
       " '25696',\n",
       " '256f5',\n",
       " '2695f',\n",
       " '274ee',\n",
       " '282d3',\n",
       " '2840e',\n",
       " '287bd',\n",
       " '28bb3',\n",
       " '28dfc',\n",
       " '29089',\n",
       " '2909b',\n",
       " '29bab',\n",
       " '29d44',\n",
       " '29f8c',\n",
       " '2ad74',\n",
       " '2aecc',\n",
       " '2bc73',\n",
       " '2c9c5',\n",
       " '2d78f',\n",
       " '2d98b',\n",
       " '2de86',\n",
       " '2ecf8',\n",
       " '2ee04',\n",
       " '2f547',\n",
       " '2fc12',\n",
       " '2fe3c',\n",
       " '315e9',\n",
       " '31f2a',\n",
       " '321b6',\n",
       " '323f1',\n",
       " '32998',\n",
       " '32b3e',\n",
       " '32c77',\n",
       " '33814',\n",
       " '355c1',\n",
       " '35ef3',\n",
       " '37d7e',\n",
       " '388c9',\n",
       " '38d3a',\n",
       " '38ff0',\n",
       " '39293',\n",
       " '39dd0',\n",
       " '3b1f7',\n",
       " '3bb15',\n",
       " '3bea8',\n",
       " '3c897',\n",
       " '3ccf1',\n",
       " '3ec4d',\n",
       " '3f493',\n",
       " '3f875',\n",
       " '3faff',\n",
       " '3fb17',\n",
       " '3fc9d',\n",
       " '3fce2',\n",
       " '404ee',\n",
       " '410fa',\n",
       " '4164f',\n",
       " '420df',\n",
       " '4237e',\n",
       " '430b2',\n",
       " '43b04',\n",
       " '43fc0',\n",
       " '442b7',\n",
       " '4486f',\n",
       " '46d49',\n",
       " '46da4',\n",
       " '47e8d',\n",
       " '491a5',\n",
       " '49543',\n",
       " '49d63',\n",
       " '4a67e',\n",
       " '4a918',\n",
       " '4bd2c',\n",
       " '4bede',\n",
       " '4c1eb',\n",
       " '4c555',\n",
       " '4dd57',\n",
       " '4e900',\n",
       " '4ea73',\n",
       " '4f2ae',\n",
       " '4f450',\n",
       " '4fbd1',\n",
       " '520fb',\n",
       " '52578',\n",
       " '52f27',\n",
       " '53396',\n",
       " '5354e',\n",
       " '536d9',\n",
       " '54cdd',\n",
       " '54e68',\n",
       " '554fe',\n",
       " '5639c',\n",
       " '577fa',\n",
       " '59c73',\n",
       " '5a61c',\n",
       " '5b13f',\n",
       " '5cb47',\n",
       " '5d12f',\n",
       " '5d80e',\n",
       " '5dd61',\n",
       " '5dec6',\n",
       " '5e80b',\n",
       " '60b9d',\n",
       " '616ec',\n",
       " '61820',\n",
       " '64092',\n",
       " '65e30',\n",
       " '66b3c',\n",
       " '66c6b',\n",
       " '66dcd',\n",
       " '68549',\n",
       " '68ab7',\n",
       " '693cb',\n",
       " '69449',\n",
       " '6973e',\n",
       " '698a4',\n",
       " '698ff',\n",
       " '69d9a',\n",
       " '6a35e',\n",
       " '6aac8',\n",
       " '6ab59',\n",
       " '6b437',\n",
       " '6c17f',\n",
       " '6c6bf',\n",
       " '6d129',\n",
       " '6da0a',\n",
       " '6e6fa',\n",
       " '6ee5f',\n",
       " '6fb36',\n",
       " '7027a',\n",
       " '717bb',\n",
       " '72649',\n",
       " '730c3',\n",
       " '75626',\n",
       " '75f18',\n",
       " '76329',\n",
       " '767ff',\n",
       " '778c0',\n",
       " '77f82',\n",
       " '7898e',\n",
       " '79b06',\n",
       " '7a227',\n",
       " '7aed9',\n",
       " '7b103',\n",
       " '7b132',\n",
       " '7b60a',\n",
       " '7c272',\n",
       " '7d004',\n",
       " '7d133',\n",
       " '7d3e1',\n",
       " '7df87',\n",
       " '7ec1c',\n",
       " '7ed14',\n",
       " '7ee9b',\n",
       " '7f2ef',\n",
       " '7f71c',\n",
       " '80505',\n",
       " '80518',\n",
       " '80539',\n",
       " '809cd',\n",
       " '80be1',\n",
       " '8143b',\n",
       " '82926',\n",
       " '83c53',\n",
       " '8406e',\n",
       " '84145',\n",
       " '85430',\n",
       " '860f4',\n",
       " '86b1b',\n",
       " '870e3',\n",
       " '880ae',\n",
       " '8845d',\n",
       " '88654',\n",
       " '8b859',\n",
       " '8ba2b',\n",
       " '8c07c',\n",
       " '8c120',\n",
       " '8cf95',\n",
       " '8edea',\n",
       " '903fa',\n",
       " '9065c',\n",
       " '91223',\n",
       " '91230',\n",
       " '91b16',\n",
       " '91dff',\n",
       " '9282d',\n",
       " '92bb2',\n",
       " '92bd8',\n",
       " '92f59',\n",
       " '93080',\n",
       " '932bb',\n",
       " '9351a',\n",
       " '93bf3',\n",
       " '93d3b',\n",
       " '948d3',\n",
       " '95907',\n",
       " '9642e',\n",
       " '967fd',\n",
       " '976b1',\n",
       " '9813c',\n",
       " '98b81',\n",
       " '9b864',\n",
       " '9d46f',\n",
       " '9d7bf',\n",
       " '9e923',\n",
       " '9f25e',\n",
       " '9fa90',\n",
       " 'a0277',\n",
       " 'a06a1',\n",
       " 'a1d4a',\n",
       " 'a3a6d',\n",
       " 'a43b5',\n",
       " 'a46e2',\n",
       " 'a4750',\n",
       " 'a699c',\n",
       " 'a916a',\n",
       " 'aabe7',\n",
       " 'ab946',\n",
       " 'ac0df',\n",
       " 'ac904',\n",
       " 'ad69a',\n",
       " 'adf01',\n",
       " 'af81d',\n",
       " 'b00ba',\n",
       " 'b08d0',\n",
       " 'b52ab',\n",
       " 'b5fdc',\n",
       " 'b618b',\n",
       " 'b78ec',\n",
       " 'b7be9',\n",
       " 'b8bba',\n",
       " 'b8bdd',\n",
       " 'b8bf0',\n",
       " 'b8ed8',\n",
       " 'b8fe1',\n",
       " 'b9122',\n",
       " 'b9a81',\n",
       " 'b9b98',\n",
       " 'ba2f6',\n",
       " 'baa02',\n",
       " 'bbb22',\n",
       " 'bc81e',\n",
       " 'bcaa8',\n",
       " 'bd576',\n",
       " 'bdd05',\n",
       " 'bf530',\n",
       " 'bfac7',\n",
       " 'c0965',\n",
       " 'c0d65',\n",
       " 'c11b1',\n",
       " 'c2082',\n",
       " 'c3cca',\n",
       " 'c41d8',\n",
       " 'c4973',\n",
       " 'c5479',\n",
       " 'c5dc8',\n",
       " 'c6848',\n",
       " 'c749e',\n",
       " 'c7705',\n",
       " 'c8f7f',\n",
       " 'c958a',\n",
       " 'c9c19',\n",
       " 'ca088',\n",
       " 'ca831',\n",
       " 'cb1aa',\n",
       " 'cb627',\n",
       " 'cb842',\n",
       " 'cc18a',\n",
       " 'cc9fd',\n",
       " 'cd3ac',\n",
       " 'ce180',\n",
       " 'ce629',\n",
       " 'ce775',\n",
       " 'ced74',\n",
       " 'cfc65',\n",
       " 'd0c70',\n",
       " 'd136b',\n",
       " 'd1a48',\n",
       " 'd1b12',\n",
       " 'd2294',\n",
       " 'd2d71',\n",
       " 'd2faf',\n",
       " 'd3e06',\n",
       " 'd41be',\n",
       " 'd5213',\n",
       " 'd5f7b',\n",
       " 'd711f',\n",
       " 'd8c80',\n",
       " 'd9081',\n",
       " 'd9b11',\n",
       " 'd9ba9',\n",
       " 'dae88',\n",
       " 'dafb6',\n",
       " 'db181',\n",
       " 'db735',\n",
       " 'dba43',\n",
       " 'dec2d',\n",
       " 'dfefa',\n",
       " 'e03ce',\n",
       " 'e03fc',\n",
       " 'e2c11',\n",
       " 'e31e7',\n",
       " 'e3d8b',\n",
       " 'e3f89',\n",
       " 'e43d4',\n",
       " 'e6203',\n",
       " 'e6752',\n",
       " 'e6d83',\n",
       " 'e6e48',\n",
       " 'e7230',\n",
       " 'e754e',\n",
       " 'e89bb',\n",
       " 'e8dde',\n",
       " 'e9589',\n",
       " 'e9759',\n",
       " 'e9fa7',\n",
       " 'ea5be',\n",
       " 'ea8fa',\n",
       " 'eaa7a',\n",
       " 'ec91c',\n",
       " 'ee4ad',\n",
       " 'ee886',\n",
       " 'ee994',\n",
       " 'eecdd',\n",
       " 'ef915',\n",
       " 'f1240',\n",
       " 'f143a',\n",
       " 'f1ea4',\n",
       " 'f1ff2',\n",
       " 'f2097',\n",
       " 'f2780',\n",
       " 'f280a',\n",
       " 'f600a',\n",
       " 'f658d',\n",
       " 'f6e55',\n",
       " 'f7a7f',\n",
       " 'f837a',\n",
       " 'f89d7',\n",
       " 'f9dfb',\n",
       " 'fb4cb',\n",
       " 'fcc89',\n",
       " 'fcefa',\n",
       " 'fd880',\n",
       " 'fdcba',\n",
       " 'ff1c3',\n",
       " 'ff1eb',\n",
       " 'ff970')"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# We save our speakers class order for future referral and debugging in case some function implementations change\r\n",
    "user_choice = input(\"Are you sure, you want to rewrite the speaker class order?\")\r\n",
    "if user_choice == \"YES\":\r\n",
    "\r\n",
    "    with open(DEV_SPEAKER_CLASS_ORDER_LOCATION, \"w\") as file:\r\n",
    "        for spk in devset.speakers_list:\r\n",
    "            file.write(spk + \"\\n\")\r\n",
    "\r\n",
    "    with open(TEST_SPEAKER_CLASS_ORDER_LOCATION, \"w\") as file:\r\n",
    "        for spk in testset.speakers_list:\r\n",
    "            file.write(spk + \"\\n\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model !"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "from torch import nn\r\n",
    "from torchsummary import summary\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import math\r\n",
    "\r\n",
    "import torch.nn.functional as F\r\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## FRM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "class FRM(nn.Module):\r\n",
    "    def __init__(self, nb_dim, do_add = True, do_mul = True):\r\n",
    "        super(FRM, self).__init__()\r\n",
    "        self.fc = nn.Linear(nb_dim, nb_dim)\r\n",
    "        self.sig = nn.Sigmoid()\r\n",
    "        self.do_add = do_add\r\n",
    "        self.do_mul = do_mul\r\n",
    "    def forward(self, x):\r\n",
    "        y = F.adaptive_avg_pool1d(x, 1).view(x.size(0), -1)\r\n",
    "        \r\n",
    "        y = self.sig(self.fc(y)).view(x.size(0), x.size(1), -1)\r\n",
    "\r\n",
    "        if self.do_mul: x = x * y\r\n",
    "        if self.do_add: x = x + y\r\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Residual Block wFRM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "class Residual_block_wFRM(nn.Module):\r\n",
    "    def __init__(self, nb_filts, first = False):\r\n",
    "        super(Residual_block_wFRM, self).__init__()\r\n",
    "        self.first = first\r\n",
    "        if not self.first:\r\n",
    "            self.bn1 = nn.BatchNorm1d(num_features = nb_filts[0])\r\n",
    "        self.lrelu = nn.LeakyReLU()\r\n",
    "        self.lrelu_keras = nn.LeakyReLU(negative_slope=0.3)\r\n",
    "        \r\n",
    "        self.conv1 = nn.Conv1d(in_channels = nb_filts[0],\r\n",
    "            out_channels = nb_filts[1],\r\n",
    "            kernel_size = 3,\r\n",
    "            padding = 1,\r\n",
    "            stride = 1)\r\n",
    "        self.bn2 = nn.BatchNorm1d(num_features = nb_filts[1])\r\n",
    "        self.conv2 = nn.Conv1d(in_channels = nb_filts[1],\r\n",
    "            out_channels = nb_filts[1],\r\n",
    "            padding = 1,\r\n",
    "            kernel_size = 3,\r\n",
    "            stride = 1)\r\n",
    "        \r\n",
    "        if nb_filts[0] != nb_filts[1]:\r\n",
    "            self.downsample = True\r\n",
    "            self.conv_downsample = nn.Conv1d(in_channels = nb_filts[0],\r\n",
    "                out_channels = nb_filts[1],\r\n",
    "                padding = 0,\r\n",
    "                kernel_size = 1,\r\n",
    "                stride = 1)\r\n",
    "            \r\n",
    "        else:\r\n",
    "            self.downsample = False\r\n",
    "        self.mp = nn.MaxPool1d(3)\r\n",
    "        self.frm = FRM(\r\n",
    "            nb_dim = nb_filts[1],\r\n",
    "            do_add = True,\r\n",
    "            do_mul = True)\r\n",
    "        \r\n",
    "    def forward(self, x):\r\n",
    "        identity = x\r\n",
    "        if not self.first:\r\n",
    "            out = self.bn1(x)\r\n",
    "            out = self.lrelu_keras(out)\r\n",
    "        else:\r\n",
    "            out = x\r\n",
    "            \r\n",
    "        out = self.conv1(out)\r\n",
    "        out = self.bn2(out)\r\n",
    "        out = self.lrelu_keras(out)\r\n",
    "        out = self.conv2(out)\r\n",
    "        \r\n",
    "        if self.downsample:\r\n",
    "            identity = self.conv_downsample(identity)\r\n",
    "            \r\n",
    "        out += identity\r\n",
    "        out = self.mp(out)\r\n",
    "        out = self.frm(out)\r\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LayerNorm"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "class LayerNorm(nn.Module):\r\n",
    "\r\n",
    "    def __init__(self, features, eps=1e-6):\r\n",
    "        super(LayerNorm,self).__init__()\r\n",
    "        self.gamma = nn.Parameter(torch.ones(features))\r\n",
    "        self.beta = nn.Parameter(torch.zeros(features))\r\n",
    "        self.eps = eps\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        mean = x.mean(-1, keepdim=True)\r\n",
    "        std = x.std(-1, keepdim=True)\r\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SincConv Fast"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "class SincConv_fast(nn.Module):\r\n",
    "    \"\"\"Sinc-based convolution\r\n",
    "    Parameters\r\n",
    "    ----------\r\n",
    "    in_channels : `int`\r\n",
    "        Number of input channels. Must be 1.\r\n",
    "    out_channels : `int`\r\n",
    "        Number of filters.\r\n",
    "    kernel_size : `int`\r\n",
    "        Filter length.\r\n",
    "    sample_rate : `int`, optional\r\n",
    "        Sample rate. Defaults to 16000.\r\n",
    "    Usage\r\n",
    "    -----\r\n",
    "    See `torch.nn.Conv1d`\r\n",
    "    Reference\r\n",
    "    ---------\r\n",
    "    Mirco Ravanelli, Yoshua Bengio,\r\n",
    "    \"Speaker Recognition from raw waveform with SincNet\".\r\n",
    "    https://arxiv.org/abs/1808.00158\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    @staticmethod\r\n",
    "    def to_mel(hz):\r\n",
    "        return 2595 * np.log10(1 + hz / 700)\r\n",
    "\r\n",
    "    @staticmethod\r\n",
    "    def to_hz(mel):\r\n",
    "        return 700 * (10 ** (mel / 2595) - 1)\r\n",
    "\r\n",
    "    def __init__(self, out_channels, kernel_size, sample_rate=16000, in_channels=1,\r\n",
    "                 stride=1, padding=0, dilation=1, bias=False, groups=1, min_low_hz=50, min_band_hz=50):\r\n",
    "\r\n",
    "        super(SincConv_fast,self).__init__()\r\n",
    "\r\n",
    "        if in_channels != 1:\r\n",
    "            #msg = (f'SincConv only support one input channel '\r\n",
    "            #       f'(here, in_channels = {in_channels:d}).')\r\n",
    "            msg = \"SincConv only support one input channel (here, in_channels = {%i})\" % (in_channels)\r\n",
    "            raise ValueError(msg)\r\n",
    "\r\n",
    "        self.out_channels = out_channels\r\n",
    "        self.kernel_size = kernel_size\r\n",
    "        \r\n",
    "        # Forcing the filters to be odd (i.e, perfectly symmetrics)\r\n",
    "        if kernel_size%2==0:\r\n",
    "            self.kernel_size=self.kernel_size+1\r\n",
    "            \r\n",
    "        self.stride = stride\r\n",
    "        self.padding = padding\r\n",
    "        self.dilation = dilation\r\n",
    "\r\n",
    "        if bias:\r\n",
    "            raise ValueError('SincConv does not support bias.')\r\n",
    "        if groups > 1:\r\n",
    "            raise ValueError('SincConv does not support groups.')\r\n",
    "\r\n",
    "        self.sample_rate = sample_rate\r\n",
    "        self.min_low_hz = min_low_hz\r\n",
    "        self.min_band_hz = min_band_hz\r\n",
    "\r\n",
    "        # initialize filterbanks such that they are equally spaced in Mel scale\r\n",
    "        low_hz = 30\r\n",
    "        high_hz = self.sample_rate / 2 - (self.min_low_hz + self.min_band_hz)\r\n",
    "\r\n",
    "        mel = np.linspace(self.to_mel(low_hz),\r\n",
    "                          self.to_mel(high_hz),\r\n",
    "                          self.out_channels + 1)\r\n",
    "        hz = self.to_hz(mel)\r\n",
    "        \r\n",
    "\r\n",
    "        # filter lower frequency (out_channels, 1)\r\n",
    "        self.low_hz_ = nn.Parameter(torch.Tensor(hz[:-1]).view(-1, 1))\r\n",
    "\r\n",
    "        # filter frequency band (out_channels, 1)\r\n",
    "        self.band_hz_ = nn.Parameter(torch.Tensor(np.diff(hz)).view(-1, 1))\r\n",
    "\r\n",
    "        # Hamming window\r\n",
    "        #self.window_ = torch.hamming_window(self.kernel_size)\r\n",
    "        n_lin=torch.linspace(0, (self.kernel_size/2)-1, steps=int((self.kernel_size/2))) # computing only half of the window\r\n",
    "        self.window_=0.54-0.46*torch.cos(2*math.pi*n_lin/self.kernel_size);\r\n",
    "\r\n",
    "        # (1, kernel_size/2)\r\n",
    "        n = (self.kernel_size - 1) / 2.0\r\n",
    "        self.n_ = 2*math.pi*torch.arange(-n, 0).view(1, -1) / self.sample_rate # Due to symmetry, I only need half of the time axes\r\n",
    "\r\n",
    "    def forward(self, waveforms):\r\n",
    "        \"\"\"\r\n",
    "        Parameters\r\n",
    "        ----------\r\n",
    "        waveforms : `torch.Tensor` (batch_size, 1, n_samples)\r\n",
    "            Batch of waveforms.\r\n",
    "        Returns\r\n",
    "        -------\r\n",
    "        features : `torch.Tensor` (batch_size, out_channels, n_samples_out)\r\n",
    "            Batch of sinc filters activations.\r\n",
    "        \"\"\"\r\n",
    "\r\n",
    "        self.n_ = self.n_.to(waveforms.device)\r\n",
    "\r\n",
    "        self.window_ = self.window_.to(waveforms.device)\r\n",
    "\r\n",
    "        low = self.min_low_hz  + torch.abs(self.low_hz_)\r\n",
    "        \r\n",
    "        high = torch.clamp(low + self.min_band_hz + torch.abs(self.band_hz_),self.min_low_hz,self.sample_rate/2)\r\n",
    "        band=(high-low)[:,0]\r\n",
    "        \r\n",
    "        f_times_t_low = torch.matmul(low, self.n_)\r\n",
    "        f_times_t_high = torch.matmul(high, self.n_)\r\n",
    "\r\n",
    "        band_pass_left=((torch.sin(f_times_t_high)-torch.sin(f_times_t_low))/(self.n_/2))*self.window_ # Equivalent of Eq.4 of the reference paper (SPEAKER RECOGNITION FROM RAW WAVEFORM WITH SINCNET). I just have expanded the sinc and simplified the terms. This way I avoid several useless computations. \r\n",
    "        band_pass_center = 2*band.view(-1,1)\r\n",
    "        band_pass_right= torch.flip(band_pass_left,dims=[1])\r\n",
    "        \r\n",
    "        \r\n",
    "        band_pass=torch.cat([band_pass_left,band_pass_center,band_pass_right],dim=1)\r\n",
    "\r\n",
    "        \r\n",
    "        band_pass = band_pass / (2*band[:,None])\r\n",
    "        \r\n",
    "\r\n",
    "        self.filters = (band_pass).view(\r\n",
    "            self.out_channels, 1, self.kernel_size)\r\n",
    "\r\n",
    "        return F.conv1d(waveforms, self.filters, stride=self.stride,\r\n",
    "                        padding=self.padding, dilation=self.dilation,\r\n",
    "                         bias=None, groups=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RawNet2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "class RawNet2(nn.Module):\r\n",
    "    def __init__(self, d_args):\r\n",
    "        super(RawNet2, self).__init__()\r\n",
    "\r\n",
    "        self.ln = LayerNorm(d_args['nb_samp'])\r\n",
    "        self.first_conv = SincConv_fast(in_channels = d_args['in_channels'],\r\n",
    "            out_channels = d_args['filts'][0],\r\n",
    "            kernel_size = d_args['first_conv']\r\n",
    "            )\r\n",
    "        \r\n",
    "        self.first_bn = nn.BatchNorm1d(num_features = d_args['filts'][0])\r\n",
    "        self.lrelu = nn.LeakyReLU()\r\n",
    "        self.lrelu_keras = nn.LeakyReLU(negative_slope = 0.3)\r\n",
    "        \r\n",
    "        self.block0 = nn.Sequential(Residual_block_wFRM(nb_filts = d_args['filts'][1], first = True))\r\n",
    "        self.block1 = nn.Sequential(Residual_block_wFRM(nb_filts = d_args['filts'][1]))\r\n",
    " \r\n",
    "        self.block2 = nn.Sequential(Residual_block_wFRM(nb_filts = d_args['filts'][2]))\r\n",
    "        d_args['filts'][2][0] = d_args['filts'][2][1]\r\n",
    "        self.block3 = nn.Sequential(Residual_block_wFRM(nb_filts = d_args['filts'][2]))\r\n",
    "        self.block4 = nn.Sequential(Residual_block_wFRM(nb_filts = d_args['filts'][2]))\r\n",
    "        self.block5 = nn.Sequential(Residual_block_wFRM(nb_filts = d_args['filts'][2]))\r\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\r\n",
    "\r\n",
    "        self.bn_before_gru = nn.BatchNorm1d(num_features = d_args['filts'][2][-1])\r\n",
    "        self.gru = nn.GRU(input_size = d_args['filts'][2][-1],\r\n",
    "            hidden_size = d_args['gru_node'],\r\n",
    "            num_layers = d_args['nb_gru_layer'],\r\n",
    "            batch_first = True)\r\n",
    "\r\n",
    "        \r\n",
    "        self.fc1_gru = nn.Linear(in_features = d_args['gru_node'],\r\n",
    "            out_features = d_args['nb_fc_node'])\r\n",
    "        self.fc2_gru = nn.Linear(in_features = d_args['nb_fc_node'],\r\n",
    "            out_features = d_args['nb_classes'],\r\n",
    "            bias = True)\r\n",
    "        \r\n",
    "        self.sig = nn.Sigmoid()\r\n",
    "        \r\n",
    "    def forward(self, x, y = 0, is_test=False):\r\n",
    "        #follow sincNet recipe\r\n",
    "        nb_samp = x.shape[0]\r\n",
    "        len_seq = x.shape[1]\r\n",
    "\r\n",
    "        x = self.ln(x)\r\n",
    "        x=x.view(nb_samp,1,len_seq)\r\n",
    "        x = F.max_pool1d(torch.abs(self.first_conv(x)), 3)\r\n",
    "        x = self.first_bn(x)\r\n",
    "        x = self.lrelu_keras(x)\r\n",
    "        \r\n",
    "        x = self.block0(x)\r\n",
    "        x = self.block1(x)\r\n",
    "\r\n",
    "        x = self.block2(x)\r\n",
    "        x = self.block3(x)\r\n",
    "        x = self.block4(x)\r\n",
    "        x = self.block5(x)\r\n",
    "\r\n",
    "        x = self.bn_before_gru(x)\r\n",
    "        x = self.lrelu_keras(x)\r\n",
    "        x = x.permute(0, 2, 1)  #(batch, filt, time) >> (batch, time, filt)\r\n",
    "        self.gru.flatten_parameters()\r\n",
    "        x, _ = self.gru(x)\r\n",
    "        x = x[:,-1,:]\r\n",
    "        code = self.fc1_gru(x)\r\n",
    "        if is_test: return code\r\n",
    "        \r\n",
    "        code_norm = code.norm(p=2,dim=1, keepdim=True) / 10.\r\n",
    "        code = torch.div(code, code_norm)\r\n",
    "        out = self.fc2_gru(code)\r\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "model_dict = {}\r\n",
    "model_dict['nb_classes'] = len(devset.speakers_list)\r\n",
    "model_dict['first_conv'] = 251\r\n",
    "model_dict['in_channels'] = 1\r\n",
    "model_dict['filts'] = [128, [128,128], [128,128], [256,256]]\r\n",
    "model_dict['m_blocks'] = [2, 4]\r\n",
    "model_dict['nb_fc_att_node'] =[1]\r\n",
    "model_dict['nb_fc_node'] = 1024\r\n",
    "model_dict['gru_node'] = 1024\r\n",
    "model_dict['nb_gru_layer'] = 1\r\n",
    "model_dict['nb_samp'] = NUMBER_OF_SAMPLES\r\n",
    "\r\n",
    "model_dict['lr_decay'] = \"keras\"\r\n",
    "model_dict['do_lr_decay'] = True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "LEARNING_RATE = 1e-3\r\n",
    "WEIGHT_DECAY = 1e-4\r\n",
    "AMSGRAD = True\r\n",
    "EPOCHS = 5\r\n",
    "BATCH_SIZE = 16\r\n",
    "\r\n",
    "# Higher number may cause errors in notebook\r\n",
    "NUMBER_OF_WORKERS = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "176467"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train Eval split"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "devset_size = len(devset)\r\n",
    "evalset_size = int(devset_size * EVAL_TRAIN_RATIO)\r\n",
    "trainset_size = devset_size - evalset_size\r\n",
    "\r\n",
    "train_set, eval_set = torch.utils.data.random_split(devset, [trainset_size, evalset_size])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "len(train_set), len(eval_set)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(158821, 17646)"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "trainset_loader = data.DataLoader(train_set,\r\n",
    "            batch_size = BATCH_SIZE, \r\n",
    "            shuffle = False,\r\n",
    "            drop_last = False,\r\n",
    "            num_workers = NUMBER_OF_WORKERS)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "evalset_loader = data.DataLoader(eval_set,\r\n",
    "            batch_size = BATCH_SIZE, \r\n",
    "            shuffle = False,\r\n",
    "            drop_last = False,\r\n",
    "            num_workers = NUMBER_OF_WORKERS)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "torch.cuda.empty_cache()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\r\n",
    "### Batch explained:\r\n",
    "\r\n",
    "If batch size = 4\r\n",
    "\r\n",
    "One batch = [ tensor([[[x,x,x]]], [[x,x,x]], [[x,x,x]], [[x,x,x]]])    , (label1, label2, label3, label4) ]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "model = RawNet2(model_dict)\r\n",
    "\r\n",
    "model.to(device)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RawNet2(\n",
       "  (ln): LayerNorm()\n",
       "  (first_conv): SincConv_fast()\n",
       "  (first_bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "  (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "  (block0): Sequential(\n",
       "    (0): Residual_block_wFRM(\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "      (frm): FRM(\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (sig): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block1): Sequential(\n",
       "    (0): Residual_block_wFRM(\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "      (frm): FRM(\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (sig): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block2): Sequential(\n",
       "    (0): Residual_block_wFRM(\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "      (frm): FRM(\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (sig): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block3): Sequential(\n",
       "    (0): Residual_block_wFRM(\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "      (frm): FRM(\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (sig): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block4): Sequential(\n",
       "    (0): Residual_block_wFRM(\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "      (frm): FRM(\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (sig): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block5): Sequential(\n",
       "    (0): Residual_block_wFRM(\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "      (frm): FRM(\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (sig): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (bn_before_gru): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (gru): GRU(128, 1024, batch_first=True)\n",
       "  (fc1_gru): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (fc2_gru): Linear(in_features=1024, out_features=408, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "# print(devset[0][0].shape)\r\n",
    "# print(devset[0][0])\r\n",
    "# model(devset[0][0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "def keras_lr_decay(step, decay = 0.0001):\r\n",
    "    return 1./(1. + decay * step)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "params = [\r\n",
    "    {\r\n",
    "        'params': [\r\n",
    "            param for name, param in model.named_parameters()\r\n",
    "            if 'bn' not in name\r\n",
    "        ]\r\n",
    "    },\r\n",
    "    {\r\n",
    "        'params': [\r\n",
    "            param for name, param in model.named_parameters()\r\n",
    "            if 'bn' in name\r\n",
    "        ],\r\n",
    "        'weight_decay':\r\n",
    "        0\r\n",
    "    },\r\n",
    "]\r\n",
    "\r\n",
    "criterion = {}\r\n",
    "criterion['cce'] = nn.CrossEntropyLoss()\r\n",
    "\r\n",
    "optimizer = torch.optim.Adam(params,\r\n",
    "            lr = LEARNING_RATE,\r\n",
    "            weight_decay = WEIGHT_DECAY,\r\n",
    "            amsgrad = AMSGRAD)\r\n",
    "\r\n",
    "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda step: keras_lr_decay(step))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "def train_model(model, db_gen, optimizer, epoch, args, device, lr_scheduler, criterion):\r\n",
    "    model.train()\r\n",
    "    with tqdm(total = len(db_gen), ncols = 70) as pbar:\r\n",
    "        for m_batch, m_label in db_gen:\r\n",
    "            \r\n",
    "            m_batch = m_batch.squeeze(1)\r\n",
    "\r\n",
    "            m_batch, m_label = m_batch.to(device), m_label.to(device)\r\n",
    "\r\n",
    "            output = model(m_batch, m_label)\r\n",
    "            \r\n",
    "            cce_loss = criterion['cce'](output, m_label)\r\n",
    "            loss = cce_loss\r\n",
    "\r\n",
    "            optimizer.zero_grad()\r\n",
    "            loss.backward()\r\n",
    "            optimizer.step()\r\n",
    "            pbar.set_description('epoch: %d, cce:%.3f'%(epoch, cce_loss))\r\n",
    "            pbar.update(1)\r\n",
    "            if args['do_lr_decay']:\r\n",
    "                if model_dict['lr_decay'] == 'keras': lr_scheduler.step()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "for epoch in range(EPOCHS):\r\n",
    "    train_model(model = model,\r\n",
    "        db_gen = devset_loader,\r\n",
    "        args = model_dict,\r\n",
    "        optimizer = optimizer,\r\n",
    "        lr_scheduler = lr_scheduler,\r\n",
    "        criterion = criterion,\r\n",
    "        device = device,\r\n",
    "        epoch = epoch)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch: 0, cce:6.014:   0%|       | 15/11030 [00:06<1:14:36,  2.46it/s]"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('pytorch': conda)"
  },
  "interpreter": {
   "hash": "83fea67622c0fa859a960c0bf8a89199ff21c56f3cdd300f29516e12427a6ea7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}