{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TrainRawNet2: On BD ASR Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Necessary Imports:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\r\n",
    "from torch.utils import data\r\n",
    "\r\n",
    "import torchaudio\r\n",
    "\r\n",
    "import csv\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "import glob\r\n",
    "from pathlib import Path"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Directories and lcoations:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Directories are assumed to have a trailing '/' or '\\\\' in all the subsequent code\r\n",
    "\r\n",
    "CURRENT_WORKING_DIRECTORY = \"W:/SpeakerRecognitionResearch\"\r\n",
    "\r\n",
    "BANGLA_ASR_DATASET_DIRECTORY = \"data/BanglaASR/WavFiles/\"\r\n",
    "BANGLA_ASR_TSV_LOCATION = \"data/BanglaASR/utt_spk_text.tsv\"\r\n",
    "\r\n",
    "# To avoid file location related errors, we make sure \"SpeakerRecognitionResearch\" root folder is the current working directory.\r\n",
    "os.chdir(CURRENT_WORKING_DIRECTORY)\r\n",
    "os.getcwd()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'W:\\\\SpeakerRecognitionResearch'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Constants:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# If sample_rate = 16K and number_of_samples = 32000, then each tensor will be equivalent to 2 seconds of data\r\n",
    "SAMPLE_RATE = 16000\r\n",
    "NUMBER_OF_SAMPLES = 32000\r\n",
    "\r\n",
    "# Bangla ASR Dataset has around half of second of silence in the beginning\r\n",
    "# This constant will be used to cut samples from the left of the audio\r\n",
    "TRIM_AMOUNT_TIME = 0.5"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Device\r\n",
    "\r\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n",
    "print(\"Using {}.\".format(device))\r\n",
    "if device==\"cuda\": print(torch.cuda.get_device_name(0))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cuda.\n",
      "NVIDIA GeForce GTX 1050\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Custom dataset for Bangla ASR\r\n",
    "\r\n",
    "This custom dataset is written with the assumption that the Dataset has been already converted into wav format. Check evaluate_asr_ds.ipynb notebook for conversion method.\r\n",
    "\r\n",
    "A stem is the name of the file. I.E: \"a/b/c/bat.jpg\" --stem--> bat"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "class BanglaAsrDataset(data.Dataset):\r\n",
    "    def __init__(self, dataset_dir, wav_stem_list, tsv_loc, target_sample_rate, target_num_samples, trim_amount_time, device):\r\n",
    "\r\n",
    "        # wav_stem_list will have a list of stems that will be included in this dataset\r\n",
    "\r\n",
    "        tsv_dataframe = pd.read_csv(tsv_loc, quoting=csv.QUOTE_NONE, sep='\\t', header=None)\r\n",
    "\r\n",
    "        # The TSV file contains speech annotations in the third column.\r\n",
    "        # We don't need the annotations, so we drop the column\r\n",
    "        tsv_dataframe = tsv_dataframe.iloc[:,:-1]\r\n",
    "\r\n",
    "        self.dataset_dir = dataset_dir\r\n",
    "        self.wav_to_spk_mapping = dict(sorted(tsv_dataframe.values.tolist()))\r\n",
    "        self.wav_path_list = self._stem_list_to_path_list(wav_stem_list)\r\n",
    "        self.target_sample_rate = target_sample_rate\r\n",
    "        self.target_num_samples = target_num_samples\r\n",
    "        self.trim_amount_time = trim_amount_time\r\n",
    "        self.device = device\r\n",
    "        \r\n",
    "    def _get_audio_path_list(self, dataset_dir):\r\n",
    "\r\n",
    "        pattern = '**/*.wav'\r\n",
    "        files = glob.glob(self.dataset_dir + pattern , recursive=True)\r\n",
    "\r\n",
    "        # Normalize the file paths. To get file paths with '/' or '\\\\' consistently depending on OS\r\n",
    "        wav_list = [os.path.normpath(i) for i in files]\r\n",
    "\r\n",
    "        return wav_list\r\n",
    "\r\n",
    "    def _stem_list_to_path_list(self, stem_list):\r\n",
    "        # Sets are faster to search\r\n",
    "        stem_list = set(stem_list)\r\n",
    "        all_path_list = self._get_audio_path_list(self.dataset_dir)\r\n",
    "        path_list = [path for path in all_path_list if Path(path).stem in stem_list]\r\n",
    "        return path_list\r\n",
    "\r\n",
    "\r\n",
    "    def _resample_to_target_sr(self, signal, sample_rate):\r\n",
    "        if sample_rate != self.target_sample_rate:\r\n",
    "            resampler = torchaudio.transforms.Resample(sample_rate, self.target_sample_rate)\r\n",
    "            signal = resampler(signal)\r\n",
    "        return signal\r\n",
    "\r\n",
    "    def _mix_down_to_mono(self, signal):\r\n",
    "        if signal.shape[0] > 1:\r\n",
    "            signal = torch.mean(siggnal, dim=0, keepdim=True)\r\n",
    "        return signal\r\n",
    "\r\n",
    "    def _trim(self, signal):\r\n",
    "        total_samples = signal.shape[-1]\r\n",
    "\r\n",
    "        # We cut a fixed amount on the left side if the signal is big enough\r\n",
    "        trim_samples_amount = int(self.target_sample_rate * self.trim_amount_time)\r\n",
    "\r\n",
    "        if total_samples >= trim_samples_amount + self.target_num_samples:\r\n",
    "            signal = signal[: , trim_samples_amount:]\r\n",
    "            total_samples = signal.shape[-1]\r\n",
    "\r\n",
    "        # We cut from the right side if the signal is too big\r\n",
    "        if total_samples > self.target_num_samples:\r\n",
    "            signal = signal[:, :self.target_num_samples]\r\n",
    "        \r\n",
    "        # We add zero padding on the right if signal is too small\r\n",
    "        if total_samples < self.target_num_samples:\r\n",
    "            num_missing_samples = self.target_num_samples - total_samples\r\n",
    "            last_dim_padding = (0, num_missing_samples)\r\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\r\n",
    "            \r\n",
    "        return signal\r\n",
    "\r\n",
    "    def _normalize_like_sincnet(self, signal):\r\n",
    "        return signal/torch.max(torch.abs(signal))\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.wav_path_list)\r\n",
    "\r\n",
    "    def __getitem__(self, index):\r\n",
    "        wav_path = self.wav_path_list[index]\r\n",
    "        wav_name = Path(wav_path).stem\r\n",
    "        label = self.wav_to_spk_mapping[wav_name]\r\n",
    "\r\n",
    "        signal, sample_rate = torchaudio.load(wav_path)\r\n",
    "        signal = signal.to(self.device)\r\n",
    "\r\n",
    "        signal = self._resample_to_target_sr(signal, sample_rate)\r\n",
    "        signal = self._mix_down_to_mono(signal)\r\n",
    "\r\n",
    "        signal =  self._trim(signal)\r\n",
    "        signal = self._normalize_like_sincnet(signal)\r\n",
    "\r\n",
    "        return signal, label"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train-Evaluate-Test SPLITTING\r\n",
    "\r\n",
    "Strategy:\r\n",
    "1. train_set will be strictly used for training.\r\n",
    "2. eval_set will be used for evaluation during training.\r\n",
    "3. test_set will be used to assess the model after training.\r\n",
    "\r\n",
    "Dev indicates data used during the whole training process, which indicates both train and eval datasets.\r\n",
    "\r\n",
    "Each set will have entirely different speakers (classes)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import random\r\n",
    "\r\n",
    "# The official files provided will be generated using seed 999\r\n",
    "# Run this part of the notebooks sequentially\r\n",
    "random.seed(1001)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# There are 508 speakers in the Bangla ASR dataset\r\n",
    "\r\n",
    "# Test dataset will be created from all audio of TEST_CLASS_NUMBERS of speakers\r\n",
    "# These speakers will not appear during training or evaluation\r\n",
    "TEST_CLASS_NUMBERS = 100\r\n",
    "\r\n",
    "# Train and eval dataset will be created from the same pool of dataset\r\n",
    "EVAL_TRAIN_RATIO = 0.10"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "DEV_WAV_LIST_LOCATION = 'notebooks/TrainBdAsrOnRawNet2/dev_test_list/dev_list.txt'\r\n",
    "TEST_WAV_LIST_LOCATION = 'notebooks/TrainBdAsrOnRawNet2/dev_test_list/test_list.txt'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def get_wav_list(dataset_dir):\r\n",
    "    pattern = '**/*.wav'\r\n",
    "    files = glob.glob(dataset_dir + pattern , recursive=True)\r\n",
    "\r\n",
    "    # Normalize the file paths. To get file paths with '/' or '\\\\' consistently depending on OS\r\n",
    "    wav_list = [os.path.normpath(i) for i in files]\r\n",
    "    return wav_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def generate_labels_from_tsv(wav_list, tsv_loc):\r\n",
    "    labels = {}\r\n",
    "\r\n",
    "    with open(tsv_loc, encoding=\"utf-8\") as tsvfile:\r\n",
    "        tsvreader = csv.reader(tsvfile, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\r\n",
    "        for line in tsvreader:\r\n",
    "            wav_file_name = line[0]\r\n",
    "            speaker_id = line[1]\r\n",
    "\r\n",
    "            labels[wav_file_name] = speaker_id\r\n",
    "\r\n",
    "    return labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def get_speaker_to_paths_dict(wav_list, labels):\r\n",
    "    \r\n",
    "    spk_to_path_d = {}\r\n",
    "\r\n",
    "    for wav_path in wav_list:\r\n",
    "        wav_name = Path(wav_path).stem\r\n",
    "        spk_id = labels[wav_name]\r\n",
    "\r\n",
    "        if spk_id in spk_to_path_d.keys():\r\n",
    "            spk_to_path_d[spk_id].append(wav_path)\r\n",
    "        else:\r\n",
    "            spk_to_path_d[spk_id] = [wav_path]\r\n",
    "    \r\n",
    "    return spk_to_path_d"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "wav_list = get_wav_list(BANGLA_ASR_DATASET_DIRECTORY)\r\n",
    "labels = generate_labels_from_tsv(wav_list, BANGLA_ASR_TSV_LOCATION)\r\n",
    "speaker_to_paths_dict = get_speaker_to_paths_dict(wav_list, labels)\r\n",
    "\r\n",
    "print(len(wav_list), len(labels.keys()), len(speaker_to_paths_dict.keys()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "218703 218703 508\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# In this section we divide the whole dataset lists in two different lists\r\n",
    "# One for the dev={train, eval} set, another for the test set\r\n",
    "\r\n",
    "test_speakers_keys = random.sample(speaker_to_paths_dict.keys(), TEST_CLASS_NUMBERS)\r\n",
    "print\r\n",
    "test_speakers_wavs_stems = []\r\n",
    "dev_speakers_wavs_stems = []\r\n",
    "\r\n",
    "total = 0\r\n",
    "# Add test speakers paths to the test speaker dictionary, and\r\n",
    "# Remove the test speakers from speaker_to_paths_dict\r\n",
    "for key in speaker_to_paths_dict.keys():\r\n",
    "    current_speaker_wavs = speaker_to_paths_dict[key]\r\n",
    "    current_speaker_wavs_stems = [Path(x).stem for x in current_speaker_wavs]\r\n",
    "    total += len(current_speaker_wavs_stems)\r\n",
    "\r\n",
    "    if key in test_speakers_keys:\r\n",
    "        test_speakers_wavs_stems += current_speaker_wavs_stems\r\n",
    "    else:\r\n",
    "        dev_speakers_wavs_stems += current_speaker_wavs_stems\r\n",
    "    \r\n",
    "print(total)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "218703\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "print(len(test_speakers_wavs_stems), len(dev_speakers_wavs_stems))\r\n",
    "print(len(test_speakers_wavs_stems) + len(dev_speakers_wavs_stems))\r\n",
    "\r\n",
    "#42236 176467\r\n",
    "#218703"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "42236 176467\n",
      "218703\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "user_choice = input(\"Generate the dev test split files? Enter YES/NO\")\r\n",
    "if user_choice == \"YES\":\r\n",
    "    # Generate the dev list file and the test list file\r\n",
    "    with open(DEV_WAV_LIST_LOCATION, \"w\") as file:\r\n",
    "        for stem in dev_speakers_wavs_stems:\r\n",
    "            file.write(stem + \"\\n\")\r\n",
    "\r\n",
    "    with open(TEST_WAV_LIST_LOCATION, \"w\") as file:\r\n",
    "        for stem in test_speakers_wavs_stems:\r\n",
    "            file.write(stem + \"\\n\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get DevSet and TestSet\r\n",
    "Devset and test sets will be generated following the splitted lists generated in the previous sections. Devset will then be split into trainset and evalset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def get_stem_list_from_file(file_path):\r\n",
    "    stem_list = []\r\n",
    "    # The file should have one stem per line\r\n",
    "    with open(file_path, \"r\") as file:\r\n",
    "        for line in file.readlines():\r\n",
    "            line = line.strip()\r\n",
    "            stem_list.append(line)\r\n",
    "    \r\n",
    "    return stem_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "dev_stem_list = get_stem_list_from_file(DEV_WAV_LIST_LOCATION)\r\n",
    "test_stem_list = get_stem_list_from_file(TEST_WAV_LIST_LOCATION)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "devset = BanglaAsrDataset(\r\n",
    "    dataset_dir=BANGLA_ASR_DATASET_DIRECTORY,\r\n",
    "    wav_stem_list=dev_stem_list,\r\n",
    "    tsv_loc = BANGLA_ASR_TSV_LOCATION,\r\n",
    "    target_sample_rate=SAMPLE_RATE,\r\n",
    "    target_num_samples = NUMBER_OF_SAMPLES,\r\n",
    "    trim_amount_time = TRIM_AMOUNT_TIME,\r\n",
    "    device = device\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "testset = BanglaAsrDataset(\r\n",
    "    dataset_dir=BANGLA_ASR_DATASET_DIRECTORY,\r\n",
    "    wav_stem_list=test_stem_list,\r\n",
    "    tsv_loc = BANGLA_ASR_TSV_LOCATION,\r\n",
    "    target_sample_rate=SAMPLE_RATE,\r\n",
    "    target_num_samples = NUMBER_OF_SAMPLES,\r\n",
    "    trim_amount_time = TRIM_AMOUNT_TIME,\r\n",
    "    device = device\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "print(len(dev_stem_list), len(wav_list))\r\n",
    "path_list = stem_list_to_path_list(dev_stem_list, wav_list)\r\n",
    "path_list[0]\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "176467 218703\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'data\\\\BanglaASR\\\\WavFiles\\\\asr_bengali_0\\\\asr_bengali\\\\data\\\\00\\\\000020a912.wav'"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model !"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from torch import nn\r\n",
    "from torchsummary import summary\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import math\r\n",
    "\r\n",
    "import torch.nn.functional as F\r\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## FRM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "class FRM(nn.Module):\r\n",
    "    def __init__(self, nb_dim, do_add = True, do_mul = True):\r\n",
    "        super(FRM, self).__init__()\r\n",
    "        self.fc = nn.Linear(nb_dim, nb_dim)\r\n",
    "        self.sig = nn.Sigmoid()\r\n",
    "        self.do_add = do_add\r\n",
    "        self.do_mul = do_mul\r\n",
    "    def forward(self, x):\r\n",
    "        y = F.adaptive_avg_pool1d(x, 1).view(x.size(0), -1)\r\n",
    "        \r\n",
    "        y = self.sig(self.fc(y)).view(x.size(0), x.size(1), -1)\r\n",
    "\r\n",
    "        if self.do_mul: x = x * y\r\n",
    "        if self.do_add: x = x + y\r\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Residual Block wFRM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "class Residual_block_wFRM(nn.Module):\r\n",
    "    def __init__(self, nb_filts, first = False):\r\n",
    "        super(Residual_block_wFRM, self).__init__()\r\n",
    "        self.first = first\r\n",
    "        if not self.first:\r\n",
    "            self.bn1 = nn.BatchNorm1d(num_features = nb_filts[0])\r\n",
    "        self.lrelu = nn.LeakyReLU()\r\n",
    "        self.lrelu_keras = nn.LeakyReLU(negative_slope=0.3)\r\n",
    "        \r\n",
    "        self.conv1 = nn.Conv1d(in_channels = nb_filts[0],\r\n",
    "            out_channels = nb_filts[1],\r\n",
    "            kernel_size = 3,\r\n",
    "            padding = 1,\r\n",
    "            stride = 1)\r\n",
    "        self.bn2 = nn.BatchNorm1d(num_features = nb_filts[1])\r\n",
    "        self.conv2 = nn.Conv1d(in_channels = nb_filts[1],\r\n",
    "            out_channels = nb_filts[1],\r\n",
    "            padding = 1,\r\n",
    "            kernel_size = 3,\r\n",
    "            stride = 1)\r\n",
    "        \r\n",
    "        if nb_filts[0] != nb_filts[1]:\r\n",
    "            self.downsample = True\r\n",
    "            self.conv_downsample = nn.Conv1d(in_channels = nb_filts[0],\r\n",
    "                out_channels = nb_filts[1],\r\n",
    "                padding = 0,\r\n",
    "                kernel_size = 1,\r\n",
    "                stride = 1)\r\n",
    "            \r\n",
    "        else:\r\n",
    "            self.downsample = False\r\n",
    "        self.mp = nn.MaxPool1d(3)\r\n",
    "        self.frm = FRM(\r\n",
    "            nb_dim = nb_filts[1],\r\n",
    "            do_add = True,\r\n",
    "            do_mul = True)\r\n",
    "        \r\n",
    "    def forward(self, x):\r\n",
    "        identity = x\r\n",
    "        if not self.first:\r\n",
    "            out = self.bn1(x)\r\n",
    "            out = self.lrelu_keras(out)\r\n",
    "        else:\r\n",
    "            out = x\r\n",
    "            \r\n",
    "        out = self.conv1(out)\r\n",
    "        out = self.bn2(out)\r\n",
    "        out = self.lrelu_keras(out)\r\n",
    "        out = self.conv2(out)\r\n",
    "        \r\n",
    "        if self.downsample:\r\n",
    "            identity = self.conv_downsample(identity)\r\n",
    "            \r\n",
    "        out += identity\r\n",
    "        out = self.mp(out)\r\n",
    "        out = self.frm(out)\r\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LayerNorm"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "class LayerNorm(nn.Module):\r\n",
    "\r\n",
    "    def __init__(self, features, eps=1e-6):\r\n",
    "        super(LayerNorm,self).__init__()\r\n",
    "        self.gamma = nn.Parameter(torch.ones(features))\r\n",
    "        self.beta = nn.Parameter(torch.zeros(features))\r\n",
    "        self.eps = eps\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        mean = x.mean(-1, keepdim=True)\r\n",
    "        std = x.std(-1, keepdim=True)\r\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SincConv Fast"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "class SincConv_fast(nn.Module):\r\n",
    "    \"\"\"Sinc-based convolution\r\n",
    "    Parameters\r\n",
    "    ----------\r\n",
    "    in_channels : `int`\r\n",
    "        Number of input channels. Must be 1.\r\n",
    "    out_channels : `int`\r\n",
    "        Number of filters.\r\n",
    "    kernel_size : `int`\r\n",
    "        Filter length.\r\n",
    "    sample_rate : `int`, optional\r\n",
    "        Sample rate. Defaults to 16000.\r\n",
    "    Usage\r\n",
    "    -----\r\n",
    "    See `torch.nn.Conv1d`\r\n",
    "    Reference\r\n",
    "    ---------\r\n",
    "    Mirco Ravanelli, Yoshua Bengio,\r\n",
    "    \"Speaker Recognition from raw waveform with SincNet\".\r\n",
    "    https://arxiv.org/abs/1808.00158\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    @staticmethod\r\n",
    "    def to_mel(hz):\r\n",
    "        return 2595 * np.log10(1 + hz / 700)\r\n",
    "\r\n",
    "    @staticmethod\r\n",
    "    def to_hz(mel):\r\n",
    "        return 700 * (10 ** (mel / 2595) - 1)\r\n",
    "\r\n",
    "    def __init__(self, out_channels, kernel_size, sample_rate=16000, in_channels=1,\r\n",
    "                 stride=1, padding=0, dilation=1, bias=False, groups=1, min_low_hz=50, min_band_hz=50):\r\n",
    "\r\n",
    "        super(SincConv_fast,self).__init__()\r\n",
    "\r\n",
    "        if in_channels != 1:\r\n",
    "            #msg = (f'SincConv only support one input channel '\r\n",
    "            #       f'(here, in_channels = {in_channels:d}).')\r\n",
    "            msg = \"SincConv only support one input channel (here, in_channels = {%i})\" % (in_channels)\r\n",
    "            raise ValueError(msg)\r\n",
    "\r\n",
    "        self.out_channels = out_channels\r\n",
    "        self.kernel_size = kernel_size\r\n",
    "        \r\n",
    "        # Forcing the filters to be odd (i.e, perfectly symmetrics)\r\n",
    "        if kernel_size%2==0:\r\n",
    "            self.kernel_size=self.kernel_size+1\r\n",
    "            \r\n",
    "        self.stride = stride\r\n",
    "        self.padding = padding\r\n",
    "        self.dilation = dilation\r\n",
    "\r\n",
    "        if bias:\r\n",
    "            raise ValueError('SincConv does not support bias.')\r\n",
    "        if groups > 1:\r\n",
    "            raise ValueError('SincConv does not support groups.')\r\n",
    "\r\n",
    "        self.sample_rate = sample_rate\r\n",
    "        self.min_low_hz = min_low_hz\r\n",
    "        self.min_band_hz = min_band_hz\r\n",
    "\r\n",
    "        # initialize filterbanks such that they are equally spaced in Mel scale\r\n",
    "        low_hz = 30\r\n",
    "        high_hz = self.sample_rate / 2 - (self.min_low_hz + self.min_band_hz)\r\n",
    "\r\n",
    "        mel = np.linspace(self.to_mel(low_hz),\r\n",
    "                          self.to_mel(high_hz),\r\n",
    "                          self.out_channels + 1)\r\n",
    "        hz = self.to_hz(mel)\r\n",
    "        \r\n",
    "\r\n",
    "        # filter lower frequency (out_channels, 1)\r\n",
    "        self.low_hz_ = nn.Parameter(torch.Tensor(hz[:-1]).view(-1, 1))\r\n",
    "\r\n",
    "        # filter frequency band (out_channels, 1)\r\n",
    "        self.band_hz_ = nn.Parameter(torch.Tensor(np.diff(hz)).view(-1, 1))\r\n",
    "\r\n",
    "        # Hamming window\r\n",
    "        #self.window_ = torch.hamming_window(self.kernel_size)\r\n",
    "        n_lin=torch.linspace(0, (self.kernel_size/2)-1, steps=int((self.kernel_size/2))) # computing only half of the window\r\n",
    "        self.window_=0.54-0.46*torch.cos(2*math.pi*n_lin/self.kernel_size);\r\n",
    "\r\n",
    "        # (1, kernel_size/2)\r\n",
    "        n = (self.kernel_size - 1) / 2.0\r\n",
    "        self.n_ = 2*math.pi*torch.arange(-n, 0).view(1, -1) / self.sample_rate # Due to symmetry, I only need half of the time axes\r\n",
    "\r\n",
    "    def forward(self, waveforms):\r\n",
    "        \"\"\"\r\n",
    "        Parameters\r\n",
    "        ----------\r\n",
    "        waveforms : `torch.Tensor` (batch_size, 1, n_samples)\r\n",
    "            Batch of waveforms.\r\n",
    "        Returns\r\n",
    "        -------\r\n",
    "        features : `torch.Tensor` (batch_size, out_channels, n_samples_out)\r\n",
    "            Batch of sinc filters activations.\r\n",
    "        \"\"\"\r\n",
    "\r\n",
    "        self.n_ = self.n_.to(waveforms.device)\r\n",
    "\r\n",
    "        self.window_ = self.window_.to(waveforms.device)\r\n",
    "\r\n",
    "        low = self.min_low_hz  + torch.abs(self.low_hz_)\r\n",
    "        \r\n",
    "        high = torch.clamp(low + self.min_band_hz + torch.abs(self.band_hz_),self.min_low_hz,self.sample_rate/2)\r\n",
    "        band=(high-low)[:,0]\r\n",
    "        \r\n",
    "        f_times_t_low = torch.matmul(low, self.n_)\r\n",
    "        f_times_t_high = torch.matmul(high, self.n_)\r\n",
    "\r\n",
    "        band_pass_left=((torch.sin(f_times_t_high)-torch.sin(f_times_t_low))/(self.n_/2))*self.window_ # Equivalent of Eq.4 of the reference paper (SPEAKER RECOGNITION FROM RAW WAVEFORM WITH SINCNET). I just have expanded the sinc and simplified the terms. This way I avoid several useless computations. \r\n",
    "        band_pass_center = 2*band.view(-1,1)\r\n",
    "        band_pass_right= torch.flip(band_pass_left,dims=[1])\r\n",
    "        \r\n",
    "        \r\n",
    "        band_pass=torch.cat([band_pass_left,band_pass_center,band_pass_right],dim=1)\r\n",
    "\r\n",
    "        \r\n",
    "        band_pass = band_pass / (2*band[:,None])\r\n",
    "        \r\n",
    "\r\n",
    "        self.filters = (band_pass).view(\r\n",
    "            self.out_channels, 1, self.kernel_size)\r\n",
    "\r\n",
    "        return F.conv1d(waveforms, self.filters, stride=self.stride,\r\n",
    "                        padding=self.padding, dilation=self.dilation,\r\n",
    "                         bias=None, groups=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RawNet2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "class RawNet2(nn.Module):\r\n",
    "    def __init__(self, d_args):\r\n",
    "        super(RawNet2, self).__init__()\r\n",
    "\r\n",
    "        self.ln = LayerNorm(d_args['nb_samp'])\r\n",
    "        self.first_conv = SincConv_fast(in_channels = d_args['in_channels'],\r\n",
    "            out_channels = d_args['filts'][0],\r\n",
    "            kernel_size = d_args['first_conv']\r\n",
    "            )\r\n",
    "\r\n",
    "        self.first_bn = nn.BatchNorm1d(num_features = d_args['filts'][0])\r\n",
    "        self.lrelu = nn.LeakyReLU()\r\n",
    "        self.lrelu_keras = nn.LeakyReLU(negative_slope = 0.3)\r\n",
    "        \r\n",
    "        self.block0 = nn.Sequential(Residual_block_wFRM(nb_filts = d_args['filts'][1], first = True))\r\n",
    "        self.block1 = nn.Sequential(Residual_block_wFRM(nb_filts = d_args['filts'][1]))\r\n",
    " \r\n",
    "        self.block2 = nn.Sequential(Residual_block_wFRM(nb_filts = d_args['filts'][2]))\r\n",
    "        d_args['filts'][2][0] = d_args['filts'][2][1]\r\n",
    "        self.block3 = nn.Sequential(Residual_block_wFRM(nb_filts = d_args['filts'][2]))\r\n",
    "        self.block4 = nn.Sequential(Residual_block_wFRM(nb_filts = d_args['filts'][2]))\r\n",
    "        self.block5 = nn.Sequential(Residual_block_wFRM(nb_filts = d_args['filts'][2]))\r\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\r\n",
    "\r\n",
    "        self.bn_before_gru = nn.BatchNorm1d(num_features = d_args['filts'][2][-1])\r\n",
    "        self.gru = nn.GRU(input_size = d_args['filts'][2][-1],\r\n",
    "            hidden_size = d_args['gru_node'],\r\n",
    "            num_layers = d_args['nb_gru_layer'],\r\n",
    "            batch_first = True)\r\n",
    "\r\n",
    "        \r\n",
    "        self.fc1_gru = nn.Linear(in_features = d_args['gru_node'],\r\n",
    "            out_features = d_args['nb_fc_node'])\r\n",
    "        self.fc2_gru = nn.Linear(in_features = d_args['nb_fc_node'],\r\n",
    "            out_features = d_args['nb_classes'],\r\n",
    "            bias = True)\r\n",
    "        \r\n",
    "        self.sig = nn.Sigmoid()\r\n",
    "        \r\n",
    "    def forward(self, x, y = 0, is_test=False):\r\n",
    "        #follow sincNet recipe\r\n",
    "        nb_samp = x.shape[0]\r\n",
    "        len_seq = x.shape[1]\r\n",
    "        x = self.ln(x)\r\n",
    "        x=x.view(nb_samp,1,len_seq)\r\n",
    "        x = F.max_pool1d(torch.abs(self.first_conv(x)), 3)\r\n",
    "        x = self.first_bn(x)\r\n",
    "        x = self.lrelu_keras(x)\r\n",
    "        \r\n",
    "        x = self.block0(x)\r\n",
    "        x = self.block1(x)\r\n",
    "\r\n",
    "        x = self.block2(x)\r\n",
    "        x = self.block3(x)\r\n",
    "        x = self.block4(x)\r\n",
    "        x = self.block5(x)\r\n",
    "\r\n",
    "        x = self.bn_before_gru(x)\r\n",
    "        x = self.lrelu_keras(x)\r\n",
    "        x = x.permute(0, 2, 1)  #(batch, filt, time) >> (batch, time, filt)\r\n",
    "        self.gru.flatten_parameters()\r\n",
    "        x, _ = self.gru(x)\r\n",
    "        x = x[:,-1,:]\r\n",
    "        code = self.fc1_gru(x)\r\n",
    "        if is_test: return code\r\n",
    "        \r\n",
    "        code_norm = code.norm(p=2,dim=1, keepdim=True) / 10.\r\n",
    "        code = torch.div(code, code_norm)\r\n",
    "        out = self.fc2_gru(code)\r\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "model_dict = {}\r\n",
    "model_dict['nb_classes'] = 508\r\n",
    "model_dict['first_conv'] = 251\r\n",
    "model_dict['in_channels'] = 1\r\n",
    "model_dict['filts'] = [128, [128,128], [128,256], [256,256]]\r\n",
    "model_dict['m_blocks'] = [2, 4]\r\n",
    "model_dict['nb_fc_att_node'] =[1]\r\n",
    "model_dict['nb_fc_node'] = 1024\r\n",
    "model_dict['gru_node'] = 1024\r\n",
    "model_dict['nb_gru_layer'] = 1\r\n",
    "model_dict['nb_samp'] = NUMBER_OF_SAMPLES"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "LEARNING_RATE = 1e-3\r\n",
    "WEIGHT_DECAY = 1e-4\r\n",
    "AMSGRAD = True\r\n",
    "EPOCHS = 5\r\n",
    "BATCH_SIZE = 2\r\n",
    "\r\n",
    "# Higher number may cause errors in notebook\r\n",
    "NUMBER_OF_WORKERS = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "bangla_asr_data_loader = data.DataLoader(bangla_asr_dataset,\r\n",
    "            batch_size = BATCH_SIZE, \r\n",
    "            shuffle = False,\r\n",
    "            drop_last = False,\r\n",
    "            num_workers = NUMBER_OF_WORKERS)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Batch explained:\r\n",
    "\r\n",
    "If batch size = 4\r\n",
    "\r\n",
    "One batch = [ tensor([[[x,x,x]]], [[x,x,x]], [[x,x,x]], [[x,x,x]]])    , (label1, label2, label3, label4) ]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "model = RawNet2(model_dict)\r\n",
    "\r\n",
    "model.to(device)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RawNet2(\n",
       "  (ln): LayerNorm()\n",
       "  (first_conv): SincConv_fast()\n",
       "  (first_bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "  (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "  (block0): Sequential(\n",
       "    (0): Residual_block_wFRM(\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "      (frm): FRM(\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (sig): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block1): Sequential(\n",
       "    (0): Residual_block_wFRM(\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "      (frm): FRM(\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (sig): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block2): Sequential(\n",
       "    (0): Residual_block_wFRM(\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv_downsample): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "      (frm): FRM(\n",
       "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (sig): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block3): Sequential(\n",
       "    (0): Residual_block_wFRM(\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "      (frm): FRM(\n",
       "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (sig): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block4): Sequential(\n",
       "    (0): Residual_block_wFRM(\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "      (frm): FRM(\n",
       "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (sig): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block5): Sequential(\n",
       "    (0): Residual_block_wFRM(\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "      (frm): FRM(\n",
       "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (sig): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (bn_before_gru): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (gru): GRU(256, 1024, batch_first=True)\n",
       "  (fc1_gru): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (fc2_gru): Linear(in_features=1024, out_features=508, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "model(bangla_asr_dataset[0][0]).shape"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\molla\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 508])"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "def keras_lr_decay(step, decay = 0.0001):\r\n",
    "    return 1./(1. + decay * step)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "params = [\r\n",
    "    {\r\n",
    "        'params': [\r\n",
    "            param for name, param in model.named_parameters()\r\n",
    "            if 'bn' not in name\r\n",
    "        ]\r\n",
    "    },\r\n",
    "    {\r\n",
    "        'params': [\r\n",
    "            param for name, param in model.named_parameters()\r\n",
    "            if 'bn' in name\r\n",
    "        ],\r\n",
    "        'weight_decay':\r\n",
    "        0\r\n",
    "    },\r\n",
    "]\r\n",
    "\r\n",
    "criterion = {}\r\n",
    "criterion['cce'] = nn.CrossEntropyLoss()\r\n",
    "\r\n",
    "optimizer = torch.optim.Adam(params,\r\n",
    "            lr = LEARNING_RATE,\r\n",
    "            weight_decay = WEIGHT_DECAY,\r\n",
    "            amsgrad = AMSGRAD)\r\n",
    "\r\n",
    "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda step: keras_lr_decay(step))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "def train_model(model, db_gen, optimizer, epoch, args, device, lr_scheduler, criterion):\r\n",
    "    model.train()\r\n",
    "    with tqdm(total = len(db_gen), ncols = 70) as pbar:\r\n",
    "        for m_batch, m_label in db_gen:\r\n",
    "            \r\n",
    "            m_batch, m_label = m_batch.to(device), m_label.to(device)\r\n",
    "\r\n",
    "            output = model(m_batch, m_label)\r\n",
    "            cce_loss = criterion['cce'](output, m_label)\r\n",
    "            loss = cce_loss\r\n",
    "\r\n",
    "            optimizer.zero_grad()\r\n",
    "            loss.backward()\r\n",
    "            optimizer.step()\r\n",
    "            pbar.set_description('epoch: %d, cce:%.3f'%(epoch, cce_loss))\r\n",
    "            pbar.update(1)\r\n",
    "            if args.do_lr_decay:\r\n",
    "                if args.lr_decay == 'keras': lr_scheduler.step()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "for epoch in tqdm(range(EPOCHS)):\r\n",
    "    train_model(model = model,\r\n",
    "        db_gen = bangla_asr_data_loader,\r\n",
    "        args = model_dict,\r\n",
    "        optimizer = optimizer,\r\n",
    "        lr_scheduler = lr_scheduler,\r\n",
    "        criterion = criterion,\r\n",
    "        device = device,\r\n",
    "        epoch = epoch)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|                                      | 0/109352 [00:00<?, ?it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'to'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-95ea345652d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         epoch = epoch)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-23-e9fa8b90e213>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, db_gen, optimizer, epoch, args, device, lr_scheduler, criterion)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mm_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm_label\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdb_gen\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m             \u001b[0mm_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm_label\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'to'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('pytorch': conda)"
  },
  "interpreter": {
   "hash": "83fea67622c0fa859a960c0bf8a89199ff21c56f3cdd300f29516e12427a6ea7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}