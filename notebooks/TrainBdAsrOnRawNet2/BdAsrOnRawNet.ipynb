{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TrainRawNet2: On BD ASR Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Necessary Imports:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\r\n",
    "from torch.utils import data\r\n",
    "\r\n",
    "import torchaudio\r\n",
    "\r\n",
    "import csv\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "import glob\r\n",
    "from pathlib import Path\r\n",
    "\r\n",
    "import os"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Directories and lcoations:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Directories are assumed to have a trailing '/' or '\\\\' in all the subsequent code\r\n",
    "\r\n",
    "CURRENT_WORKING_DIRECTORY = \"W:/SpeakerRecognitionResearch\"\r\n",
    "\r\n",
    "BANGLA_ASR_DATASET_DIRECTORY = \"data/BanglaASR/WavFiles/\"\r\n",
    "BANGLA_ASR_TSV_LOCATION = \"data/BanglaASR/utt_spk_text.tsv\"\r\n",
    "\r\n",
    "# To avoid file location related errors, we make sure \"SpeakerRecognitionResearch\" root folder is the current working directory.\r\n",
    "os.chdir(CURRENT_WORKING_DIRECTORY)\r\n",
    "os.getcwd()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'W:\\\\SpeakerRecognitionResearch'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "MODEL_SAVE_DIRECTORY = \"notebooks/TrainBdAsrOnRawNet2/out\"\r\n",
    "MODEL_LOSS_EER_OUTPUT_FILE = \"notebooks/TrainBdAsrOnRawNet2/out/loss_eer.txt\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Constants:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# If sample_rate = 16K and number_of_samples = 32000, then each tensor will be equivalent to 2 seconds of data\r\n",
    "SAMPLE_RATE = 16000\r\n",
    "NUMBER_OF_SAMPLES = 32000\r\n",
    "\r\n",
    "# Bangla ASR Dataset has around half of second of silence in the beginning\r\n",
    "# This constant will be used to cut samples from the left of the audio\r\n",
    "TRIM_AMOUNT_TIME = 0.5"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Device\r\n",
    "\r\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n",
    "print(\"Using {}.\".format(device))\r\n",
    "if device==\"cuda\": print(torch.cuda.get_device_name(0))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cuda.\n",
      "NVIDIA GeForce GTX 1050\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Dataset preparation files\r\n",
    "\r\n",
    "These files were generated by the prepareDataset notebook."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "TRAINSET_LIST_LOCATION = \"notebooks/TrainBdAsrOnRawNet2/DatasetPreparations/trainset_list.txt\"\r\n",
    "EVALSET_LIST_LOCATION = \"notebooks/TrainBdAsrOnRawNet2/DatasetPreparations/evalset_list.txt\"\r\n",
    "TESTSET_LIST_LOCATION = \"notebooks/TrainBdAsrOnRawNet2/DatasetPreparations/testset_list.txt\"\r\n",
    "DEVSET_CLASS_ORDER_LOCATION = \"notebooks/TrainBdAsrOnRawNet2/DatasetPreparations/devset_classes.txt\"\r\n",
    "TESTSET_CLASS_ORDER_LOCATION = \"notebooks/TrainBdAsrOnRawNet2/DatasetPreparations/testset_classes.txt\"\r\n",
    "\r\n",
    "EVAL_TRIALS_LOCATION = \"notebooks/TrainBdAsrOnRawNet2/DatasetPreparations/eval_trials.txt\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def load_list_from_file(file_loc, trial_file=False):\r\n",
    "    list_ = []\r\n",
    "    if not trial_file:\r\n",
    "        with open(file_loc, \"r\") as file:\r\n",
    "            for line in file.readlines():\r\n",
    "                list_.append(line.strip())\r\n",
    "    else:\r\n",
    "        # The trial file has 3 elements in each line\r\n",
    "        with open(file_loc, \"r\") as file:\r\n",
    "            for line in file.readlines():\r\n",
    "                expected, utt1, utt2 = line.strip().split(\" \")\r\n",
    "                list_.append( (expected, utt1, utt2) )\r\n",
    "    return list_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "trainset_stems = load_list_from_file(TRAINSET_LIST_LOCATION)\r\n",
    "evalset_stems = load_list_from_file(EVALSET_LIST_LOCATION)\r\n",
    "testset_stems = load_list_from_file(TESTSET_LIST_LOCATION)\r\n",
    "\r\n",
    "devset_classes = load_list_from_file(DEVSET_CLASS_ORDER_LOCATION)\r\n",
    "testset_classes = load_list_from_file(TESTSET_CLASS_ORDER_LOCATION)\r\n",
    "\r\n",
    "eval_trials = load_list_from_file(EVAL_TRIALS_LOCATION, trial_file=True)\r\n",
    "\r\n",
    "len(trainset_stems) + len(evalset_stems) + len(testset_stems)\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "218703"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Custom dataset for Bangla ASR\r\n",
    "\r\n",
    "This custom dataset is written with the assumption that the Dataset has been already converted into wav format. Check evaluate_asr_ds.ipynb notebook for conversion method.\r\n",
    "\r\n",
    "Strategy:\r\n",
    "1. train_set will be strictly used for training.\r\n",
    "2. eval_set will be used for evaluation during training.\r\n",
    "3. test_set will be used to assess the model after training.\r\n",
    "\r\n",
    "Dev indicates data used during the whole training process, which indicates both train and eval datasets.\r\n",
    "\r\n",
    "Each set will have entirely different speakers (classes).\r\n",
    "\r\n",
    "A stem is the name of the file. I.E: \"a/b/c/bat.jpg\" --stem--> bat"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "class BanglaAsrDataset(data.Dataset):\r\n",
    "    def __init__(self, dataset_dir, wav_stem_list, tsv_loc, target_sample_rate, target_num_samples, trim_amount_time, device, is_evalset=False):\r\n",
    "\r\n",
    "        # wav_stem_list will have a list of stems that will be included in this dataset\r\n",
    "\r\n",
    "        tsv_dataframe = pd.read_csv(tsv_loc, quoting=csv.QUOTE_NONE, sep='\\t', header=None)\r\n",
    "\r\n",
    "        # The TSV file contains speech annotations in the third column.\r\n",
    "        # We don't need the annotations, so we drop the column\r\n",
    "        tsv_dataframe = tsv_dataframe.iloc[:,:-1]\r\n",
    "\r\n",
    "        self.dataset_dir = dataset_dir\r\n",
    "        self.stem_to_spk_mapping = dict(sorted(tsv_dataframe.values.tolist()))\r\n",
    "        self.wav_path_list = self._stem_list_to_path_list(wav_stem_list)\r\n",
    "        self.target_sample_rate = target_sample_rate\r\n",
    "        self.target_num_samples = target_num_samples\r\n",
    "        self.trim_amount_time = trim_amount_time\r\n",
    "        self.device = device\r\n",
    "        self.is_evalset=is_evalset\r\n",
    "\r\n",
    "        # Set will ensure uniquness\r\n",
    "        # sorted list will make the order consistent\r\n",
    "        # Tuple will make sure the order doesn't change\r\n",
    "        self.speakers_list = tuple(sorted(list(set([self.stem_to_spk_mapping[stem] for stem in wav_stem_list]))))\r\n",
    "        \r\n",
    "    def _get_audio_path_list(self, dataset_dir):\r\n",
    "        \r\n",
    "        pattern = '**/*.wav'\r\n",
    "        files = glob.glob(self.dataset_dir + pattern , recursive=True)\r\n",
    "\r\n",
    "        # Normalize the file paths. To get file paths with '/' or '\\\\' consistently depending on OS\r\n",
    "        wav_list = [os.path.normpath(i) for i in files]\r\n",
    "\r\n",
    "        return wav_list\r\n",
    "\r\n",
    "    def _stem_list_to_path_list(self, stem_list):\r\n",
    "        # Sets are faster to search\r\n",
    "        stem_list = set(stem_list)\r\n",
    "        all_path_list = self._get_audio_path_list(self.dataset_dir)\r\n",
    "        path_list = [path for path in all_path_list if Path(path).stem in stem_list]\r\n",
    "        return path_list\r\n",
    "\r\n",
    "\r\n",
    "    def _resample_to_target_sr(self, signal, sample_rate):\r\n",
    "        if sample_rate != self.target_sample_rate:\r\n",
    "            resampler = torchaudio.transforms.Resample(sample_rate, self.target_sample_rate)\r\n",
    "            signal = resampler(signal)\r\n",
    "        return signal\r\n",
    "\r\n",
    "    def _mix_down_to_mono(self, signal):\r\n",
    "        if signal.shape[0] > 1:\r\n",
    "            signal = torch.mean(siggnal, dim=0, keepdim=True)\r\n",
    "        return signal\r\n",
    "\r\n",
    "    def _trim(self, signal):\r\n",
    "        total_samples = signal.shape[-1]\r\n",
    "\r\n",
    "        # We cut a fixed amount on the left side if the signal is big enough\r\n",
    "        trim_samples_amount = int(self.target_sample_rate * self.trim_amount_time)\r\n",
    "\r\n",
    "        if total_samples >= trim_samples_amount + self.target_num_samples:\r\n",
    "            signal = signal[: , trim_samples_amount:]\r\n",
    "            total_samples = signal.shape[-1]\r\n",
    "\r\n",
    "        # We cut from the right side if the signal is too big\r\n",
    "        if total_samples > self.target_num_samples:\r\n",
    "            signal = signal[:, :self.target_num_samples]\r\n",
    "        \r\n",
    "        # We add zero padding on the right if signal is too small\r\n",
    "        if total_samples < self.target_num_samples:\r\n",
    "            num_missing_samples = self.target_num_samples - total_samples\r\n",
    "            last_dim_padding = (0, num_missing_samples)\r\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\r\n",
    "            \r\n",
    "        return signal\r\n",
    "\r\n",
    "    def _normalize_like_sincnet(self, signal):\r\n",
    "        return signal/torch.max(torch.abs(signal))\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.wav_path_list)\r\n",
    "\r\n",
    "    def __getitem__(self, index):\r\n",
    "        wav_path = self.wav_path_list[index]\r\n",
    "        wav_name = Path(wav_path).stem\r\n",
    "        label = self.stem_to_spk_mapping[wav_name]\r\n",
    "\r\n",
    "        signal, sample_rate = torchaudio.load(wav_path)\r\n",
    "\r\n",
    "        # moving to CPU/CUDA is now done in the training phase\r\n",
    "        # signal = signal.to(self.device)\r\n",
    "\r\n",
    "        signal = self._resample_to_target_sr(signal, sample_rate)\r\n",
    "        signal = self._mix_down_to_mono(signal)\r\n",
    "\r\n",
    "        signal =  self._trim(signal)\r\n",
    "        signal = self._normalize_like_sincnet(signal)\r\n",
    "\r\n",
    "        # signal = signal.squeeze(0)\r\n",
    "\r\n",
    "        label_index = self.speakers_list.index(label)\r\n",
    "\r\n",
    "        if self.is_evalset:\r\n",
    "            return signal, label_index, wav_path\r\n",
    "        else:\r\n",
    "            return signal, label_index\r\n",
    "\r\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "trainset = BanglaAsrDataset(\r\n",
    "    dataset_dir=BANGLA_ASR_DATASET_DIRECTORY,\r\n",
    "    wav_stem_list=trainset_stems,\r\n",
    "    tsv_loc = BANGLA_ASR_TSV_LOCATION,\r\n",
    "    target_sample_rate=SAMPLE_RATE,\r\n",
    "    target_num_samples = NUMBER_OF_SAMPLES,\r\n",
    "    trim_amount_time = TRIM_AMOUNT_TIME,\r\n",
    "    device = device\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "evalset = BanglaAsrDataset(\r\n",
    "    dataset_dir=BANGLA_ASR_DATASET_DIRECTORY,\r\n",
    "    wav_stem_list=evalset_stems,\r\n",
    "    tsv_loc = BANGLA_ASR_TSV_LOCATION,\r\n",
    "    target_sample_rate=SAMPLE_RATE,\r\n",
    "    target_num_samples = NUMBER_OF_SAMPLES,\r\n",
    "    trim_amount_time = TRIM_AMOUNT_TIME,\r\n",
    "    device = device,\r\n",
    "    is_evalset=True\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "trainset[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[ 0.0045,  0.0053,  0.0050,  ..., -0.0040, -0.0044, -0.0065]]), 250)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model !"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "from torch import nn\r\n",
    "from torchsummary import summary\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import math\r\n",
    "\r\n",
    "import torch.nn.functional as F\r\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## FRM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "class FRM(nn.Module):\r\n",
    "    def __init__(self, nb_dim, do_add = True, do_mul = True):\r\n",
    "        super(FRM, self).__init__()\r\n",
    "        self.fc = nn.Linear(nb_dim, nb_dim)\r\n",
    "        self.sig = nn.Sigmoid()\r\n",
    "        self.do_add = do_add\r\n",
    "        self.do_mul = do_mul\r\n",
    "    def forward(self, x):\r\n",
    "        y = F.adaptive_avg_pool1d(x, 1).view(x.size(0), -1)\r\n",
    "        \r\n",
    "        y = self.sig(self.fc(y)).view(x.size(0), x.size(1), -1)\r\n",
    "\r\n",
    "        if self.do_mul: x = x * y\r\n",
    "        if self.do_add: x = x + y\r\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Residual Block wFRM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "class Residual_block_wFRM(nn.Module):\r\n",
    "    def __init__(self, nb_filts, first = False):\r\n",
    "        super(Residual_block_wFRM, self).__init__()\r\n",
    "        self.first = first\r\n",
    "        if not self.first:\r\n",
    "            self.bn1 = nn.BatchNorm1d(num_features = nb_filts[0])\r\n",
    "        self.lrelu = nn.LeakyReLU()\r\n",
    "        self.lrelu_keras = nn.LeakyReLU(negative_slope=0.3)\r\n",
    "        \r\n",
    "        self.conv1 = nn.Conv1d(in_channels = nb_filts[0],\r\n",
    "            out_channels = nb_filts[1],\r\n",
    "            kernel_size = 3,\r\n",
    "            padding = 1,\r\n",
    "            stride = 1)\r\n",
    "        self.bn2 = nn.BatchNorm1d(num_features = nb_filts[1])\r\n",
    "        self.conv2 = nn.Conv1d(in_channels = nb_filts[1],\r\n",
    "            out_channels = nb_filts[1],\r\n",
    "            padding = 1,\r\n",
    "            kernel_size = 3,\r\n",
    "            stride = 1)\r\n",
    "        \r\n",
    "        if nb_filts[0] != nb_filts[1]:\r\n",
    "            self.downsample = True\r\n",
    "            self.conv_downsample = nn.Conv1d(in_channels = nb_filts[0],\r\n",
    "                out_channels = nb_filts[1],\r\n",
    "                padding = 0,\r\n",
    "                kernel_size = 1,\r\n",
    "                stride = 1)\r\n",
    "            \r\n",
    "        else:\r\n",
    "            self.downsample = False\r\n",
    "        self.mp = nn.MaxPool1d(3)\r\n",
    "        self.frm = FRM(\r\n",
    "            nb_dim = nb_filts[1],\r\n",
    "            do_add = True,\r\n",
    "            do_mul = True)\r\n",
    "        \r\n",
    "    def forward(self, x):\r\n",
    "        identity = x\r\n",
    "        if not self.first:\r\n",
    "            out = self.bn1(x)\r\n",
    "            out = self.lrelu_keras(out)\r\n",
    "        else:\r\n",
    "            out = x\r\n",
    "            \r\n",
    "        out = self.conv1(out)\r\n",
    "        out = self.bn2(out)\r\n",
    "        out = self.lrelu_keras(out)\r\n",
    "        out = self.conv2(out)\r\n",
    "        \r\n",
    "        if self.downsample:\r\n",
    "            identity = self.conv_downsample(identity)\r\n",
    "            \r\n",
    "        out += identity\r\n",
    "        out = self.mp(out)\r\n",
    "        out = self.frm(out)\r\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LayerNorm"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "class LayerNorm(nn.Module):\r\n",
    "\r\n",
    "    def __init__(self, features, eps=1e-6):\r\n",
    "        super(LayerNorm,self).__init__()\r\n",
    "        self.gamma = nn.Parameter(torch.ones(features))\r\n",
    "        self.beta = nn.Parameter(torch.zeros(features))\r\n",
    "        self.eps = eps\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        mean = x.mean(-1, keepdim=True)\r\n",
    "        std = x.std(-1, keepdim=True)\r\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SincConv Fast"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "class SincConv_fast(nn.Module):\r\n",
    "    \"\"\"Sinc-based convolution\r\n",
    "    Parameters\r\n",
    "    ----------\r\n",
    "    in_channels : `int`\r\n",
    "        Number of input channels. Must be 1.\r\n",
    "    out_channels : `int`\r\n",
    "        Number of filters.\r\n",
    "    kernel_size : `int`\r\n",
    "        Filter length.\r\n",
    "    sample_rate : `int`, optional\r\n",
    "        Sample rate. Defaults to 16000.\r\n",
    "    Usage\r\n",
    "    -----\r\n",
    "    See `torch.nn.Conv1d`\r\n",
    "    Reference\r\n",
    "    ---------\r\n",
    "    Mirco Ravanelli, Yoshua Bengio,\r\n",
    "    \"Speaker Recognition from raw waveform with SincNet\".\r\n",
    "    https://arxiv.org/abs/1808.00158\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    @staticmethod\r\n",
    "    def to_mel(hz):\r\n",
    "        return 2595 * np.log10(1 + hz / 700)\r\n",
    "\r\n",
    "    @staticmethod\r\n",
    "    def to_hz(mel):\r\n",
    "        return 700 * (10 ** (mel / 2595) - 1)\r\n",
    "\r\n",
    "    def __init__(self, out_channels, kernel_size, sample_rate=16000, in_channels=1,\r\n",
    "                 stride=1, padding=0, dilation=1, bias=False, groups=1, min_low_hz=50, min_band_hz=50):\r\n",
    "\r\n",
    "        super(SincConv_fast,self).__init__()\r\n",
    "\r\n",
    "        if in_channels != 1:\r\n",
    "            #msg = (f'SincConv only support one input channel '\r\n",
    "            #       f'(here, in_channels = {in_channels:d}).')\r\n",
    "            msg = \"SincConv only support one input channel (here, in_channels = {%i})\" % (in_channels)\r\n",
    "            raise ValueError(msg)\r\n",
    "\r\n",
    "        self.out_channels = out_channels\r\n",
    "        self.kernel_size = kernel_size\r\n",
    "        \r\n",
    "        # Forcing the filters to be odd (i.e, perfectly symmetrics)\r\n",
    "        if kernel_size%2==0:\r\n",
    "            self.kernel_size=self.kernel_size+1\r\n",
    "            \r\n",
    "        self.stride = stride\r\n",
    "        self.padding = padding\r\n",
    "        self.dilation = dilation\r\n",
    "\r\n",
    "        if bias:\r\n",
    "            raise ValueError('SincConv does not support bias.')\r\n",
    "        if groups > 1:\r\n",
    "            raise ValueError('SincConv does not support groups.')\r\n",
    "\r\n",
    "        self.sample_rate = sample_rate\r\n",
    "        self.min_low_hz = min_low_hz\r\n",
    "        self.min_band_hz = min_band_hz\r\n",
    "\r\n",
    "        # initialize filterbanks such that they are equally spaced in Mel scale\r\n",
    "        low_hz = 30\r\n",
    "        high_hz = self.sample_rate / 2 - (self.min_low_hz + self.min_band_hz)\r\n",
    "\r\n",
    "        mel = np.linspace(self.to_mel(low_hz),\r\n",
    "                          self.to_mel(high_hz),\r\n",
    "                          self.out_channels + 1)\r\n",
    "        hz = self.to_hz(mel)\r\n",
    "        \r\n",
    "\r\n",
    "        # filter lower frequency (out_channels, 1)\r\n",
    "        self.low_hz_ = nn.Parameter(torch.Tensor(hz[:-1]).view(-1, 1))\r\n",
    "\r\n",
    "        # filter frequency band (out_channels, 1)\r\n",
    "        self.band_hz_ = nn.Parameter(torch.Tensor(np.diff(hz)).view(-1, 1))\r\n",
    "\r\n",
    "        # Hamming window\r\n",
    "        #self.window_ = torch.hamming_window(self.kernel_size)\r\n",
    "        n_lin=torch.linspace(0, (self.kernel_size/2)-1, steps=int((self.kernel_size/2))) # computing only half of the window\r\n",
    "        self.window_=0.54-0.46*torch.cos(2*math.pi*n_lin/self.kernel_size);\r\n",
    "\r\n",
    "        # (1, kernel_size/2)\r\n",
    "        n = (self.kernel_size - 1) / 2.0\r\n",
    "        self.n_ = 2*math.pi*torch.arange(-n, 0).view(1, -1) / self.sample_rate # Due to symmetry, I only need half of the time axes\r\n",
    "\r\n",
    "    def forward(self, waveforms):\r\n",
    "        \"\"\"\r\n",
    "        Parameters\r\n",
    "        ----------\r\n",
    "        waveforms : `torch.Tensor` (batch_size, 1, n_samples)\r\n",
    "            Batch of waveforms.\r\n",
    "        Returns\r\n",
    "        -------\r\n",
    "        features : `torch.Tensor` (batch_size, out_channels, n_samples_out)\r\n",
    "            Batch of sinc filters activations.\r\n",
    "        \"\"\"\r\n",
    "\r\n",
    "        self.n_ = self.n_.to(waveforms.device)\r\n",
    "\r\n",
    "        self.window_ = self.window_.to(waveforms.device)\r\n",
    "\r\n",
    "        low = self.min_low_hz  + torch.abs(self.low_hz_)\r\n",
    "        \r\n",
    "        high = torch.clamp(low + self.min_band_hz + torch.abs(self.band_hz_),self.min_low_hz,self.sample_rate/2)\r\n",
    "        band=(high-low)[:,0]\r\n",
    "        \r\n",
    "        f_times_t_low = torch.matmul(low, self.n_)\r\n",
    "        f_times_t_high = torch.matmul(high, self.n_)\r\n",
    "\r\n",
    "        band_pass_left=((torch.sin(f_times_t_high)-torch.sin(f_times_t_low))/(self.n_/2))*self.window_ # Equivalent of Eq.4 of the reference paper (SPEAKER RECOGNITION FROM RAW WAVEFORM WITH SINCNET). I just have expanded the sinc and simplified the terms. This way I avoid several useless computations. \r\n",
    "        band_pass_center = 2*band.view(-1,1)\r\n",
    "        band_pass_right= torch.flip(band_pass_left,dims=[1])\r\n",
    "        \r\n",
    "        \r\n",
    "        band_pass=torch.cat([band_pass_left,band_pass_center,band_pass_right],dim=1)\r\n",
    "\r\n",
    "        \r\n",
    "        band_pass = band_pass / (2*band[:,None])\r\n",
    "        \r\n",
    "\r\n",
    "        self.filters = (band_pass).view(\r\n",
    "            self.out_channels, 1, self.kernel_size)\r\n",
    "\r\n",
    "        return F.conv1d(waveforms, self.filters, stride=self.stride,\r\n",
    "                        padding=self.padding, dilation=self.dilation,\r\n",
    "                         bias=None, groups=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RawNet2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "class RawNet2(nn.Module):\r\n",
    "    def __init__(self, d_args):\r\n",
    "        super(RawNet2, self).__init__()\r\n",
    "\r\n",
    "        self.ln = LayerNorm(d_args['nb_samp'])\r\n",
    "        self.first_conv = SincConv_fast(in_channels = d_args['in_channels'],\r\n",
    "            out_channels = d_args['filts'][0],\r\n",
    "            kernel_size = d_args['first_conv']\r\n",
    "            )\r\n",
    "        \r\n",
    "        self.first_bn = nn.BatchNorm1d(num_features = d_args['filts'][0])\r\n",
    "        self.lrelu = nn.LeakyReLU()\r\n",
    "        self.lrelu_keras = nn.LeakyReLU(negative_slope = 0.3)\r\n",
    "        \r\n",
    "        self.block0 = nn.Sequential(Residual_block_wFRM(nb_filts = d_args['filts'][1], first = True))\r\n",
    "        self.block1 = nn.Sequential(Residual_block_wFRM(nb_filts = d_args['filts'][1]))\r\n",
    " \r\n",
    "        self.block2 = nn.Sequential(Residual_block_wFRM(nb_filts = d_args['filts'][2]))\r\n",
    "        d_args['filts'][2][0] = d_args['filts'][2][1]\r\n",
    "        self.block3 = nn.Sequential(Residual_block_wFRM(nb_filts = d_args['filts'][2]))\r\n",
    "        self.block4 = nn.Sequential(Residual_block_wFRM(nb_filts = d_args['filts'][2]))\r\n",
    "        self.block5 = nn.Sequential(Residual_block_wFRM(nb_filts = d_args['filts'][2]))\r\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\r\n",
    "\r\n",
    "        self.bn_before_gru = nn.BatchNorm1d(num_features = d_args['filts'][2][-1])\r\n",
    "        self.gru = nn.GRU(input_size = d_args['filts'][2][-1],\r\n",
    "            hidden_size = d_args['gru_node'],\r\n",
    "            num_layers = d_args['nb_gru_layer'],\r\n",
    "            batch_first = True)\r\n",
    "\r\n",
    "        \r\n",
    "        self.fc1_gru = nn.Linear(in_features = d_args['gru_node'],\r\n",
    "            out_features = d_args['nb_fc_node'])\r\n",
    "        self.fc2_gru = nn.Linear(in_features = d_args['nb_fc_node'],\r\n",
    "            out_features = d_args['nb_classes'],\r\n",
    "            bias = True)\r\n",
    "        \r\n",
    "        self.sig = nn.Sigmoid()\r\n",
    "        \r\n",
    "    def forward(self, x, y = 0, is_test=False):\r\n",
    "        #follow sincNet recipe\r\n",
    "        nb_samp = x.shape[0]\r\n",
    "        len_seq = x.shape[1]\r\n",
    "\r\n",
    "        x = self.ln(x)\r\n",
    "        x=x.view(nb_samp,1,len_seq)\r\n",
    "        x = F.max_pool1d(torch.abs(self.first_conv(x)), 3)\r\n",
    "        x = self.first_bn(x)\r\n",
    "        x = self.lrelu_keras(x)\r\n",
    "        \r\n",
    "        x = self.block0(x)\r\n",
    "        x = self.block1(x)\r\n",
    "\r\n",
    "        x = self.block2(x)\r\n",
    "        x = self.block3(x)\r\n",
    "        x = self.block4(x)\r\n",
    "        x = self.block5(x)\r\n",
    "\r\n",
    "        x = self.bn_before_gru(x)\r\n",
    "        x = self.lrelu_keras(x)\r\n",
    "        x = x.permute(0, 2, 1)  #(batch, filt, time) >> (batch, time, filt)\r\n",
    "        self.gru.flatten_parameters()\r\n",
    "        x, _ = self.gru(x)\r\n",
    "        x = x[:,-1,:]\r\n",
    "        code = self.fc1_gru(x)\r\n",
    "        if is_test: return code\r\n",
    "        \r\n",
    "        code_norm = code.norm(p=2,dim=1, keepdim=True) / 10.\r\n",
    "        code = torch.div(code, code_norm)\r\n",
    "        out = self.fc2_gru(code)\r\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "from sklearn.metrics import roc_curve\r\n",
    "from scipy.optimize import brentq\r\n",
    "from scipy.interpolate import interp1d"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "model_dict = {}\r\n",
    "model_dict['nb_classes'] = len(trainset.speakers_list)\r\n",
    "model_dict['first_conv'] = 251\r\n",
    "model_dict['in_channels'] = 1\r\n",
    "model_dict['filts'] = [128, [128,128], [128,128], [256,256]]\r\n",
    "model_dict['m_blocks'] = [2, 4]\r\n",
    "model_dict['nb_fc_att_node'] =[1]\r\n",
    "model_dict['nb_fc_node'] = 1024\r\n",
    "model_dict['gru_node'] = 1024\r\n",
    "model_dict['nb_gru_layer'] = 1\r\n",
    "model_dict['nb_samp'] = NUMBER_OF_SAMPLES\r\n",
    "\r\n",
    "model_dict['lr_decay'] = \"keras\"\r\n",
    "model_dict['do_lr_decay'] = True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "LEARNING_RATE = 1e-3\r\n",
    "WEIGHT_DECAY = 1e-4\r\n",
    "AMSGRAD = True\r\n",
    "EPOCHS = 5\r\n",
    "BATCH_SIZE = 16\r\n",
    "\r\n",
    "# Higher number may cause errors in notebook\r\n",
    "NUMBER_OF_WORKERS = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "trainset_loader = data.DataLoader(trainset,\r\n",
    "            batch_size = BATCH_SIZE, \r\n",
    "            shuffle = False,\r\n",
    "            drop_last = False,\r\n",
    "            num_workers = NUMBER_OF_WORKERS)\r\n",
    "\r\n",
    "evalset_loader = data.DataLoader(evalset,\r\n",
    "            batch_size = 1, \r\n",
    "            shuffle = False,\r\n",
    "            drop_last = False,\r\n",
    "            num_workers = NUMBER_OF_WORKERS)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "torch.cuda.empty_cache()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\r\n",
    "### Batch explained:\r\n",
    "\r\n",
    "If batch size = 4\r\n",
    "\r\n",
    "One batch = [ tensor([[[x,x,x]]], [[x,x,x]], [[x,x,x]], [[x,x,x]]])    , (label1, label2, label3, label4) ]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "model = RawNet2(model_dict)\r\n",
    "\r\n",
    "model.to(device)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RawNet2(\n",
       "  (ln): LayerNorm()\n",
       "  (first_conv): SincConv_fast()\n",
       "  (first_bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "  (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "  (block0): Sequential(\n",
       "    (0): Residual_block_wFRM(\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "      (frm): FRM(\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (sig): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block1): Sequential(\n",
       "    (0): Residual_block_wFRM(\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "      (frm): FRM(\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (sig): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block2): Sequential(\n",
       "    (0): Residual_block_wFRM(\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "      (frm): FRM(\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (sig): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block3): Sequential(\n",
       "    (0): Residual_block_wFRM(\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "      (frm): FRM(\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (sig): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block4): Sequential(\n",
       "    (0): Residual_block_wFRM(\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "      (frm): FRM(\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (sig): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block5): Sequential(\n",
       "    (0): Residual_block_wFRM(\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "      (frm): FRM(\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (sig): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (bn_before_gru): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (gru): GRU(128, 1024, batch_first=True)\n",
       "  (fc1_gru): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (fc2_gru): Linear(in_features=1024, out_features=408, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "def keras_lr_decay(step, decay = 0.0001):\r\n",
    "    return 1./(1. + decay * step)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "params = [\r\n",
    "    {\r\n",
    "        'params': [\r\n",
    "            param for name, param in model.named_parameters()\r\n",
    "            if 'bn' not in name\r\n",
    "        ]\r\n",
    "    },\r\n",
    "    {\r\n",
    "        'params': [\r\n",
    "            param for name, param in model.named_parameters()\r\n",
    "            if 'bn' in name\r\n",
    "        ],\r\n",
    "        'weight_decay':\r\n",
    "        0\r\n",
    "    },\r\n",
    "]\r\n",
    "\r\n",
    "criterion = {}\r\n",
    "criterion['cce'] = nn.CrossEntropyLoss()\r\n",
    "\r\n",
    "optimizer = torch.optim.Adam(params,\r\n",
    "            lr = LEARNING_RATE,\r\n",
    "            weight_decay = WEIGHT_DECAY,\r\n",
    "            amsgrad = AMSGRAD)\r\n",
    "\r\n",
    "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda step: keras_lr_decay(step))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "def cos_sim(a,b):\r\n",
    "    return np.dot(a,b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "def get_stem_to_path_dict(stem_list, wav_paths_list):\r\n",
    "    # Sets are faster to search\r\n",
    "    stem_set = set(stem_list)\r\n",
    "    \r\n",
    "    stem_to_path_d = {}\r\n",
    "    for wav_path in wav_paths_list:\r\n",
    "        wav_stem = Path(wav_path).stem\r\n",
    "        if wav_stem in stem_set:\r\n",
    "            stem_to_path_d[wav_stem] = wav_path\r\n",
    "\r\n",
    "    return stem_to_path_d          "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "def get_wav_list(dataset_dir):\r\n",
    "    # Given a directory, return path list of all wav files\r\n",
    "    pattern = '**/*.wav'\r\n",
    "    files = glob.glob(dataset_dir + pattern , recursive=True)\r\n",
    "\r\n",
    "    # Normalize the file paths. To get file paths with '/' or '\\\\' consistently depending on OS\r\n",
    "    wav_list = [os.path.normpath(i) for i in files]\r\n",
    "    return wav_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "stem_to_path_dict = get_stem_to_path_dict(evalset_stems, get_wav_list(BANGLA_ASR_DATASET_DIRECTORY))\r\n",
    "\r\n",
    "for key in stem_to_path_dict:\r\n",
    "    print(stem_to_path_dict[key])\r\n",
    "    break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "data\\BanglaASR\\WavFiles\\asr_bengali_0\\asr_bengali\\data\\00\\000020a912.wav\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "def evaluate_model(model, evalset_loader, device, eval_trials, stem_to_path_dict):\r\n",
    "\r\n",
    "    # The number of validation trials will be twice the number of data in val_dataset\r\n",
    "    # One target trial, another non target trial\r\n",
    "\r\n",
    "    model.eval()\r\n",
    "    with torch.set_grad_enabled(False):\r\n",
    "        #1st, extract speaker embeddings.\r\n",
    "        wav_path_to_embeddings_dict = {}\r\n",
    "\r\n",
    "        with tqdm(total = len(evalset_loader), ncols = 70) as pbar:\r\n",
    "            for m_batch in evalset_loader:\r\n",
    "\r\n",
    "                l_code = []\r\n",
    "                for batch in m_batch[0]:\r\n",
    "                    batch = batch.to(device)\r\n",
    "                    code = model(x = batch, is_test=True)\r\n",
    "                    l_code.extend(code.cpu().numpy())\r\n",
    "                \r\n",
    "                wav_stem = Path(m_batch[2][0]).stem\r\n",
    "                wav_path = stem_to_path_dict[wav_stem]\r\n",
    "                wav_path_to_embeddings_dict[wav_path] = np.mean(l_code, axis=0)\r\n",
    "                pbar.update(1)\r\n",
    "        \r\n",
    "        # print(\"Key\", normalized_wav_path)\r\n",
    "        # 2nd, calculate EER\r\n",
    "        y_score = [] # score for each sample\r\n",
    "        y = [] # label for each sample \r\n",
    "        \r\n",
    "        for trial in eval_trials:\r\n",
    "            trg, utt_a, utt_b = trial\r\n",
    "            y.append(int(trg))\r\n",
    "\r\n",
    "            y_score.append(cos_sim(wav_path_to_embeddings_dict[utt_a], wav_path_to_embeddings_dict[utt_b]))\r\n",
    "\r\n",
    "        fpr, tpr, _ = roc_curve(y, y_score, pos_label=1)\r\n",
    "        eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\r\n",
    "    return eer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "def train_model(model, db_gen, evalset_loader, optimizer, epoch, args, device, lr_scheduler, criterion, eval_trials, stem_to_path_dict):\r\n",
    "    # Defining it up here to make it accessable outside loop. I.E: For saving to file\r\n",
    "    loss = 0.0\r\n",
    "    # model.train()\r\n",
    "    # with tqdm(total = len(db_gen), ncols = 70) as pbar:\r\n",
    "    #     for m_batch, m_label in db_gen:\r\n",
    "            \r\n",
    "    #         m_batch = m_batch.squeeze(1)\r\n",
    "\r\n",
    "    #         m_batch, m_label = m_batch.to(device), m_label.to(device)\r\n",
    "\r\n",
    "    #         output = model(m_batch, m_label)\r\n",
    "            \r\n",
    "    #         cce_loss = criterion['cce'](output, m_label)\r\n",
    "    #         loss = cce_loss\r\n",
    "\r\n",
    "    #         optimizer.zero_grad()\r\n",
    "    #         loss.backward()\r\n",
    "    #         optimizer.step()\r\n",
    "    #         pbar.set_description('epoch: %d, cce:%.3f'%(epoch, cce_loss))\r\n",
    "    #         pbar.update(1)\r\n",
    "    #         if args['do_lr_decay']:\r\n",
    "    #             if model_dict['lr_decay'] == 'keras': lr_scheduler.step()\r\n",
    "    eer = evaluate_model(model, evalset_loader, device, eval_trials, stem_to_path_dict)\r\n",
    "    print(\"Validation set EER:\", eer)\r\n",
    "\r\n",
    "    # Save EER and model to file\r\n",
    "    with open(MODEL_LOSS_EER_OUTPUT_FILE, \"a+\") as file:\r\n",
    "        file.write(\"Epoch: {}, Loss: {}, EER: {}\\n\".format(epoch, loss, eer))\r\n",
    "    \r\n",
    "    torch.save(model.state_dict(), MODEL_SAVE_DIRECTORY+\"model_epoch_\"+str(epoch)+\".pth\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "for epoch in range(EPOCHS):\r\n",
    "    train_model(model = model,\r\n",
    "        db_gen = trainset_loader,\r\n",
    "        args = model_dict,\r\n",
    "        evalset_loader=evalset_loader,\r\n",
    "        optimizer = optimizer,\r\n",
    "        lr_scheduler = lr_scheduler,\r\n",
    "        criterion = criterion,\r\n",
    "        device = device,\r\n",
    "        epoch = epoch,\r\n",
    "        eval_trials=eval_trials,\r\n",
    "        stem_to_path_dict=stem_to_path_dict)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 14%|███▉                        | 2491/17662 [00:59<06:04, 41.62it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-b45c98e63992>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mepoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0meval_trials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_trials\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         stem_to_path_dict=stem_to_path_dict)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-72-a0637cf25d55>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, db_gen, evalset_loader, optimizer, epoch, args, device, lr_scheduler, criterion, eval_trials, stem_to_path_dict)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m#         if args['do_lr_decay']:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m#             if model_dict['lr_decay'] == 'keras': lr_scheduler.step()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0meer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalset_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_trials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstem_to_path_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Validation set EER:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-71-2b4937045e89>\u001b[0m in \u001b[0;36mevaluate_model\u001b[1;34m(model, evalset_loader, device, eval_trials, stem_to_path_dict)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevalset_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mncols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mm_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mevalset_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                 \u001b[0ml_code\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-91c918b73831>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem_to_spk_mapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwav_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m         \u001b[0msignal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwav_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;31m# moving to CPU/CUDA is now done in the training phase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\torchaudio\\backend\\soundfile_backend.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(filepath, frame_offset, num_frames, normalize, channels_first, format)\u001b[0m\n\u001b[0;32m    195\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchannel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m]\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \"\"\"\n\u001b[1;32m--> 197\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0msoundfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSoundFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfile_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"WAV\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"float32\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\soundfile.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[0;32m    627\u001b[0m         self._info = _create_info_struct(file, mode, samplerate, channels,\n\u001b[0;32m    628\u001b[0m                                          format, subtype, endian)\n\u001b[1;32m--> 629\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode_int\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosefd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    630\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missuperset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'r+'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseekable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m             \u001b[1;31m# Move write position to 0 (like in Python file objects)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\soundfile.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[0;32m   1173\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1174\u001b[0m                     \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetfilesystemencoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1175\u001b[1;33m             \u001b[0mfile_ptr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopenfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode_int\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1176\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m             \u001b[0mfile_ptr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_snd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msf_open_fd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode_int\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosefd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('pytorch': conda)"
  },
  "interpreter": {
   "hash": "83fea67622c0fa859a960c0bf8a89199ff21c56f3cdd300f29516e12427a6ea7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}