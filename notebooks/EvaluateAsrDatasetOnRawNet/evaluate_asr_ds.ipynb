{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation of Bangla ASR on RawNet"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Procedure:\r\n",
    "1. The Dataset comes with flac files. Hence we first convert the dataset in wav format.\r\n",
    "2. We load Bangla ASR dataset\r\n",
    "3. We load the RawNet Model with the best weights provided in the RawNet repository.\r\n",
    "4. We extract the Embeddings of Bangla ASR dataset using RawNet.\r\n",
    "5. We evaluate the embeddings."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Necessary Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\r\n",
    "import glob\r\n",
    "\r\n",
    "import sys\r\n",
    "import csv\r\n",
    "\r\n",
    "from pathlib import PurePath, Path\r\n",
    "from pydub import AudioSegment\r\n",
    "\r\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Constants and directories"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Directories should have a trailing '/' to avoid path related errors\r\n",
    "\r\n",
    "CURRENT_WORKING_DIRECTORY = \"W:/SpeakerRecognitionResearch/\"\r\n",
    "BANGLA_ASR_FLAC_LOCATION = \"data/BanglaASR/FlacFiles/\"\r\n",
    "BANGLA_ASR_WAV_LOCATION = \"data/BanglaASR/WavFiles/\"\r\n",
    "\r\n",
    "RAWNET2_BEST_WEIGHTS_LOCATION = \"data/RawNet2/rawnet2_best_weights.pt\"\r\n",
    "\r\n",
    "# This file is provided with BANGLA_ASR dataset\r\n",
    "# It contains the anonymous speaker id of each audio\r\n",
    "BANGLA_ASR_TSV_LOCATION = \"data/BanglaASR/utt_spk_text.tsv\"\r\n",
    "\r\n",
    "# To avoid file location related errors, we make sure \"SpeakerRecognitionResearch\" root folder is the current working directory.\r\n",
    "os.chdir(CURRENT_WORKING_DIRECTORY)\r\n",
    "os.getcwd()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'W:\\\\SpeakerRecognitionResearch'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Converting FLAC to WAV"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# We calculate the rows provided in utt_spk_text.tsv, this number can be used to verify if we have all the files from the dataset.\r\n",
    "\r\n",
    "def get_number_of_utt_in_tsv(tsv_loc):\r\n",
    "    total_utterrances = 0\r\n",
    "\r\n",
    "    with open(tsv_loc, encoding=\"utf-8\") as tsvfile:\r\n",
    "        tsvreader = csv.reader(tsvfile, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\r\n",
    "        for line in tsvreader:\r\n",
    "            total_utterrances += 1\r\n",
    "    \r\n",
    "    return total_utterrances\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def get_relative_location_of_audio_files(root_path, audio_format=\"wav\"):\r\n",
    "    # Traverses recursively through the root folder\r\n",
    "    # Returns list of relative locations to all the files \r\n",
    "    # matching the audio format provided\r\n",
    "\r\n",
    "    pattern = '**/*.' + audio_format\r\n",
    "    files = glob.glob(root_path + pattern , recursive=True)\r\n",
    "\r\n",
    "    # Normalize the file paths. To get file paths with '/' or '\\\\' consistently depending on OS\r\n",
    "    files = [os.path.normpath(i) for i in files]\r\n",
    "    return files"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "file_locs = get_relative_location_of_audio_files(BANGLA_ASR_FLAC_LOCATION, audio_format=\"flac\")\r\n",
    "\r\n",
    "number_of_files = len(file_locs)\r\n",
    "number_of_rows_in_tsv = get_number_of_utt_in_tsv(BANGLA_ASR_TSV_LOCATION)\r\n",
    "\r\n",
    "assert number_of_files == number_of_rows_in_tsv , \"Number of files in the dataset doesn't match the number of rows provided in the tsv file. Please check the dataset.\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Paths may be in a non normalized form,\r\n",
    "# So, trying to replace portions of them may lead to unwanted errors\r\n",
    "\r\n",
    "def replace_root_of_path(path, old_root, new_root):\r\n",
    "    path = str(os.path.normpath(path))\r\n",
    "    old_root = str(os.path.normpath(old_root))\r\n",
    "    new_root = str(os.path.normpath(new_root))\r\n",
    "\r\n",
    "    return path.replace(old_root, new_root)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "assert replace_root_of_path(path=\"foo/bar/hello/00/a.jpg\", old_root=\"foo/bar\", new_root=\"spam\") == os.path.normpath(\"spam/hello/00/a.jpg\"), \"The root replacing is not working.\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Python does not automatically create directory structure when creating a file\r\n",
    "# This function will create the subdirectories required\r\n",
    "\r\n",
    "def mkdirs_from_filepath(file_path):\r\n",
    "\r\n",
    "    parent = Path(file_path).parent\r\n",
    "    os.makedirs(parent, exist_ok=True)\r\n",
    "\r\n",
    "# mkdirs_from_filepath(\"hello/bar/foo/k.jpg\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def convert_flacs_to_wavs(flacs_path, wavs_path):\r\n",
    "    flac_files = get_relative_location_of_audio_files(flacs_path, audio_format=\"flac\")\r\n",
    "\r\n",
    "    # wav file locations will be the same as the flac files\r\n",
    "    # Only the root folder will be changed\r\n",
    "    wav_files = [replace_root_of_path(flac_file_path, flacs_path, wavs_path) for flac_file_path in flac_files]\r\n",
    "\r\n",
    "    for flac_file, wav_file in tqdm(zip(flac_files, wav_files), total=len(wav_files)):\r\n",
    "\r\n",
    "        flac_file = PurePath(flac_file)\r\n",
    "\r\n",
    "        # Replace .flac with .wav from name\r\n",
    "        wav_file = wav_file.replace(wav_file.split(\".\")[-1], \"wav\")\r\n",
    "\r\n",
    "        mkdirs_from_filepath(wav_file)\r\n",
    "\r\n",
    "        flac_tmp_audio_data = AudioSegment.from_file(flac_file, flac_file.suffix[1:])\r\n",
    "        flac_tmp_audio_data.export(wav_file, format=\"wav\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Leave this function call commented out to avoid unwanted conversions\r\n",
    "\r\n",
    "print(\"Do you want to convert the .flac files in {} to .wav files in the directory {} ?\".format(BANGLA_ASR_FLAC_LOCATION, BANGLA_ASR_WAV_LOCATION))\r\n",
    "user_input = input(\"Type YES_AND_YES to continue:\")\r\n",
    "\r\n",
    "if user_input == \"YES_AND_YES\":\r\n",
    "    convert_flacs_to_wavs(BANGLA_ASR_FLAC_LOCATION, BANGLA_ASR_WAV_LOCATION)\r\n",
    "else:\r\n",
    "    print(\"Conversion aborted!\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Do you want to convert the .flac files in data/BanglaASR/FlacFiles/ to .wav files in the directory data/BanglaASR/WavFiles/ ?\n",
      "Conversion aborted!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Loading BanglaAsrDataset\r\n",
    "\r\n",
    "In this step we load the RawNet provided [here](https://github.com/Jungjee/RawNet/blob/master/python/RawNet2/Pre-trained_model/model_RawNet2_original_code.py). We changed the file name to simply model_RawNet2.py in our use. The model is then initialized with the best weights file that can be found [here](https://github.com/Jungjee/RawNet/blob/master/python/RawNet2/Pre-trained_model/rawnet2_best_weights.pt)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import soundfile as sf\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "from torch.utils import data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Using default parameter values used in the VoxCeleb Dataset\r\n",
    "\r\n",
    "WINDOW_SIZE = 11810\r\n",
    "NB_SAMP = 59049"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bangla ASR Dataset for PyTorch usage"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "class BanglaAsrDataset(data.Dataset):\r\n",
    "\tdef __init__(self, list_IDs, nb_samp = 0, window_size = 0, labels = {}, cut = True, norm_scale = True):\r\n",
    "\t\t'''\r\n",
    "\t\tself.list_IDs\t: list of strings (each string: utt key)\r\n",
    "\t\tself.labels\t\t: dictionary (key: utt key, value: label integer)\r\n",
    "\t\tself.nb_samp\t: integer, the number of timesteps for each mini-batch\r\n",
    "\t\tcut\t\t\t\t: (boolean) adjust utterance duration for mini-batch construction\r\n",
    "\t\treturn_label\t: (boolean) \r\n",
    "\t\tnorm_scale\t\t: (boolean) normalize scale alike SincNet github repo\r\n",
    "\t\t'''\r\n",
    "\t\tself.list_IDs = list_IDs\r\n",
    "\t\tself.window_size = window_size\r\n",
    "\t\tself.nb_samp = nb_samp\r\n",
    "\t\tself.labels = labels\r\n",
    "\t\tself.cut = cut\r\n",
    "\t\tself.norm_scale = norm_scale\r\n",
    "\t\tif self.cut and self.nb_samp == 0: \r\n",
    "\t\t\traise ValueError('when adjusting utterance length, \"nb_samp\" should be passed as parameter')\r\n",
    "\r\n",
    "\tdef _normalize_scale(self, x):\r\n",
    "\t\t'''\r\n",
    "\t\tNormalize sample scale alike SincNet.\r\n",
    "\t\t'''\r\n",
    "\t\treturn x/np.max(np.abs(x))\r\n",
    "\r\n",
    "\tdef __len__(self):\r\n",
    "\t\treturn len(self.list_IDs)\r\n",
    "\r\n",
    "\tdef __getitem__(self, index):\r\n",
    "\t\tID = self.list_IDs[index]\r\n",
    "\t\ttry:\r\n",
    "\t\t\tX, _ = sf.read(ID) \r\n",
    "\t\t\tX = X.astype(np.float64)\r\n",
    "\t\texcept:\r\n",
    "\t\t\traise ValueError('%s'%ID)\r\n",
    "\r\n",
    "\t\tif self.norm_scale:\r\n",
    "\t\t\tX = self._normalize_scale(X).astype(np.float32)\r\n",
    "\t\tX = X.reshape(1,-1)\r\n",
    "\r\n",
    "\t\tlist_X = []\r\n",
    "\t\tnb_time = X.shape[1]\r\n",
    "\t\tif nb_time < self.nb_samp:\r\n",
    "\t\t\tnb_dup = int(self.nb_samp / nb_time) + 1\r\n",
    "\t\t\tlist_X.append(np.tile(X, (1, nb_dup))[:, :self.nb_samp][0])\r\n",
    "\t\telif nb_time > self.nb_samp:\r\n",
    "\t\t\tstep = self.nb_samp - self.window_size\r\n",
    "\t\t\titeration = int( (nb_time - self.window_size) / step ) + 1\r\n",
    "\t\t\tfor i in range(iteration):\r\n",
    "\t\t\t\tif i == 0:\r\n",
    "\t\t\t\t\tlist_X.append(X[:, :self.nb_samp][0])\r\n",
    "\t\t\t\telif i < iteration - 1:\r\n",
    "\t\t\t\t\tlist_X.append(X[:, i*step : i*step + self.nb_samp][0])\r\n",
    "\t\t\t\telse:\r\n",
    "\t\t\t\t\tlist_X.append(X[:, -self.nb_samp:][0])\r\n",
    "\t\telse :\r\n",
    "\t\t\tlist_X.append(X[0])\r\n",
    "\r\n",
    "\t\t\r\n",
    "\t\tfilename_from_path = Path(ID).stem\r\n",
    "\t\ty = self.labels[filename_from_path]\r\n",
    "\r\n",
    "\t\treturn list_X, y, ID"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def generate_labels_from_tsv(wav_list, tsv_loc):\r\n",
    "    labels = {}\r\n",
    "\r\n",
    "    with open(tsv_loc, encoding=\"utf-8\") as tsvfile:\r\n",
    "        tsvreader = csv.reader(tsvfile, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\r\n",
    "        for line in tsvreader:\r\n",
    "            wav_file_name = line[0]\r\n",
    "            speaker_id = line[1]\r\n",
    "\r\n",
    "            labels[wav_file_name] = speaker_id\r\n",
    "\r\n",
    "    return labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "wav_list = get_relative_location_of_audio_files(BANGLA_ASR_WAV_LOCATION)\r\n",
    "\r\n",
    "labels = generate_labels_from_tsv(wav_list, BANGLA_ASR_TSV_LOCATION)\r\n",
    "\r\n",
    "assert labels['000020a912'] == '16cfb', \"Either file id to speaker id mapping changed, or labels were not retrieved properly.\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "bangla_asr_dataset = BanglaAsrDataset(\r\n",
    "    list_IDs=wav_list,\r\n",
    "    labels=labels,\r\n",
    "    window_size=WINDOW_SIZE,\r\n",
    "    nb_samp=NB_SAMP,\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "bangla_asr_dataset[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "([array([ 0.01430657,  0.01109489,  0.01080292, ..., -0.00583942,\n",
       "         -0.00554745, -0.00963504], dtype=float32)],\n",
       " '16cfb',\n",
       " 'data\\\\BanglaASR\\\\WavFiles\\\\asr_bengali_0\\\\asr_bengali\\\\data\\\\00\\\\000020a912.wav')"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Loading RawNet with Best Weights"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "import torch\r\n",
    "from model_RawNet2 import RawNet\r\n",
    "\r\n",
    "import pickle"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# Default value was 8, however NoteBook doesn't support >0\r\n",
    "NB_WORKER = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n",
    "print(\"Using {}.\".format(device))\r\n",
    "if device==\"cuda\": print(torch.cuda.get_device_name(0))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cuda.\n",
      "NVIDIA GeForce GTX 1050\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "model_dict = {}\r\n",
    "model_dict['nb_classes'] = 6112\r\n",
    "model_dict['first_conv'] = 251\r\n",
    "model_dict['in_channels'] = 1\r\n",
    "model_dict['filts'] = [128, [128,128], [128,256], [256,256]]\r\n",
    "model_dict['m_blocks'] = [2, 4]\r\n",
    "model_dict['nb_fc_att_node'] =[1]\r\n",
    "model_dict['nb_fc_node'] = 1024\r\n",
    "model_dict['gru_node'] = 1024\r\n",
    "model_dict['nb_gru_layer'] = 1\r\n",
    "model_dict['nb_samp'] = 59049"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "model = RawNet(model_dict, device)\r\n",
    "model.load_state_dict(torch.load(RAWNET2_BEST_WEIGHTS_LOCATION))\r\n",
    "model.to(device)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RawNet(\n",
       "  (ln): LayerNorm()\n",
       "  (first_conv): SincConv_fast()\n",
       "  (first_bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "  (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "  (block0): Sequential(\n",
       "    (0): Residual_block(\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (block1): Sequential(\n",
       "    (0): Residual_block(\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (block2): Sequential(\n",
       "    (0): Residual_block(\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv_downsample): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (block3): Sequential(\n",
       "    (0): Residual_block(\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (block4): Sequential(\n",
       "    (0): Residual_block(\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (block5): Sequential(\n",
       "    (0): Residual_block(\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (fc_attention0): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (fc_attention1): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (fc_attention2): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (fc_attention3): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (fc_attention4): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (fc_attention5): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (bn_before_gru): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (gru): GRU(256, 1024, batch_first=True)\n",
       "  (fc1_gru): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (fc2_gru): Linear(in_features=1024, out_features=6112, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# Create DataLoader for Dataset\r\n",
    "\r\n",
    "bangla_asr_dataloader = data.DataLoader(bangla_asr_dataset,\r\n",
    "            batch_size = 1, \r\n",
    "            shuffle = False,\r\n",
    "            drop_last = False,\r\n",
    "            num_workers = NB_WORKER)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Extract Embeddings"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "EMBEDDINGS_SAVE_FILE_LOCATION = \"notebooks/EvaluateAsrDatasetOnRawNet/out/embeddings.pkl\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# Model must be in eval mode to extract embedding\r\n",
    "\r\n",
    "model.eval()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RawNet(\n",
       "  (ln): LayerNorm()\n",
       "  (first_conv): SincConv_fast()\n",
       "  (first_bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "  (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "  (block0): Sequential(\n",
       "    (0): Residual_block(\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (block1): Sequential(\n",
       "    (0): Residual_block(\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (block2): Sequential(\n",
       "    (0): Residual_block(\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv_downsample): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (block3): Sequential(\n",
       "    (0): Residual_block(\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (block4): Sequential(\n",
       "    (0): Residual_block(\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (block5): Sequential(\n",
       "    (0): Residual_block(\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (fc_attention0): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (fc_attention1): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (fc_attention2): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (fc_attention3): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (fc_attention4): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (fc_attention5): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (bn_before_gru): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (gru): GRU(256, 1024, batch_first=True)\n",
       "  (fc1_gru): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (fc2_gru): Linear(in_features=1024, out_features=6112, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "user_choice = input(\"Do You Want To Generate Embeddings? Type YES/NO\")\r\n",
    "\r\n",
    "if user_choice == \"YES\":\r\n",
    "    #Generating Embeddings\r\n",
    "    with torch.set_grad_enabled(False):\r\n",
    "        #1st, extract speaker embeddings.\r\n",
    "\r\n",
    "        # Each m_batch is a single wav\r\n",
    "        with open(EMBEDDINGS_SAVE_FILE_LOCATION, 'ab+') as fp:\r\n",
    "            for m_batch in tqdm(bangla_asr_dataloader, total=len(wav_list)):\r\n",
    "                l_code = []\r\n",
    "                for batch in m_batch[0]:\r\n",
    "                    batch = batch.to(device)\r\n",
    "                    code = model(x = batch, is_test=True)\r\n",
    "                    l_code.extend(code.cpu().numpy())\r\n",
    "                file_location = m_batch[2][0]\r\n",
    "                l_embedding = np.mean(l_code, axis=0)\r\n",
    "                pickle.dump((file_location, l_embedding), fp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# Load from pickle\r\n",
    "\r\n",
    "embeddings = []\r\n",
    "with open(EMBEDDINGS_SAVE_FILE_LOCATION, 'rb') as fr:\r\n",
    "    try:\r\n",
    "        while True:\r\n",
    "            embeddings.append(pickle.load(fr))\r\n",
    "    except EOFError:\r\n",
    "        pass\r\n",
    "len(embeddings)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "218703"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "embeddings[218702]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('data\\\\BanglaASR\\\\WavFiles\\\\asr_bengali_f\\\\asr_bengali\\\\data\\\\ff\\\\fffff1c677.wav',\n",
       " array([ 0.08765147, -0.05041703,  0.05247699, ...,  0.08953784,\n",
       "        -0.05553602, -0.02825456], dtype=float32))"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "def speech_per_speaker(labels):\r\n",
    "    # Returns a dictionary containing the number of utterrances for each speaker\r\n",
    "    speaker_ids = {}\r\n",
    "    for wav in labels.keys():\r\n",
    "        spk_id = labels[wav]\r\n",
    "        if spk_id in speaker_ids.keys():\r\n",
    "            speaker_ids[spk_id] += 1\r\n",
    "        else:\r\n",
    "            speaker_ids[spk_id] = 0\r\n",
    "\r\n",
    "    return speaker_ids"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "speech_per_spk = speech_per_speaker(labels)\r\n",
    "total_number_of_speakers = len(speech_per_spk.keys())\r\n",
    "\r\n",
    "# This is the total number of speakers in this dataset\r\n",
    "total_number_of_speakers"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "508"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Evaluate Bangla ASR on RawNet"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "source": [
    "# Specifies how many trials to make\r\n",
    "# In other words, how many times to calculate cos_sim to calculate EER\r\n",
    "NB_TRIALS = 500\r\n",
    "\r\n",
    "# Where the validation trials should be saved\r\n",
    "VAL_TRIAL_LOCATION = \"notebooks/EvaluateAsrDatasetOnRawNet/out/val_trials.txt\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "def cos_sim(a,b):\r\n",
    "    return np.dot(a,b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "print(embeddings[0], labels[name_from_path(embeddings[0][0])])\r\n",
    "print(embeddings[116], labels[name_from_path(embeddings[116][0])])\r\n",
    "\r\n",
    "print(embeddings[2], labels[name_from_path(embeddings[2][0])])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('data\\\\BanglaASR\\\\WavFiles\\\\asr_bengali_0\\\\asr_bengali\\\\data\\\\00\\\\000020a912.wav', array([-0.00931683, -0.05722116,  0.09655098, ..., -0.07048331,\n",
      "        0.11888041,  0.00730451], dtype=float32)) 16cfb\n",
      "('data\\\\BanglaASR\\\\WavFiles\\\\asr_bengali_0\\\\asr_bengali\\\\data\\\\00\\\\002648db9d.wav', array([-0.07721571, -0.19243167,  0.01763637, ..., -0.0403515 ,\n",
      "        0.12470041,  0.1361751 ], dtype=float32)) 16cfb\n",
      "('data\\\\BanglaASR\\\\WavFiles\\\\asr_bengali_0\\\\asr_bengali\\\\data\\\\00\\\\00005debc7.wav', array([ 0.10274858, -0.0370782 , -0.0966461 , ..., -0.10026335,\n",
      "       -0.04393942,  0.08075546], dtype=float32)) f83df\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "# Searching for a specific user indexes in the embeddings\r\n",
    "\r\n",
    "def get_indices_of_user_in_embeddings(embeddings, labels, user_key):\r\n",
    "\r\n",
    "    total_embeddings = len(embeddings)\r\n",
    "    indices = []\r\n",
    "    for i in range(0,total_embeddings):\r\n",
    "        if labels[name_from_path(embeddings[i][0])] == user_key:\r\n",
    "            indices.append(i)\r\n",
    "    \r\n",
    "    return indices"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "get_indices_of_user_in_embeddings(embeddings, labels, \"f83df\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[2,\n",
       " 381,\n",
       " 1415,\n",
       " 2088,\n",
       " 2097,\n",
       " 2222,\n",
       " 2310,\n",
       " 2371,\n",
       " 2670,\n",
       " 2700,\n",
       " 2864,\n",
       " 3581,\n",
       " 4271,\n",
       " 4797,\n",
       " 4978,\n",
       " 5084,\n",
       " 5810,\n",
       " 5987,\n",
       " 6211,\n",
       " 6962,\n",
       " 7253,\n",
       " 7459,\n",
       " 8211,\n",
       " 8272,\n",
       " 8452,\n",
       " 9874,\n",
       " 9912,\n",
       " 10957,\n",
       " 11316,\n",
       " 12256,\n",
       " 12749,\n",
       " 13203,\n",
       " 13492,\n",
       " 13535,\n",
       " 13745,\n",
       " 14380,\n",
       " 14382,\n",
       " 15738,\n",
       " 16012,\n",
       " 16121,\n",
       " 16590,\n",
       " 16712,\n",
       " 16720,\n",
       " 16854,\n",
       " 16947,\n",
       " 18080,\n",
       " 18457,\n",
       " 19575,\n",
       " 20952,\n",
       " 21047,\n",
       " 21263,\n",
       " 21681,\n",
       " 22122,\n",
       " 22459,\n",
       " 22729,\n",
       " 24011,\n",
       " 24358,\n",
       " 24694,\n",
       " 24849,\n",
       " 25070,\n",
       " 25704,\n",
       " 27263,\n",
       " 27515,\n",
       " 28037,\n",
       " 28515,\n",
       " 28692,\n",
       " 29486,\n",
       " 29715,\n",
       " 29944,\n",
       " 30329,\n",
       " 30374,\n",
       " 30623,\n",
       " 33513,\n",
       " 34497,\n",
       " 34522,\n",
       " 35343,\n",
       " 35931,\n",
       " 36411,\n",
       " 36526,\n",
       " 37812,\n",
       " 38346,\n",
       " 38979,\n",
       " 39351,\n",
       " 40473,\n",
       " 40628,\n",
       " 40866,\n",
       " 41127,\n",
       " 41574,\n",
       " 42654,\n",
       " 43879,\n",
       " 43975,\n",
       " 44360,\n",
       " 45726,\n",
       " 49035,\n",
       " 50830,\n",
       " 50831,\n",
       " 53321,\n",
       " 53734,\n",
       " 54179,\n",
       " 54283,\n",
       " 54591,\n",
       " 55404,\n",
       " 55686,\n",
       " 55723,\n",
       " 56101,\n",
       " 56840,\n",
       " 57591,\n",
       " 59098,\n",
       " 59958,\n",
       " 59979,\n",
       " 61295,\n",
       " 61371,\n",
       " 61910,\n",
       " 62992,\n",
       " 63723,\n",
       " 63789,\n",
       " 65080,\n",
       " 65762,\n",
       " 66241,\n",
       " 66620,\n",
       " 67034,\n",
       " 67163,\n",
       " 67240,\n",
       " 67327,\n",
       " 67535,\n",
       " 68100,\n",
       " 68185,\n",
       " 70753,\n",
       " 71190,\n",
       " 71349,\n",
       " 72059,\n",
       " 72290,\n",
       " 72989,\n",
       " 73928,\n",
       " 74321,\n",
       " 74381,\n",
       " 74623,\n",
       " 74669,\n",
       " 75073,\n",
       " 76199,\n",
       " 76443,\n",
       " 76548,\n",
       " 78490,\n",
       " 78518,\n",
       " 78878,\n",
       " 79008,\n",
       " 79472,\n",
       " 80734,\n",
       " 81069,\n",
       " 81097,\n",
       " 81425,\n",
       " 82028,\n",
       " 82274,\n",
       " 82625,\n",
       " 83315,\n",
       " 85568,\n",
       " 87398,\n",
       " 88487,\n",
       " 88941,\n",
       " 88955,\n",
       " 89146,\n",
       " 89530,\n",
       " 90037,\n",
       " 90130,\n",
       " 91355,\n",
       " 91428,\n",
       " 92595,\n",
       " 93097,\n",
       " 94030,\n",
       " 94610,\n",
       " 94937,\n",
       " 95144,\n",
       " 95160,\n",
       " 95161,\n",
       " 95853,\n",
       " 96564,\n",
       " 96889,\n",
       " 98867,\n",
       " 98970,\n",
       " 99313,\n",
       " 99754,\n",
       " 100064,\n",
       " 100357,\n",
       " 101128,\n",
       " 101272,\n",
       " 101443,\n",
       " 101449,\n",
       " 102107,\n",
       " 102425,\n",
       " 102674,\n",
       " 102896,\n",
       " 103325,\n",
       " 103804,\n",
       " 104510,\n",
       " 105747,\n",
       " 106199,\n",
       " 107433,\n",
       " 107761,\n",
       " 108754,\n",
       " 109359,\n",
       " 109494,\n",
       " 109570,\n",
       " 109612,\n",
       " 109733,\n",
       " 109978,\n",
       " 110167,\n",
       " 110256,\n",
       " 110323,\n",
       " 110587,\n",
       " 110615,\n",
       " 111810,\n",
       " 112055,\n",
       " 112751,\n",
       " 115070,\n",
       " 115562,\n",
       " 116287,\n",
       " 117105,\n",
       " 117372,\n",
       " 118017,\n",
       " 118138,\n",
       " 118343,\n",
       " 121169,\n",
       " 121194,\n",
       " 121944,\n",
       " 122286,\n",
       " 122302,\n",
       " 122443,\n",
       " 122585,\n",
       " 123772,\n",
       " 124025,\n",
       " 124423,\n",
       " 125179,\n",
       " 126702,\n",
       " 127110,\n",
       " 127213,\n",
       " 127906,\n",
       " 127912,\n",
       " 128087,\n",
       " 128097,\n",
       " 128562,\n",
       " 128887,\n",
       " 129278,\n",
       " 131973,\n",
       " 132787,\n",
       " 133065,\n",
       " 133197,\n",
       " 133760,\n",
       " 134404,\n",
       " 134914,\n",
       " 136617,\n",
       " 136943,\n",
       " 138093,\n",
       " 138204,\n",
       " 138462,\n",
       " 138486,\n",
       " 139535,\n",
       " 139646,\n",
       " 142772,\n",
       " 143242,\n",
       " 143730,\n",
       " 144638,\n",
       " 145175,\n",
       " 145416,\n",
       " 145437,\n",
       " 146242,\n",
       " 146584,\n",
       " 146668,\n",
       " 146697,\n",
       " 148103,\n",
       " 149731,\n",
       " 149778,\n",
       " 152099,\n",
       " 152115,\n",
       " 153017,\n",
       " 153677,\n",
       " 156465,\n",
       " 158994,\n",
       " 159607,\n",
       " 160950,\n",
       " 161863,\n",
       " 163323,\n",
       " 163522,\n",
       " 163573,\n",
       " 163629,\n",
       " 163655,\n",
       " 164652,\n",
       " 164748,\n",
       " 164751,\n",
       " 164754,\n",
       " 165695,\n",
       " 166240,\n",
       " 166418,\n",
       " 166444,\n",
       " 166763,\n",
       " 167200,\n",
       " 167685,\n",
       " 168290,\n",
       " 168529,\n",
       " 169177,\n",
       " 169319,\n",
       " 169410,\n",
       " 170772,\n",
       " 171486,\n",
       " 171571,\n",
       " 173329,\n",
       " 173453,\n",
       " 174073,\n",
       " 174661,\n",
       " 174921,\n",
       " 176872,\n",
       " 176896,\n",
       " 177084,\n",
       " 178269,\n",
       " 179118,\n",
       " 179994,\n",
       " 181001,\n",
       " 182079,\n",
       " 183868,\n",
       " 187145,\n",
       " 188528,\n",
       " 189393,\n",
       " 189613,\n",
       " 189652,\n",
       " 190940,\n",
       " 191763,\n",
       " 191899,\n",
       " 192365,\n",
       " 193517,\n",
       " 194196,\n",
       " 194517,\n",
       " 194551,\n",
       " 194666,\n",
       " 195825,\n",
       " 195895,\n",
       " 196587,\n",
       " 196698,\n",
       " 196746,\n",
       " 197377,\n",
       " 197745,\n",
       " 199650,\n",
       " 199876,\n",
       " 199939,\n",
       " 200281,\n",
       " 202310,\n",
       " 202530,\n",
       " 203423,\n",
       " 203443,\n",
       " 204159,\n",
       " 206596,\n",
       " 207417,\n",
       " 208951,\n",
       " 208982,\n",
       " 209503,\n",
       " 209735,\n",
       " 210131,\n",
       " 210591,\n",
       " 210973,\n",
       " 211328,\n",
       " 211474,\n",
       " 212580,\n",
       " 212839,\n",
       " 213161,\n",
       " 213548,\n",
       " 213591,\n",
       " 213612,\n",
       " 213617,\n",
       " 213748,\n",
       " 213997,\n",
       " 214205,\n",
       " 214314,\n",
       " 216872,\n",
       " 216955,\n",
       " 217288]"
      ]
     },
     "metadata": {},
     "execution_count": 87
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "cos_sim(embeddings[0][1], embeddings[2][1])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-0.015909072"
      ]
     },
     "metadata": {},
     "execution_count": 88
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "source": [
    "def get_speaker_to_paths_dict(wav_list, labels):\r\n",
    "    \r\n",
    "    spk_to_path_d = {}\r\n",
    "\r\n",
    "    for wav_path in wav_list:\r\n",
    "        wav_name = Path(wav_path).stem\r\n",
    "        spk_id = labels[wav_name]\r\n",
    "\r\n",
    "        if spk_id in spk_to_path_d.keys():\r\n",
    "            spk_to_path_d[spk_id].append(wav_path)\r\n",
    "        else:\r\n",
    "            spk_to_path_d[spk_id] = [wav_path]\r\n",
    "    \r\n",
    "    return spk_to_path_d"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "source": [
    "spk_to_paths_d = get_speaker_to_paths_dict(wav_list, labels)\r\n",
    "\r\n",
    "assert len( list(filter(lambda wav_path: \"003399c103\" in wav_path, spk_to_paths_d[\"948d3\"])) )==1, \"Verify if the tsv file is upto date with the Dataset\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "source": [
    "# This function will generate a file that will contain which two files should be compared with cos similarities\r\n",
    "# Format:\r\n",
    "# 1 spk/_1_/audio/1.wav spk/_1_/audio/2.wav\r\n",
    "# 0 spk/_1_/audio/1.wav spk/_2_/audio/1.wav\r\n",
    "\r\n",
    "def generate_validation_trials(wav_list, nb_trial, val_trial_location, speaker_to_paths_d):\r\n",
    "    val_trial_file = open(val_trial_location, 'w')\r\n",
    "\r\n",
    "    # We define, same speaker trial as target trial\r\n",
    "    # Target trial: 1; Non-trg: 0\r\n",
    "\r\n",
    "    # There will be equal numbers of target and non target trials\r\n",
    "    nb_target_trials = int(nb_trial / 2)\r\n",
    "        \r\n",
    "    speakers_list = list(speaker_to_paths_d.keys())\r\n",
    "\r\n",
    "    #compose target trials\r\n",
    "    selected_spks = np.random.choice(speakers_list, size=nb_target_trials, replace=True)\r\n",
    "    for spk in selected_spks:\r\n",
    "        wav_paths_of_speaker = speaker_to_paths_d[spk]\r\n",
    "        utt_a, utt_b = np.random.choice(wav_paths_of_speaker, size=2, replace=False)\r\n",
    "        val_trial_file.write('1 %s %s\\n'%(utt_a, utt_b))\r\n",
    "\r\n",
    "    #compose non-target trials\r\n",
    "    for i in range(nb_target_trials):\r\n",
    "        two_different_speakers = np.random.choice(speakers_list, size=2, replace = False)\r\n",
    "        utt_a = np.random.choice(speaker_to_paths_d[two_different_speakers[0]], size=1)[0]\r\n",
    "        utt_b = np.random.choice(speaker_to_paths_d[two_different_speakers[1]], size=1)[0]\r\n",
    "        val_trial_file.write('0 %s %s\\n'%(utt_a, utt_b))\r\n",
    "\r\n",
    "    val_trial_file.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "source": [
    "generate_validation_trials(wav_list=wav_list, \r\n",
    "                            nb_trial=NB_TRIALS, \r\n",
    "                            val_trial_location=VAL_TRIAL_LOCATION, \r\n",
    "                            speaker_to_paths_d=spk_to_paths_d)\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('pytorch': conda)"
  },
  "interpreter": {
   "hash": "83fea67622c0fa859a960c0bf8a89199ff21c56f3cdd300f29516e12427a6ea7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}