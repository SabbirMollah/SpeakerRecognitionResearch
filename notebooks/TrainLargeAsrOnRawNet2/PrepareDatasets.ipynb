{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Preparing LARGE ASR DATASET\r\n",
    "\r\n",
    "https://www.isca-speech.org/archive/sltu_2018/kjartansson18_sltu.html\r\n",
    "\r\n",
    "This notebook will make the regulations for splitting the entire dataset into train, validation and test sets. It will create text files containing the stems (file names) of the wavs in each different set.\r\n",
    "\r\n",
    "[[[ If the DatasetPreparations folder already contains the 6 output files, then it is not necessary to run this notebook. ]]]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Terminologies:\r\n",
    "\r\n",
    "* Stem: Stem is the file name from a path. For example, the stem of \"a/b/c/abc.wav\" is \"abc\"\r\n",
    "* devset: Devset indicates the set of files that will be used during development or training. This includes both train set and evaluation set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Necessary imports:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import glob\r\n",
    "import csv\r\n",
    "\r\n",
    "from pathlib import Path\r\n",
    "\r\n",
    "import random\r\n",
    "\r\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Directories are assumed to have a trailing '/' or '\\\\' in all the subsequent code\r\n",
    "\r\n",
    "CURRENT_WORKING_DIRECTORY = \"W:/SpeakerRecognitionResearch/\"\r\n",
    "\r\n",
    "LARGE_ASR_DATASET_DIRECTORY = \"S:/Large ASR/WavFiles/\"\r\n",
    "\r\n",
    "# This is a combination of the 5 TSV files, and it will be generated in this notebook\r\n",
    "LARGE_ASR_TSV_LOCATION = \"S:/Large ASR/WavFiles/utt_spk_text.tsv\"\r\n",
    "\r\n",
    "# To avoid file location related errors, we make sure \"SpeakerRecognitionResearch\" root folder is the current working directory.\r\n",
    "os.chdir(CURRENT_WORKING_DIRECTORY)\r\n",
    "os.getcwd()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'W:\\\\SpeakerRecognitionResearch'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Output files:\r\n",
    "\r\n",
    "The following output files will be generated by this notebook:\r\n",
    "1. trainset_list.txt: List of all stems that will be used for training\r\n",
    "2. evalset_list.txt: List of all stems that will be used for evaluation in the training loop\r\n",
    "3. test_set.txt: List of all stems that will be used for testing/validating after training phase is complete\r\n",
    "4. devset_class_order.txt: Order of the classes used for both trainset and evalset\r\n",
    "5. testset_class_order.txt: Order of the classes used for testset\r\n",
    "6. eval_trials.txt: (expected utt1 utt2) tuples for each trial to compute for evaluation\r\n",
    "7. test_trials.txt: (expected utt1 utt2) tuples for each trial to compute for test"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "TRAINSET_LIST_LOCATION = \"notebooks/TrainLargeAsrOnRawNet2/DatasetPreparations/trainset_list.txt\"\r\n",
    "EVALSET_LIST_LOCATION = \"notebooks/TrainLargeAsrOnRawNet2/DatasetPreparations/evalset_list.txt\"\r\n",
    "TESTSET_LIST_LOCATION = \"notebooks/TrainLargeAsrOnRawNet2/DatasetPreparations/testset_list.txt\"\r\n",
    "DEVSET_CLASS_ORDER_LOCATION = \"notebooks/TrainLargeAsrOnRawNet2/DatasetPreparations/devset_classes.txt\"\r\n",
    "TESTSET_CLASS_ORDER_LOCATION = \"notebooks/TrainLargeAsrOnRawNet2/DatasetPreparations/testset_classes.txt\"\r\n",
    "EVAL_TRIALS_LOCATION = \"notebooks/TrainLargeAsrOnRawNet2/DatasetPreparations/eval_trials.txt\"\r\n",
    "TEST_TRIALS_LOCATION = \"notebooks/TrainLargeAsrOnRawNet2/DatasetPreparations/test_trials.txt\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Joining the TSV files into one"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "BENGALI_TSV = \"S:/Large ASR/WavFiles/BengaliASR/ben_utt_spk_text.tsv\"\r\n",
    "JAVANESE_TSV = \"S:/Large ASR/WavFiles/JavaneseASR/jav_utt_spk_text.tsv\"\r\n",
    "NEPALI_TSV = \"S:/Large ASR/WavFiles/NepaliASR/nep_utt_spk_text.tsv\"\r\n",
    "SINHALA_TSV = \"S:/Large ASR/WavFiles/SinhalaASR/sin_utt_spk_text.tsv\"\r\n",
    "SUNDANESE_TSV = \"S:/Large ASR/WavFiles/SundaneseASR/sun_utt_spk_text.tsv\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "bengali_tsv_df = pd.read_csv(BENGALI_TSV, quoting=csv.QUOTE_NONE, sep='\\t', header=None)\r\n",
    "bengali_tsv_df = bengali_tsv_df.iloc[:,:-1]\r\n",
    "\r\n",
    "javanese_tsv_df = pd.read_csv(JAVANESE_TSV, quoting=csv.QUOTE_NONE, sep='\\t', header=None)\r\n",
    "javanese_tsv_df = javanese_tsv_df.iloc[:,:-1]\r\n",
    "\r\n",
    "nepali_tsv_df = pd.read_csv(NEPALI_TSV, quoting=csv.QUOTE_NONE, sep='\\t', header=None)\r\n",
    "nepali_tsv_df = nepali_tsv_df.iloc[:,:-1]\r\n",
    "\r\n",
    "sinhala_tsv_df = pd.read_csv(SINHALA_TSV, quoting=csv.QUOTE_NONE, sep='\\t', header=None)\r\n",
    "sinhala_tsv_df = sinhala_tsv_df.iloc[:,:-1]\r\n",
    "\r\n",
    "sundanese_tsv_df = pd.read_csv(SUNDANESE_TSV, quoting=csv.QUOTE_NONE, sep='\\t', header=None)\r\n",
    "sundanese_tsv_df = sundanese_tsv_df.iloc[:,:-1]\r\n",
    "\r\n",
    "print(\"Bengali: {}, Javanese: {}, Nepali: {}, Sinhala: {}, Sundanese: {}\".format(\r\n",
    "\tlen(bengali_tsv_df), len(javanese_tsv_df), len(nepali_tsv_df), len(sinhala_tsv_df), len(sundanese_tsv_df)\r\n",
    "))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Bengali: 218703, Javanese: 185076, Nepali: 157905, Sinhala: 185293, Sundanese: 219156\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "frames = [bengali_tsv_df, javanese_tsv_df, nepali_tsv_df, sinhala_tsv_df, sundanese_tsv_df]\r\n",
    "large_asr_tsv_df = pd.concat(frames)\r\n",
    "\r\n",
    "print(\"Total utterrances:\", len(large_asr_tsv_df))\r\n",
    "large_asr_tsv_df.head()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total utterrances: 966133\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            0      1\n",
       "0  000020a912  16cfb\n",
       "1  000039928e  976b1\n",
       "2  00005debc7  f83df\n",
       "3  00009e687c  9813c\n",
       "4  00012843bc  7ec1c"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000020a912</td>\n",
       "      <td>16cfb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000039928e</td>\n",
       "      <td>976b1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00005debc7</td>\n",
       "      <td>f83df</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00009e687c</td>\n",
       "      <td>9813c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00012843bc</td>\n",
       "      <td>7ec1c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "all_speakers = large_asr_tsv_df.iloc[:,1].unique()\r\n",
    "print(\"Total number of speakers:\", len(all_speakers))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total number of speakers: 3071\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Save th ecombined dataframe as a tsv file\r\n",
    "\r\n",
    "large_asr_tsv_df.to_csv(LARGE_ASR_TSV_LOCATION, index=False, header=False, sep=\"\\t\")"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'LARGE_TSV_LOCATION' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-57043d0511f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Save th ecombined dataframe as a tsv file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mlarge_asr_tsv_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLARGE_TSV_LOCATION\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\\t\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'LARGE_TSV_LOCATION' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# There are in total 3071 speakers in the dataset\r\n",
    "\r\n",
    "# In our splitting strategy, we keep 10% speakers for TEST\r\n",
    "TEST_CLASS_NUMBERS = 300\r\n",
    "DEV_CLASS_NUMBERS = 2771\r\n",
    "\r\n",
    "# 10% of the devset will be used for evaluating each epoch\r\n",
    "EVAL_TRAIN_RATIO = 0.10\r\n",
    "\r\n",
    "# Don't change it, to keep the work reproducible\r\n",
    "RANDOM_SEED = 99"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_wav_list(dataset_dir):\r\n",
    "    # Given a directory, return path list of all wav files\r\n",
    "    pattern = '**/*.wav'\r\n",
    "    files = glob.glob(dataset_dir + pattern , recursive=True)\r\n",
    "\r\n",
    "    # Normalize the file paths. To get file paths with '/' or '\\\\' consistently depending on OS\r\n",
    "    wav_list = [os.path.normpath(i) for i in files]\r\n",
    "    return wav_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "wav_paths_list = get_wav_list(LARGE_ASR_DATASET_DIRECTORY)\r\n",
    "len(wav_paths_list)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "966133"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Label dictionaries\r\n",
    "\r\n",
    "Three dictionaries will be helpful to us\r\n",
    "\r\n",
    "1. stem_to_speaker_dict: Given stem, who is it's speaker?\r\n",
    "2. speaker_to_paths_dict: Given a speaker, what are their audio paths?\r\n",
    "3. stem_to_path_dict: Given the stem, what is its path?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def get_stem_to_speaker_dict(tsv_loc):\r\n",
    "    # Reads the annotation tsv file provided with the dataset \r\n",
    "    # and returns a stem to speaker mapping dictionary\r\n",
    "    stem_to_speaker_d = {}\r\n",
    "\r\n",
    "    with open(tsv_loc, encoding=\"utf-8\") as tsvfile:\r\n",
    "        tsvreader = csv.reader(tsvfile, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\r\n",
    "        for line in tsvreader:\r\n",
    "            wav_file_name = line[0]\r\n",
    "            speaker_id = line[1]\r\n",
    "\r\n",
    "            stem_to_speaker_d[wav_file_name] = speaker_id\r\n",
    "\r\n",
    "    return stem_to_speaker_d"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "stem_to_speaker_dict = get_stem_to_speaker_dict(LARGE_ASR_TSV_LOCATION)\r\n",
    "print(\"Size:\", len(stem_to_speaker_dict), \"Speaker of 000020a912:\", stem_to_speaker_dict[\"000020a912\"])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Size: 966133 Speaker of 000020a912: 16cfb\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def get_speaker_to_paths_dict(wav_list, stem_to_speaker_dict):\r\n",
    "    \r\n",
    "    spk_to_path_d = {}\r\n",
    "\r\n",
    "    for wav_path in wav_list:\r\n",
    "        wav_name = Path(wav_path).stem\r\n",
    "        spk_id = stem_to_speaker_dict[wav_name]\r\n",
    "\r\n",
    "        if spk_id in spk_to_path_d.keys():\r\n",
    "            spk_to_path_d[spk_id].append(wav_path)\r\n",
    "        else:\r\n",
    "            spk_to_path_d[spk_id] = [wav_path]\r\n",
    "    \r\n",
    "    return spk_to_path_d"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "speaker_to_paths_dict = get_speaker_to_paths_dict(wav_paths_list, stem_to_speaker_dict)\r\n",
    "len(speaker_to_paths_dict.keys())"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'wav_paths_list' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-f1d8313adb18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspeaker_to_paths_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_speaker_to_paths_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwav_paths_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstem_to_speaker_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspeaker_to_paths_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wav_paths_list' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "total = 0\r\n",
    "for key in speaker_to_paths_dict.keys():\r\n",
    "    total += len(speaker_to_paths_dict[key])\r\n",
    "\r\n",
    "total"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "966133"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_stem_to_path_dict(stem_list, wav_paths_list):\r\n",
    "    # Sets are faster to search\r\n",
    "    stem_set = set(stem_list)\r\n",
    "    \r\n",
    "    stem_to_path_d ={}\r\n",
    "    for wav_path in wav_paths_list:\r\n",
    "        wav_stem = Path(wav_path).stem\r\n",
    "        if wav_stem in stem_set:\r\n",
    "            if wav_stem in stem_to_path_d:\r\n",
    "                stem_to_path_d[wav_stem].append(wav_path)\r\n",
    "            else:\r\n",
    "                stem_to_path_d[wav_stem] = [wav_path]\r\n",
    "\r\n",
    "    return stem_to_path_d          \r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "stem_list = stem_to_speaker_dict.keys()\r\n",
    "\r\n",
    "stem_to_path_dict = get_stem_to_path_dict(stem_list, wav_paths_list)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'get_stem_to_path_dict' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-435259ee95f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstem_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstem_to_speaker_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mstem_to_path_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_stem_to_path_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstem_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwav_paths_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'get_stem_to_path_dict' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "stem_to_path_dict['c47c6e6be0']"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'stem_to_path_dict' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-dfd76d637ffa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstem_to_path_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'c47c6e6be0'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'stem_to_path_dict' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split whole dataset into dev and test"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "random.seed(RANDOM_SEED)\r\n",
    "test_speakers_keys = random.sample(speaker_to_paths_dict.keys(), TEST_CLASS_NUMBERS)\r\n",
    "random.seed(RANDOM_SEED)\r\n",
    "\r\n",
    "len(test_speakers_keys)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'RANDOM_SEED' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-07b4121e6220>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRANDOM_SEED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_speakers_keys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspeaker_to_paths_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTEST_CLASS_NUMBERS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRANDOM_SEED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_speakers_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RANDOM_SEED' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "test_speakers_wavs_stems = []\r\n",
    "dev_speakers_wavs_stems = []\r\n",
    "\r\n",
    "total = 0\r\n",
    "\r\n",
    "for key in speaker_to_paths_dict.keys():\r\n",
    "    current_speaker_wavs = speaker_to_paths_dict[key]\r\n",
    "    current_speaker_wavs_stems = [Path(x).stem for x in current_speaker_wavs]\r\n",
    "    total += len(current_speaker_wavs_stems)\r\n",
    "\r\n",
    "    if key in test_speakers_keys:\r\n",
    "        test_speakers_wavs_stems += current_speaker_wavs_stems\r\n",
    "    else:\r\n",
    "        dev_speakers_wavs_stems += current_speaker_wavs_stems\r\n",
    "    \r\n",
    "print(total)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "966133\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "print(len(test_speakers_wavs_stems), len(dev_speakers_wavs_stems))\r\n",
    "print(len(test_speakers_wavs_stems) + len(dev_speakers_wavs_stems))\r\n",
    "\r\n",
    "# 42075 176628\r\n",
    "# 218703"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "94982 871151\n",
      "966133\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have divided dataset into dev and test.\r\n",
    "Now, we need to divide dev between train and eval"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "total_dev_wavs = len(dev_speakers_wavs_stems)\r\n",
    "num_eval_wavs = int(total_dev_wavs * EVAL_TRAIN_RATIO)\r\n",
    "\r\n",
    "random.seed(RANDOM_SEED)\r\n",
    "random.shuffle(dev_speakers_wavs_stems)\r\n",
    "random.seed(RANDOM_SEED)\r\n",
    "\r\n",
    "eval_stems, train_stems = dev_speakers_wavs_stems[:num_eval_wavs], dev_speakers_wavs_stems[num_eval_wavs:]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "train_size = len(train_stems)\r\n",
    "eval_size = len(eval_stems)\r\n",
    "test_size = len(test_speakers_wavs_stems)\r\n",
    "\r\n",
    "print(\"Train size:\", train_size)\r\n",
    "print(\"Eval size:\", eval_size)\r\n",
    "print(\"Test size:\", test_size)\r\n",
    "\r\n",
    "print(\"Total:\", train_size+eval_size+test_size)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train size: 784036\n",
      "Eval size: 87115\n",
      "Test size: 94982\n",
      "Total: 966133\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Writing to the output files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "with open(TRAINSET_LIST_LOCATION, \"w\") as file:\r\n",
    "    for stem in train_stems:\r\n",
    "        file.write(stem+\"\\n\")\r\n",
    "\r\n",
    "with open(EVALSET_LIST_LOCATION, \"w\") as file:\r\n",
    "    for stem in eval_stems:\r\n",
    "        file.write(stem+\"\\n\")\r\n",
    "\r\n",
    "with open(TESTSET_LIST_LOCATION, \"w\") as file:\r\n",
    "    for stem in test_speakers_wavs_stems:\r\n",
    "        file.write(stem+\"\\n\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "# Set for uniqueness\r\n",
    "# Sorted list for consistency in the order\r\n",
    "# \r\n",
    "dev_speakers_list = sorted(list(set([stem_to_speaker_dict[stem] for stem in dev_speakers_wavs_stems])))\r\n",
    "test_speakers_list = sorted(list(set([stem_to_speaker_dict[stem] for stem in test_speakers_wavs_stems])))\r\n",
    "len(dev_speakers_list)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2771"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "with open(DEVSET_CLASS_ORDER_LOCATION, \"w\") as file:\r\n",
    "    for speaker in dev_speakers_list:\r\n",
    "        file.write(speaker+\"\\n\")\r\n",
    "\r\n",
    "with open(TESTSET_CLASS_ORDER_LOCATION, \"w\") as file:\r\n",
    "    for speaker in test_speakers_list:\r\n",
    "        file.write(speaker+\"\\n\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation Trials"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "import numpy as np\r\n",
    "np.random.seed(0)\r\n",
    "\r\n",
    "NUM_TRIALS = 50000"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "# This function will generate a file that will contain which two files should be compared with cos similarities\r\n",
    "# Format:\r\n",
    "# 1 spk/_1_/audio/1.wav spk/_1_/audio/2.wav\r\n",
    "# 0 spk/_1_/audio/1.wav spk/_2_/audio/1.wav\r\n",
    "\r\n",
    "def generate_validation_trials(wav_list, nb_trial, val_trial_location, speaker_to_paths_d):\r\n",
    "    val_trial_file = open(val_trial_location, 'w')\r\n",
    "\r\n",
    "    # We define, same speaker trial as target trial\r\n",
    "    # Target trial: 1; Non-trg: 0\r\n",
    "\r\n",
    "    # There will be equal numbers of target and non target trials\r\n",
    "    nb_target_trials = int(nb_trial / 2)\r\n",
    "        \r\n",
    "    speakers_list = list(speaker_to_paths_d.keys())\r\n",
    "\r\n",
    "    #compose target trials\r\n",
    "    selected_spks = np.random.choice(speakers_list, size=nb_target_trials, replace=True)\r\n",
    "\r\n",
    "    for spk in selected_spks:\r\n",
    "        wav_paths_of_speaker = speaker_to_paths_d[spk]\r\n",
    "        \r\n",
    "        if len(wav_paths_of_speaker) < 2:\r\n",
    "            utt_a, utt_b = wav_paths_of_speaker[0], wav_paths_of_speaker[0]\r\n",
    "        else:    \r\n",
    "            utt_a, utt_b = np.random.choice(wav_paths_of_speaker, size=2, replace=False)\r\n",
    "            \r\n",
    "        utt_a_stem, utt_b_stem = Path(utt_a).stem, Path(utt_b).stem\r\n",
    "        val_trial_file.write('1 %s %s\\n'%(utt_a_stem, utt_b_stem))\r\n",
    "\r\n",
    "    #compose non-target trials\r\n",
    "    for i in range(nb_target_trials):\r\n",
    "        two_different_speakers = np.random.choice(speakers_list, size=2, replace = False)\r\n",
    "        utt_a = np.random.choice(speaker_to_paths_d[two_different_speakers[0]], size=1)[0]\r\n",
    "        utt_b = np.random.choice(speaker_to_paths_d[two_different_speakers[1]], size=1)[0]\r\n",
    "        utt_a_stem, utt_b_stem = Path(utt_a).stem, Path(utt_b).stem\r\n",
    "        val_trial_file.write('0 %s %s\\n'%(utt_a_stem, utt_b_stem))\r\n",
    "\r\n",
    "    val_trial_file.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "eval_speakers_to_path_dict = {}\r\n",
    "\r\n",
    "eval_stem_set = set(eval_stems)\r\n",
    "\r\n",
    "for speaker in speaker_to_paths_dict.keys():\r\n",
    "    for path in speaker_to_paths_dict[speaker]:\r\n",
    "        if Path(path).stem in eval_stem_set:\r\n",
    "            if speaker in eval_speakers_to_path_dict.keys():\r\n",
    "                eval_speakers_to_path_dict[speaker].append(path)\r\n",
    "            else:\r\n",
    "                eval_speakers_to_path_dict[speaker] = [path]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "total = 0\r\n",
    "for speaker in eval_speakers_to_path_dict:\r\n",
    "    total += len(eval_speakers_to_path_dict[speaker])\r\n",
    "\r\n",
    "total"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "87115"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "eval_paths = []\r\n",
    "for stem in eval_stems:\r\n",
    "    eval_paths.append(stem_to_path_dict[stem])\r\n",
    "len(eval_paths)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "87115"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "generate_validation_trials(eval_paths, NUM_TRIALS, EVAL_TRIALS_LOCATION, eval_speakers_to_path_dict)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testset trials"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "NUM_TEST_TRIALS = 50000"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "test_speakers_to_path_dict = {}\r\n",
    "\r\n",
    "test_stem_set = set(test_speakers_wavs_stems)\r\n",
    "\r\n",
    "for speaker in speaker_to_paths_dict.keys():\r\n",
    "    for path in speaker_to_paths_dict[speaker]:\r\n",
    "        if Path(path).stem in test_stem_set:\r\n",
    "            if speaker in test_speakers_to_path_dict.keys():\r\n",
    "                test_speakers_to_path_dict[speaker].append(path)\r\n",
    "            else:\r\n",
    "                test_speakers_to_path_dict[speaker] = [path]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "test_paths = []\r\n",
    "for stem in test_speakers_wavs_stems:\r\n",
    "    test_paths.append(stem_to_path_dict[stem])\r\n",
    "len(test_paths)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "42075"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "generate_validation_trials(test_speakers_wavs_stems, NUM_TEST_TRIALS, TEST_TRIALS_LOCATION, test_speakers_to_path_dict)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('pytorch': conda)"
  },
  "interpreter": {
   "hash": "83fea67622c0fa859a960c0bf8a89199ff21c56f3cdd300f29516e12427a6ea7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}