{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TinyRawNet: A Student of RawNet Speaker Recognition Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Necessary Imports:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "source": [
    "import torch\r\n",
    "from torch.utils import data\r\n",
    "\r\n",
    "import torchaudio\r\n",
    "\r\n",
    "import csv\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "import glob\r\n",
    "from pathlib import Path"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Directories and lcoations:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "source": [
    "# Directories are assumed to have a trailing '/' or '\\\\' in all the subsequent code\r\n",
    "\r\n",
    "CURRENT_WORKING_DIRECTORY = \"W:/SpeakerRecognitionResearch\"\r\n",
    "\r\n",
    "BANGLA_ASR_DATASET_DIRECTORY = \"data/BanglaASR/WavFiles/\"\r\n",
    "BANGLA_ASR_TSV_LOCATION = \"data/BanglaASR/utt_spk_text.tsv\"\r\n",
    "\r\n",
    "# To avoid file location related errors, we make sure \"SpeakerRecognitionResearch\" root folder is the current working directory.\r\n",
    "os.chdir(CURRENT_WORKING_DIRECTORY)\r\n",
    "os.getcwd()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'W:\\\\SpeakerRecognitionResearch'"
      ]
     },
     "metadata": {},
     "execution_count": 131
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Constants:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "source": [
    "# If sample_rate = 16K and number_of_samples = 32000, then each tensor will be equivalent to 2 seconds of data\r\n",
    "SAMPLE_RATE = 16000\r\n",
    "NUMBER_OF_SAMPLES = 32000\r\n",
    "\r\n",
    "# Bangla ASR Dataset has around half of second of silence in the beginning\r\n",
    "# This constant will be used to cut samples from the left of the audio\r\n",
    "TRIM_AMOUNT_TIME = 0.5"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Custom dataset for Bangla ASR\r\n",
    "\r\n",
    "This custom dataset is written with the assumption that the Dataset has been already converted into wav format. Check evaluate_asr_ds.ipynb notebook for conversion method."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "source": [
    "class BanglaAsrDataset(data.Dataset):\r\n",
    "    def __init__(self, dataset_dir, tsv_loc, target_sample_rate, target_num_samples, trim_amount_time):\r\n",
    "\r\n",
    "        tsv_dataframe = pd.read_csv(tsv_loc, quoting=csv.QUOTE_NONE, sep='\\t', header=None)\r\n",
    "\r\n",
    "        # The TSV file contains speech annotations in the third column.\r\n",
    "        # We don't need the annotations, so we drop the column\r\n",
    "        tsv_dataframe = tsv_dataframe.iloc[:,:-1]\r\n",
    "\r\n",
    "        self.dataset_dir = dataset_dir\r\n",
    "        self.wav_to_spk_mapping = dict(sorted(tsv_dataframe.values.tolist()))\r\n",
    "        self.wav_path_list = self._get_audio_path_list()\r\n",
    "        self.target_sample_rate = target_sample_rate\r\n",
    "        self.target_num_samples = target_num_samples\r\n",
    "        self.trim_amount_time = trim_amount_time\r\n",
    "        \r\n",
    "    def _get_audio_path_list(self):\r\n",
    "        pattern = '**/*.wav'\r\n",
    "        files = glob.glob(self.dataset_dir + pattern , recursive=True)\r\n",
    "\r\n",
    "        # Normalize the file paths. To get file paths with '/' or '\\\\' consistently depending on OS\r\n",
    "        wav_list = [os.path.normpath(i) for i in files]\r\n",
    "        return wav_list\r\n",
    "\r\n",
    "    def _resample_to_target_sr(self, signal, sample_rate):\r\n",
    "        if sample_rate != self.target_sample_rate:\r\n",
    "            resampler = torchaudio.transforms.Resample(sample_rate, self.target_sample_rate)\r\n",
    "            signal = resampler(signal)\r\n",
    "        return signal\r\n",
    "\r\n",
    "    def _mix_down_to_mono(self, signal):\r\n",
    "        if signal.shape[0] > 1:\r\n",
    "            signal = torch.mean(siggnal, dim=0, keepdim=True)\r\n",
    "        return signal\r\n",
    "\r\n",
    "    def _trim(self, signal):\r\n",
    "        total_samples = signal.shape[-1]\r\n",
    "\r\n",
    "        # We cut a fixed amount on the left side if the signal is big enough\r\n",
    "        trim_samples_amount = int(self.target_sample_rate * self.trim_amount_time)\r\n",
    "\r\n",
    "        if total_samples >= trim_samples_amount + self.target_num_samples:\r\n",
    "            signal = signal[: , trim_samples_amount:]\r\n",
    "            total_samples = signal.shape[-1]\r\n",
    "\r\n",
    "        # We cut from the right side if the signal is too big\r\n",
    "        if total_samples > self.target_num_samples:\r\n",
    "            signal = signal[:, :self.target_num_samples]\r\n",
    "        \r\n",
    "        # We add zero padding on the right if signal is too small\r\n",
    "        if total_samples < self.target_num_samples:\r\n",
    "            num_missing_samples = self.target_num_samples - total_samples\r\n",
    "            last_dim_padding = (0, num_missing_samples)\r\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\r\n",
    "            \r\n",
    "        return signal\r\n",
    "\r\n",
    "    def _normalize_like_sincnet(self, signal):\r\n",
    "        return signal/torch.max(torch.abs(signal))\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.wav_to_spk_mapping)\r\n",
    "\r\n",
    "    def __getitem__(self, index):\r\n",
    "        wav_path = self.wav_path_list[index]\r\n",
    "        wav_name = Path(wav_path).stem\r\n",
    "        label = self.wav_to_spk_mapping[wav_name]\r\n",
    "\r\n",
    "        signal, sample_rate = torchaudio.load(wav_path)\r\n",
    "\r\n",
    "        signal = self._resample_to_target_sr(signal, sample_rate)\r\n",
    "        signal = self._mix_down_to_mono(signal)\r\n",
    "\r\n",
    "        signal =  self._trim(signal)\r\n",
    "        signal = self._normalize_like_sincnet(signal)\r\n",
    "\r\n",
    "        return signal, label"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "source": [
    "bangla_asr_dataset = BanglaAsrDataset(\r\n",
    "    dataset_dir=BANGLA_ASR_DATASET_DIRECTORY,\r\n",
    "    tsv_loc = BANGLA_ASR_TSV_LOCATION,\r\n",
    "    target_sample_rate=SAMPLE_RATE,\r\n",
    "    target_num_samples = NUMBER_OF_SAMPLES,\r\n",
    "    trim_amount_time = TRIM_AMOUNT_TIME\r\n",
    ")\r\n",
    "\r\n",
    "assert bangla_asr_dataset.wav_to_spk_mapping['000020a912'] == '16cfb' , \"The dictionary returned wrong mapping\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "source": [
    "bangla_asr_dataset[0], bangla_asr_dataset[0][0].shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((tensor([[-0.0076, -0.0041,  0.0149,  ..., -0.1191, -0.1366, -0.1302]]),\n",
       "  '16cfb'),\n",
       " torch.Size([1, 32000]))"
      ]
     },
     "metadata": {},
     "execution_count": 135
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "source": [],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "metadata": {},
     "execution_count": 120
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('pytorch': conda)"
  },
  "interpreter": {
   "hash": "83fea67622c0fa859a960c0bf8a89199ff21c56f3cdd300f29516e12427a6ea7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}