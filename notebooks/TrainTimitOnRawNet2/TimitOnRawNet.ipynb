{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TrainRawNet2: On Timit Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Necessary Imports:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "import torchaudio\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import os"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Directories and locations:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "source": [
    "# Directories are assumed to have a trailing '/' or '\\\\' in all the subsequent code\n",
    "\n",
    "CURRENT_WORKING_DIRECTORY = \"/home/abdullah/Code/dl/SpeakerRecognitionResearch/\"\n",
    "\n",
    "TIMIT_DATASET_DIRECTORY = \"/home/abdullah/Code/datasets/timit/data/\"\n",
    "\n",
    "# This is a combination of the 2 CSV files, and it will be generated in this notebook\n",
    "TIMIT_TRAIN_CSV_LOCATION = \"/home/abdullah/Code/datasets/timit/train_data.csv\"\n",
    "TIMIT_TEST_CSV_LOCATION = \"/home/abdullah/Code/datasets/timit/test_data.csv\"\n",
    "\n",
    "# To avoid file location related errors, we make sure \"SpeakerRecognitionResearch\" root folder is the current working directory.\n",
    "os.chdir(CURRENT_WORKING_DIRECTORY)\n",
    "os.getcwd()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/home/abdullah/Code/dl/SpeakerRecognitionResearch'"
      ]
     },
     "metadata": {},
     "execution_count": 395
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "source": [
    "MODEL_SAVE_DIRECTORY = \"/home/abdullah/Code/dl/SpeakerRecognitionResearch/notebooks/TrainTimitOnRawNet2/out\"\n",
    "MODEL_LOSS_EER_OUTPUT_FILE = \"/home/abdullah/Code/dl/SpeakerRecognitionResearch/notebooks/TrainTimitOnRawNet2/out/loss_eer.txt\"\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Constants:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "source": [
    "# If sample_rate = 16K and number_of_samples = 32000, then each tensor will be equivalent to 2 seconds of data\n",
    "SAMPLE_RATE = 32000\n",
    "NUMBER_OF_SAMPLES = 32000\n",
    "\n",
    "# Timit Dataset has almost no silence in the beginning\n",
    "# This constant will be used to cut samples from the left of the audio\n",
    "TRIM_AMOUNT_TIME = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "source": [
    "# Device\n",
    "\n",
    "device = \"cuda\" #if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {}.\".format(device))\n",
    "if device==\"cuda\": print(torch.cuda.get_device_name(0))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cuda.\n",
      "NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Dataset preparation files\n",
    "\n",
    "These files were generated by the prepareDataset notebook."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "source": [
    "TRAINSET_LIST_LOCATION = \"/home/abdullah/Code/dl/SpeakerRecognitionResearch/notebooks/TrainTimitOnRawNet2/DatasetPreparations/trainset_list.txt\"\n",
    "EVALSET_LIST_LOCATION = \"/home/abdullah/Code/dl/SpeakerRecognitionResearch/notebooks/TrainTimitOnRawNet2/DatasetPreparations/evalset_list.txt\"\n",
    "TESTSET_LIST_LOCATION = \"/home/abdullah/Code/dl/SpeakerRecognitionResearch/notebooks/TrainTimitOnRawNet2/DatasetPreparations/testset_list.txt\"\n",
    "DEVSET_CLASS_ORDER_LOCATION = \"/home/abdullah/Code/dl/SpeakerRecognitionResearch/notebooks/TrainTimitOnRawNet2/DatasetPreparations/devset_classes.txt\"\n",
    "TESTSET_CLASS_ORDER_LOCATION = \"/home/abdullah/Code/dl/SpeakerRecognitionResearch/notebooks/TrainTimitOnRawNet2/DatasetPreparations/testset_classes.txt\"\n",
    "EVAL_TRIALS_LOCATION = \"/home/abdullah/Code/dl/SpeakerRecognitionResearch/notebooks/TrainTimitOnRawNet2/DatasetPreparations/eval_trials.txt\"\n",
    "TEST_TRIALS_LOCATION = \"/home/abdullah/Code/dl/SpeakerRecognitionResearch/notebooks/TrainTimitOnRawNet2/DatasetPreparations/test_trials.txt\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "source": [
    "def load_list_from_file(file_loc, trial_file=False):\n",
    "    list_ = []\n",
    "    if not trial_file:\n",
    "        with open(file_loc, \"r\") as file:\n",
    "            for line in file.readlines():\n",
    "                list_.append(line.strip())\n",
    "    else:\n",
    "        # The trial file has 3 elements in each line\n",
    "        with open(file_loc, \"r\") as file:\n",
    "            for line in file.readlines():\n",
    "                expected, utt1, utt2 = line.strip().split(\" \")\n",
    "                list_.append( (expected, utt1, utt2) )\n",
    "    return list_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "source": [
    "trainset_stems = load_list_from_file(TRAINSET_LIST_LOCATION)\n",
    "evalset_stems = load_list_from_file(EVALSET_LIST_LOCATION)\n",
    "testset_stems = load_list_from_file(TESTSET_LIST_LOCATION)\n",
    "\n",
    "devset_classes = load_list_from_file(DEVSET_CLASS_ORDER_LOCATION)\n",
    "\n",
    "eval_trials = load_list_from_file(EVAL_TRIALS_LOCATION, trial_file=True)\n",
    "\n",
    "len(trainset_stems) + len(evalset_stems) + len(testset_stems)\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4956"
      ]
     },
     "metadata": {},
     "execution_count": 401
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Custom dataset for Timit\n",
    "\n",
    "This custom dataset is written with the assumption that the Dataset has been already converted into wav format. Check evaluate_asr_ds.ipynb notebook for conversion method.\n",
    "\n",
    "Strategy:\n",
    "1. train_set will be strictly used for training.\n",
    "2. eval_set will be used for evaluation during training.\n",
    "3. test_set will be used to assess the model after training.\n",
    "\n",
    "Dev indicates data used during the whole training process, which indicates both train and eval datasets.\n",
    "\n",
    "Each set will have entirely different speakers (classes).\n",
    "\n",
    "A stem is the name of the file. I.E: \"a/b/c/bat.jpg\" --stem--> bat"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "source": [
    "class TimitDataset(data.Dataset):\n",
    "    def __init__(self, dataset_dir, wav_path_list, path_to_spk_dict, target_sample_rate, target_num_samples, trim_amount_time, device, is_evalset=False):\n",
    "\n",
    "        # wav_list will have a list of stems that will be included in this dataset\n",
    "\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.path_to_spk_dict = path_to_spk_dict\n",
    "        self.wav_path_list = wav_path_list\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.target_num_samples = target_num_samples\n",
    "        self.trim_amount_time = trim_amount_time\n",
    "        self.device = device\n",
    "        self.is_evalset = is_evalset\n",
    "\n",
    "    def _resample_to_target_sr(self, signal, sample_rate):\n",
    "        if sample_rate != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(\n",
    "                sample_rate, self.target_sample_rate)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "\n",
    "    def _mix_down_to_mono(self, signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal\n",
    "\n",
    "    def _trim(self, signal):\n",
    "        total_samples = signal.shape[-1]\n",
    "\n",
    "        # We cut a fixed amount on the left side if the signal is big enough\n",
    "        trim_samples_amount = int(\n",
    "            self.target_sample_rate * self.trim_amount_time)\n",
    "\n",
    "        if total_samples >= trim_samples_amount + self.target_num_samples:\n",
    "            signal = signal[:, trim_samples_amount:]\n",
    "            total_samples = signal.shape[-1]\n",
    "\n",
    "        # We cut from the right side if the signal is too big\n",
    "        if total_samples > self.target_num_samples:\n",
    "            signal = signal[:, :self.target_num_samples]\n",
    "\n",
    "        # We add zero padding on the right if signal is too small\n",
    "        if total_samples < self.target_num_samples:\n",
    "            num_missing_samples = self.target_num_samples - total_samples\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "\n",
    "        return signal\n",
    "\n",
    "    def _normalize_like_sincnet(self, signal):\n",
    "        return signal/torch.max(torch.abs(signal))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.wav_path_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        wav_path = self.wav_path_list[index]\n",
    "        label_index = self.path_to_spk_dict(wav_path)\n",
    "\n",
    "\n",
    "        signal, sample_rate = torchaudio.load(wav_path)\n",
    "\n",
    "        # moving to CPU/CUDA is now done in the training phase\n",
    "        # signal = signal.to(self.device)\n",
    "\n",
    "        signal = self._resample_to_target_sr(signal, sample_rate)\n",
    "        signal = self._mix_down_to_mono(signal)\n",
    "\n",
    "        signal = self._trim(signal)\n",
    "        signal = self._normalize_like_sincnet(signal)\n",
    "\n",
    "        # signal = signal.squeeze(0)\n",
    "\n",
    "        if self.is_evalset:\n",
    "            return signal, label_index, wav_path\n",
    "        else:\n",
    "            return signal, label_index\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "source": [
    "# returns the index number of a speaker\n",
    "def get_index_to_speaker_dict(stem_name):   \n",
    "\n",
    "    with open(\"/home/abdullah/Code/dl/SpeakerRecognitionResearch/notebooks/TrainTimitOnRawNet2/DatasetPreparations/devset_classes.txt\", encoding=\"utf-8\") as devset:\n",
    "        stem_name = stem_name.split(\"/\")[-1]\n",
    "        stem_name = stem_name[:-4]\n",
    "        speaker_list = devset.readlines()\n",
    "        speaker_index = speaker_list.index(f\"{stem_name}\\n\")\n",
    "\n",
    "    return speaker_index\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "source": [
    "# returns the length of the speaker list\n",
    "def get_speaker_list_length():\n",
    "    with open(\"/home/abdullah/Code/dl/SpeakerRecognitionResearch/notebooks/TrainTimitOnRawNet2/DatasetPreparations/devset_classes.txt\", encoding=\"utf-8\") as devset:\n",
    "        speaker_list = devset.readlines()\n",
    "        return len(speaker_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "source": [
    "trainset = TimitDataset(\n",
    "    dataset_dir=TIMIT_DATASET_DIRECTORY,\n",
    "    wav_path_list=trainset_stems,\n",
    "    path_to_spk_dict=get_index_to_speaker_dict,\n",
    "    target_sample_rate=SAMPLE_RATE,\n",
    "    target_num_samples = NUMBER_OF_SAMPLES,\n",
    "    trim_amount_time = TRIM_AMOUNT_TIME,\n",
    "    device = device\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "source": [
    "evalset = TimitDataset(\n",
    "    dataset_dir=TIMIT_DATASET_DIRECTORY,\n",
    "    wav_path_list=evalset_stems,\n",
    "    path_to_spk_dict=get_index_to_speaker_dict,\n",
    "    target_sample_rate=SAMPLE_RATE,\n",
    "    target_num_samples = NUMBER_OF_SAMPLES,\n",
    "    trim_amount_time = TRIM_AMOUNT_TIME,\n",
    "    device = device,\n",
    "    is_evalset=True\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "source": [
    "trainset[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[ 0.0017,  0.0014,  0.0003,  ..., -0.0218, -0.0290, -0.0671]]), 1)"
      ]
     },
     "metadata": {},
     "execution_count": 407
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model !"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "source": [
    "from torch import nn\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## FRM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "source": [
    "class FRM(nn.Module):\n",
    "    def __init__(self, nb_dim, do_add = True, do_mul = True):\n",
    "        super(FRM, self).__init__()\n",
    "        self.fc = nn.Linear(nb_dim, nb_dim)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.do_add = do_add\n",
    "        self.do_mul = do_mul\n",
    "    def forward(self, x):\n",
    "        y = F.adaptive_avg_pool1d(x, 1).view(x.size(0), -1)\n",
    "        \n",
    "        y = self.sig(self.fc(y)).view(x.size(0), x.size(1), -1)\n",
    "\n",
    "        if self.do_mul: x = x * y\n",
    "        if self.do_add: x = x + y\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Residual Block wFRM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "source": [
    "class Residual_block_wFRM(nn.Module):\n",
    "    def __init__(self, nb_filts, first = False):\n",
    "        super(Residual_block_wFRM, self).__init__()\n",
    "        self.first = first\n",
    "        if not self.first:\n",
    "            self.bn1 = nn.BatchNorm1d(num_features = nb_filts[0])\n",
    "        self.lrelu = nn.LeakyReLU()\n",
    "        self.lrelu_keras = nn.LeakyReLU(negative_slope=0.3)\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels = nb_filts[0],\n",
    "            out_channels = nb_filts[1],\n",
    "            kernel_size = 3,\n",
    "            padding = 1,\n",
    "            stride = 1)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features = nb_filts[1])\n",
    "        self.conv2 = nn.Conv1d(in_channels = nb_filts[1],\n",
    "            out_channels = nb_filts[1],\n",
    "            padding = 1,\n",
    "            kernel_size = 3,\n",
    "            stride = 1)\n",
    "        \n",
    "        if nb_filts[0] != nb_filts[1]:\n",
    "            self.downsample = True\n",
    "            self.conv_downsample = nn.Conv1d(in_channels = nb_filts[0],\n",
    "                out_channels = nb_filts[1],\n",
    "                padding = 0,\n",
    "                kernel_size = 1,\n",
    "                stride = 1)\n",
    "            \n",
    "        else:\n",
    "            self.downsample = False\n",
    "        self.mp = nn.MaxPool1d(3)\n",
    "        self.frm = FRM(\n",
    "            nb_dim = nb_filts[1],\n",
    "            do_add = True,\n",
    "            do_mul = True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if not self.first:\n",
    "            out = self.bn1(x)\n",
    "            out = self.lrelu_keras(out)\n",
    "        else:\n",
    "            out = x\n",
    "            \n",
    "        out = self.conv1(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.lrelu_keras(out)\n",
    "        out = self.conv2(out)\n",
    "        \n",
    "        if self.downsample:\n",
    "            identity = self.conv_downsample(identity)\n",
    "            \n",
    "        out += identity\n",
    "        out = self.mp(out)\n",
    "        out = self.frm(out)\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LayerNorm"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "source": [
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm,self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(features))\n",
    "        self.beta = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SincConv Fast"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "source": [
    "class SincConv_fast(nn.Module):\n",
    "    \"\"\"Sinc-based convolution\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_channels : `int`\n",
    "        Number of input channels. Must be 1.\n",
    "    out_channels : `int`\n",
    "        Number of filters.\n",
    "    kernel_size : `int`\n",
    "        Filter length.\n",
    "    sample_rate : `int`, optional\n",
    "        Sample rate. Defaults to 16000.\n",
    "    Usage\n",
    "    -----\n",
    "    See `torch.nn.Conv1d`\n",
    "    Reference\n",
    "    ---------\n",
    "    Mirco Ravanelli, Yoshua Bengio,\n",
    "    \"Speaker Recognition from raw waveform with SincNet\".\n",
    "    https://arxiv.org/abs/1808.00158\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def to_mel(hz):\n",
    "        return 2595 * np.log10(1 + hz / 700)\n",
    "\n",
    "    @staticmethod\n",
    "    def to_hz(mel):\n",
    "        return 700 * (10 ** (mel / 2595) - 1)\n",
    "\n",
    "    def __init__(self, out_channels, kernel_size, sample_rate=16000, in_channels=1,\n",
    "                 stride=1, padding=0, dilation=1, bias=False, groups=1, min_low_hz=50, min_band_hz=50):\n",
    "\n",
    "        super(SincConv_fast,self).__init__()\n",
    "\n",
    "        if in_channels != 1:\n",
    "            #msg = (f'SincConv only support one input channel '\n",
    "            #       f'(here, in_channels = {in_channels:d}).')\n",
    "            msg = \"SincConv only support one input channel (here, in_channels = {%i})\" % (in_channels)\n",
    "            raise ValueError(msg)\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        # Forcing the filters to be odd (i.e, perfectly symmetrics)\n",
    "        if kernel_size%2==0:\n",
    "            self.kernel_size=self.kernel_size+1\n",
    "            \n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "\n",
    "        if bias:\n",
    "            raise ValueError('SincConv does not support bias.')\n",
    "        if groups > 1:\n",
    "            raise ValueError('SincConv does not support groups.')\n",
    "\n",
    "        self.sample_rate = sample_rate\n",
    "        self.min_low_hz = min_low_hz\n",
    "        self.min_band_hz = min_band_hz\n",
    "\n",
    "        # initialize filterbanks such that they are equally spaced in Mel scale\n",
    "        low_hz = 30\n",
    "        high_hz = self.sample_rate / 2 - (self.min_low_hz + self.min_band_hz)\n",
    "\n",
    "        mel = np.linspace(self.to_mel(low_hz),\n",
    "                          self.to_mel(high_hz),\n",
    "                          self.out_channels + 1)\n",
    "        hz = self.to_hz(mel)\n",
    "        \n",
    "\n",
    "        # filter lower frequency (out_channels, 1)\n",
    "        self.low_hz_ = nn.Parameter(torch.Tensor(hz[:-1]).view(-1, 1))\n",
    "\n",
    "        # filter frequency band (out_channels, 1)\n",
    "        self.band_hz_ = nn.Parameter(torch.Tensor(np.diff(hz)).view(-1, 1))\n",
    "\n",
    "        # Hamming window\n",
    "        #self.window_ = torch.hamming_window(self.kernel_size)\n",
    "        n_lin=torch.linspace(0, (self.kernel_size/2)-1, steps=int((self.kernel_size/2))) # computing only half of the window\n",
    "        self.window_=0.54-0.46*torch.cos(2*math.pi*n_lin/self.kernel_size);\n",
    "\n",
    "        # (1, kernel_size/2)\n",
    "        n = (self.kernel_size - 1) / 2.0\n",
    "        self.n_ = 2*math.pi*torch.arange(-n, 0).view(1, -1) / self.sample_rate # Due to symmetry, I only need half of the time axes\n",
    "\n",
    "    def forward(self, waveforms):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        waveforms : `torch.Tensor` (batch_size, 1, n_samples)\n",
    "            Batch of waveforms.\n",
    "        Returns\n",
    "        -------\n",
    "        features : `torch.Tensor` (batch_size, out_channels, n_samples_out)\n",
    "            Batch of sinc filters activations.\n",
    "        \"\"\"\n",
    "\n",
    "        self.n_ = self.n_.to(waveforms.device)\n",
    "\n",
    "        self.window_ = self.window_.to(waveforms.device)\n",
    "\n",
    "        low = self.min_low_hz  + torch.abs(self.low_hz_)\n",
    "        \n",
    "        high = torch.clamp(low + self.min_band_hz + torch.abs(self.band_hz_),self.min_low_hz,self.sample_rate/2)\n",
    "        band=(high-low)[:,0]\n",
    "        \n",
    "        f_times_t_low = torch.matmul(low, self.n_)\n",
    "        f_times_t_high = torch.matmul(high, self.n_)\n",
    "\n",
    "        band_pass_left=((torch.sin(f_times_t_high)-torch.sin(f_times_t_low))/(self.n_/2))*self.window_ # Equivalent of Eq.4 of the reference paper (SPEAKER RECOGNITION FROM RAW WAVEFORM WITH SINCNET). I just have expanded the sinc and simplified the terms. This way I avoid several useless computations. \n",
    "        band_pass_center = 2*band.view(-1,1)\n",
    "        band_pass_right= torch.flip(band_pass_left,dims=[1])\n",
    "        \n",
    "        \n",
    "        band_pass=torch.cat([band_pass_left,band_pass_center,band_pass_right],dim=1)\n",
    "\n",
    "        \n",
    "        band_pass = band_pass / (2*band[:,None])\n",
    "        \n",
    "\n",
    "        self.filters = (band_pass).view(\n",
    "            self.out_channels, 1, self.kernel_size)\n",
    "\n",
    "        return F.conv1d(waveforms, self.filters, stride=self.stride,\n",
    "                        padding=self.padding, dilation=self.dilation,\n",
    "                         bias=None, groups=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RawNet2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "source": [
    "class RawNet2(nn.Module):\n",
    "    def __init__(self, d_args):\n",
    "        super(RawNet2, self).__init__()\n",
    "\n",
    "        self.ln = LayerNorm(d_args['nb_samp'])\n",
    "        self.first_conv = SincConv_fast(in_channels = d_args['in_channels'],\n",
    "            out_channels = d_args['filts'][0],\n",
    "            kernel_size = d_args['first_conv']\n",
    "            )\n",
    "        \n",
    "        self.first_bn = nn.BatchNorm1d(num_features = d_args['filts'][0])\n",
    "        self.lrelu = nn.LeakyReLU()\n",
    "        self.lrelu_keras = nn.LeakyReLU(negative_slope = 0.3)\n",
    "        \n",
    "        self.block0 = nn.Sequential(Residual_block_wFRM(nb_filts = d_args['filts'][1], first = True))\n",
    "        self.block1 = nn.Sequential(Residual_block_wFRM(nb_filts = d_args['filts'][1]))\n",
    " \n",
    "        self.block2 = nn.Sequential(Residual_block_wFRM(nb_filts = d_args['filts'][2]))\n",
    "        d_args['filts'][2][0] = d_args['filts'][2][1]\n",
    "        self.block3 = nn.Sequential(Residual_block_wFRM(nb_filts = d_args['filts'][2]))\n",
    "        self.block4 = nn.Sequential(Residual_block_wFRM(nb_filts = d_args['filts'][2]))\n",
    "        self.block5 = nn.Sequential(Residual_block_wFRM(nb_filts = d_args['filts'][2]))\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        self.bn_before_gru = nn.BatchNorm1d(num_features = d_args['filts'][2][-1])\n",
    "        self.gru = nn.GRU(input_size = d_args['filts'][2][-1],\n",
    "            hidden_size = d_args['gru_node'],\n",
    "            num_layers = d_args['nb_gru_layer'],\n",
    "            batch_first = True)\n",
    "\n",
    "        \n",
    "        self.fc1_gru = nn.Linear(in_features = d_args['gru_node'],\n",
    "            out_features = d_args['nb_fc_node'])\n",
    "        self.fc2_gru = nn.Linear(in_features = d_args['nb_fc_node'],\n",
    "            out_features = d_args['nb_classes'],\n",
    "            bias = True)\n",
    "        \n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, y = 0, is_test=False):\n",
    "        #follow sincNet recipe\n",
    "        nb_samp = x.shape[0]\n",
    "        len_seq = x.shape[1]\n",
    "\n",
    "        x = self.ln(x)\n",
    "        x=x.view(nb_samp,1,len_seq)\n",
    "        x = F.max_pool1d(torch.abs(self.first_conv(x)), 3)\n",
    "        x = self.first_bn(x)\n",
    "        x = self.lrelu_keras(x)\n",
    "        \n",
    "        x = self.block0(x)\n",
    "        x = self.block1(x)\n",
    "\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "\n",
    "        x = self.bn_before_gru(x)\n",
    "        x = self.lrelu_keras(x)\n",
    "        x = x.permute(0, 2, 1)  #(batch, filt, time) >> (batch, time, filt)\n",
    "        self.gru.flatten_parameters()\n",
    "        x, _ = self.gru(x)\n",
    "        x = x[:,-1,:]\n",
    "        code = self.fc1_gru(x)\n",
    "        if is_test: return code\n",
    "        \n",
    "        code_norm = code.norm(p=2,dim=1, keepdim=True) / 10.\n",
    "        code = torch.div(code, code_norm)\n",
    "        out = self.fc2_gru(code)\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "source": [
    "model_dict = {}\n",
    "model_dict['nb_classes'] = get_speaker_list_length()\n",
    "model_dict['first_conv'] = 251\n",
    "model_dict['in_channels'] = 1\n",
    "model_dict['filts'] = [128, [128,128], [128,128], [256,256]]\n",
    "model_dict['m_blocks'] = [2, 4]\n",
    "model_dict['nb_fc_att_node'] =[1]\n",
    "model_dict['nb_fc_node'] = 1024\n",
    "model_dict['gru_node'] = 1024\n",
    "model_dict['nb_gru_layer'] = 1\n",
    "model_dict['nb_samp'] = NUMBER_OF_SAMPLES\n",
    "\n",
    "model_dict['lr_decay'] = \"keras\"\n",
    "model_dict['do_lr_decay'] = True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "AMSGRAD = True\n",
    "EPOCHS = 40\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Higher number may cause errors in notebook\n",
    "NUMBER_OF_WORKERS = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "source": [
    "trainset_loader = data.DataLoader(trainset,\n",
    "            batch_size = BATCH_SIZE, \n",
    "            shuffle = False,\n",
    "            drop_last = False,\n",
    "            num_workers = NUMBER_OF_WORKERS)\n",
    "\n",
    "evalset_loader = data.DataLoader(evalset,\n",
    "            batch_size = 1, \n",
    "            shuffle = False,\n",
    "            drop_last = False,\n",
    "            num_workers = NUMBER_OF_WORKERS)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "source": [
    "torch.cuda.empty_cache()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Batch explained:\n",
    "\n",
    "If batch size = 4\n",
    "\n",
    "One batch = [ tensor([[[x,x,x]]], [[x,x,x]], [[x,x,x]], [[x,x,x]]])    , (label1, label2, label3, label4) ]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "source": [
    "model = RawNet2(model_dict)\n",
    "\n",
    "model.to(device)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RawNet2(\n",
       "  (ln): LayerNorm()\n",
       "  (first_conv): SincConv_fast()\n",
       "  (first_bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "  (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "  (block0): Sequential(\n",
       "    (0): Residual_block_wFRM(\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "      (frm): FRM(\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (sig): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block1): Sequential(\n",
       "    (0): Residual_block_wFRM(\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "      (frm): FRM(\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (sig): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block2): Sequential(\n",
       "    (0): Residual_block_wFRM(\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "      (frm): FRM(\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (sig): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block3): Sequential(\n",
       "    (0): Residual_block_wFRM(\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "      (frm): FRM(\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (sig): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block4): Sequential(\n",
       "    (0): Residual_block_wFRM(\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "      (frm): FRM(\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (sig): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block5): Sequential(\n",
       "    (0): Residual_block_wFRM(\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "      (frm): FRM(\n",
       "        (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (sig): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (bn_before_gru): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (gru): GRU(128, 1024, batch_first=True)\n",
       "  (fc1_gru): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (fc2_gru): Linear(in_features=1024, out_features=1090, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 419
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "source": [
    "def keras_lr_decay(step, decay = 0.0001):\n",
    "    return 1./(1. + decay * step)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "source": [
    "params = [\n",
    "    {\n",
    "        'params': [\n",
    "            param for name, param in model.named_parameters()\n",
    "            if 'bn' not in name\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'params': [\n",
    "            param for name, param in model.named_parameters()\n",
    "            if 'bn' in name\n",
    "        ],\n",
    "        'weight_decay':\n",
    "        0\n",
    "    },\n",
    "]\n",
    "\n",
    "criterion = {}\n",
    "criterion['cce'] = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(params,\n",
    "            lr = LEARNING_RATE,\n",
    "            weight_decay = WEIGHT_DECAY,\n",
    "            amsgrad = AMSGRAD)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda step: keras_lr_decay(step))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "source": [
    "def cos_sim(a,b):\n",
    "    return np.dot(a,b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "source": [
    "def get_stem_to_path_dict(stem_list, wav_paths_list):\n",
    "    # Sets are faster to search\n",
    "    stem_set = set(stem_list)\n",
    "    stem_to_path_d = {}\n",
    "    for wav_path in wav_paths_list:\n",
    "        wav_stem = Path(wav_path).stem\n",
    "        if wav_path in stem_set:\n",
    "            stem_to_path_d[wav_stem] = wav_path\n",
    "\n",
    "\n",
    "    return stem_to_path_d\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "source": [
    "def get_wav_list(dataset_dir):\n",
    "    # Given a directory, return path list of all wav files\n",
    "    pattern = '**/*.wav'\n",
    "    files = glob.glob(dataset_dir + pattern , recursive=True)\n",
    "\n",
    "    # Normalize the file paths. To get file paths with '/' or '\\\\' consistently depending on OS\n",
    "    wav_list = [os.path.normpath(i) for i in files]\n",
    "    return wav_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "source": [
    "stem_to_path_dict = get_stem_to_path_dict(evalset_stems, get_wav_list(TIMIT_DATASET_DIRECTORY))\n",
    "\n",
    "for key in stem_to_path_dict:    \n",
    "    print(stem_to_path_dict[key])\n",
    "    break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/abdullah/Code/datasets/timit/data/TRAIN/DR1/FCJF0/SA1.WAV.wav\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "source": [
    "def evaluate_model(model, evalset_loader, device, eval_trials, stem_to_path_dict):\n",
    "\n",
    "    # The number of validation trials will be twice the number of data in val_dataset\n",
    "    # One target trial, another non target trial\n",
    "\n",
    "    model.eval()\n",
    "    with torch.set_grad_enabled(False):\n",
    "        #1st, extract speaker embeddings.\n",
    "        wav_path_to_embeddings_dict = {}\n",
    "\n",
    "        with tqdm(total = len(evalset_loader), ncols = 70) as pbar:\n",
    "            for m_batch in evalset_loader:\n",
    "\n",
    "                l_code = []\n",
    "                for batch in m_batch[0]:\n",
    "                    batch = batch.to(device)\n",
    "                    code = model(x = batch, is_test=True)\n",
    "                    l_code.extend(code.cpu().numpy())\n",
    "                \n",
    "                wav_stem = Path(m_batch[2][0]).stem\n",
    "                wav_path_to_embeddings_dict[wav_stem] = np.mean(l_code, axis=0)\n",
    "                pbar.update(1)\n",
    "        \n",
    "        # print(\"Key\", normalized_wav_path)\n",
    "        # 2nd, calculate EER\n",
    "        y_score = [] # score for each sample\n",
    "        y = [] # label for each sample \n",
    "        \n",
    "        for trial in eval_trials:\n",
    "            trg, utt_a, utt_b = trial\n",
    "            y.append(int(trg))\n",
    "\n",
    "            y_score.append(cos_sim(wav_path_to_embeddings_dict[utt_a], wav_path_to_embeddings_dict[utt_b]))\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(y, y_score, pos_label=1)\n",
    "        eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "    return eer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "source": [
    "def train_model(model, db_gen, evalset_loader, optimizer, epoch, args, device, lr_scheduler, criterion, eval_trials, stem_to_path_dict):\n",
    "    # Defining it up here to make it accessable outside loop. I.E: For saving to file\n",
    "    loss = 0.0\n",
    "    model.train()\n",
    "    with tqdm(total = len(db_gen), ncols = 70) as pbar:\n",
    "        for m_batch, m_label in db_gen:\n",
    "            \n",
    "            m_batch = m_batch.squeeze(1)\n",
    "\n",
    "            m_batch, m_label = m_batch.to(device), m_label.to(device)\n",
    "\n",
    "            output = model(m_batch, m_label)\n",
    "            \n",
    "            cce_loss = criterion['cce'](output, m_label)\n",
    "            loss = cce_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            pbar.set_description('epoch: %d, cce:%.3f'%(epoch, cce_loss))\n",
    "            pbar.update(1)\n",
    "            if args['do_lr_decay']:\n",
    "                if model_dict['lr_decay'] == 'keras': lr_scheduler.step()\n",
    "    eer = evaluate_model(model, evalset_loader, device, eval_trials, stem_to_path_dict)\n",
    "    print(\"Validation set EER:\", eer)\n",
    "\n",
    "    # Save EER and model to file\n",
    "    with open(MODEL_LOSS_EER_OUTPUT_FILE, \"a+\") as file:\n",
    "        file.write(\"Epoch: {}, Loss: {}, EER: {}\\n\".format(epoch, loss, eer))\n",
    "    \n",
    "    torch.save(model.state_dict(), MODEL_SAVE_DIRECTORY+\"model_epoch_\"+str(epoch)+\".pth\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train_model(model = model,\n",
    "        db_gen = trainset_loader,\n",
    "        args = model_dict,\n",
    "        evalset_loader=evalset_loader,\n",
    "        optimizer = optimizer,\n",
    "        lr_scheduler = lr_scheduler,\n",
    "        criterion = criterion,\n",
    "        device = device,\n",
    "        epoch = epoch,\n",
    "        eval_trials=eval_trials,\n",
    "        stem_to_path_dict=stem_to_path_dict)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch: 0, cce:4.863: 100%|████████████| 51/51 [00:25<00:00,  1.98it/s]\n",
      "100%|██████████████████████████████| 360/360 [00:02<00:00, 171.28it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation set EER: 0.2894400000000026\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch: 1, cce:3.727: 100%|████████████| 51/51 [00:25<00:00,  1.99it/s]\n",
      "100%|██████████████████████████████| 360/360 [00:02<00:00, 167.75it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation set EER: 0.2727200000000746\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch: 2, cce:3.275: 100%|████████████| 51/51 [00:25<00:00,  1.98it/s]\n",
      "100%|██████████████████████████████| 360/360 [00:02<00:00, 167.07it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation set EER: 0.26732\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch: 3, cce:2.941: 100%|████████████| 51/51 [00:25<00:00,  1.98it/s]\n",
      "100%|██████████████████████████████| 360/360 [00:02<00:00, 174.78it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation set EER: 0.27256000000008546\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch: 4, cce:2.514: 100%|████████████| 51/51 [00:25<00:00,  1.98it/s]\n",
      "100%|██████████████████████████████| 360/360 [00:02<00:00, 173.00it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation set EER: 0.2760399999999657\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch: 5, cce:2.022: 100%|████████████| 51/51 [00:25<00:00,  1.98it/s]\n",
      "100%|██████████████████████████████| 360/360 [00:02<00:00, 170.23it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation set EER: 0.2702000000004572\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch: 6, cce:1.770: 100%|████████████| 51/51 [00:25<00:00,  2.00it/s]\n",
      "100%|██████████████████████████████| 360/360 [00:02<00:00, 174.71it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation set EER: 0.2702399999992703\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch: 7, cce:1.532: 100%|████████████| 51/51 [00:24<00:00,  2.05it/s]\n",
      "100%|██████████████████████████████| 360/360 [00:02<00:00, 176.67it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation set EER: 0.28424000000001437\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch: 8, cce:1.290: 100%|████████████| 51/51 [00:24<00:00,  2.05it/s]\n",
      "100%|██████████████████████████████| 360/360 [00:02<00:00, 176.04it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation set EER: 0.2716800000001695\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch: 9, cce:1.211: 100%|████████████| 51/51 [00:24<00:00,  2.05it/s]\n",
      "100%|██████████████████████████████| 360/360 [00:02<00:00, 175.91it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation set EER: 0.29156000000017834\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch: 10, cce:0.787: 100%|███████████| 51/51 [00:24<00:00,  2.05it/s]\n",
      "100%|██████████████████████████████| 360/360 [00:02<00:00, 165.69it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation set EER: 0.3030399999992981\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch: 11, cce:0.640: 100%|███████████| 51/51 [00:24<00:00,  2.12it/s]\n",
      "100%|██████████████████████████████| 360/360 [00:02<00:00, 179.73it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation set EER: 0.29507999999999995\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch: 12, cce:0.475: 100%|███████████| 51/51 [00:23<00:00,  2.16it/s]\n",
      "100%|██████████████████████████████| 360/360 [00:02<00:00, 179.07it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation set EER: 0.2791599999989672\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch: 13, cce:0.326: 100%|███████████| 51/51 [00:23<00:00,  2.18it/s]\n",
      "100%|██████████████████████████████| 360/360 [00:02<00:00, 179.55it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation set EER: 0.2705999999991925\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch: 14, cce:0.265: 100%|███████████| 51/51 [00:23<00:00,  2.18it/s]\n",
      "100%|██████████████████████████████| 360/360 [00:01<00:00, 180.10it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation set EER: 0.2751600000009906\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch: 15, cce:0.149: 100%|███████████| 51/51 [00:23<00:00,  2.18it/s]\n",
      "100%|██████████████████████████████| 360/360 [00:01<00:00, 180.62it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation set EER: 0.28220000000000006\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch: 16, cce:0.080: 100%|███████████| 51/51 [00:23<00:00,  2.18it/s]\n",
      "100%|██████████████████████████████| 360/360 [00:02<00:00, 176.36it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation set EER: 0.28344000000002606\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch: 17, cce:0.074: 100%|███████████| 51/51 [00:23<00:00,  2.18it/s]\n",
      "100%|██████████████████████████████| 360/360 [00:02<00:00, 179.70it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation set EER: 0.26592000000000016\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch: 18, cce:0.048: 100%|███████████| 51/51 [00:23<00:00,  2.16it/s]\n",
      "100%|██████████████████████████████| 360/360 [00:02<00:00, 179.02it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation set EER: 0.2648400000000003\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch: 19, cce:0.034: 100%|███████████| 51/51 [00:23<00:00,  2.16it/s]\n",
      "100%|██████████████████████████████| 360/360 [00:02<00:00, 165.90it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation set EER: 0.27063999999999994\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch: 20, cce:0.028: 100%|███████████| 51/51 [00:23<00:00,  2.16it/s]\n",
      "100%|██████████████████████████████| 360/360 [00:02<00:00, 178.95it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation set EER: 0.27292000000006295\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch: 21, cce:0.023: 100%|███████████| 51/51 [00:23<00:00,  2.17it/s]\n",
      "100%|██████████████████████████████| 360/360 [00:01<00:00, 180.78it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation set EER: 0.2751600000000055\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch: 22, cce:0.020: 100%|███████████| 51/51 [00:23<00:00,  2.18it/s]\n",
      "100%|██████████████████████████████| 360/360 [00:02<00:00, 177.00it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation set EER: 0.2751600000000055\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch: 23, cce:0.019: 100%|███████████| 51/51 [00:25<00:00,  2.04it/s]\n",
      "100%|██████████████████████████████| 360/360 [00:02<00:00, 164.41it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation set EER: 0.27456\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch: 24, cce:0.017: 100%|███████████| 51/51 [00:24<00:00,  2.10it/s]\n",
      "100%|██████████████████████████████| 360/360 [00:02<00:00, 178.32it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation set EER: 0.2749200000000077\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch: 25, cce:0.016: 100%|███████████| 51/51 [00:23<00:00,  2.14it/s]\n",
      "100%|██████████████████████████████| 360/360 [00:02<00:00, 176.89it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation set EER: 0.2749200000000077\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch: 26, cce:0.016: 100%|███████████| 51/51 [00:23<00:00,  2.15it/s]\n",
      "100%|██████████████████████████████| 360/360 [00:02<00:00, 167.32it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation set EER: 0.27384000000002634\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch: 27, cce:0.015: 100%|███████████| 51/51 [00:23<00:00,  2.16it/s]\n",
      "100%|██████████████████████████████| 360/360 [00:02<00:00, 174.73it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation set EER: 0.27415999999999996\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch: 28, cce:0.014: 100%|███████████| 51/51 [00:23<00:00,  2.15it/s]\n",
      "100%|██████████████████████████████| 360/360 [00:02<00:00, 164.29it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation set EER: 0.2740400000000214\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch: 29, cce:0.014: 100%|███████████| 51/51 [00:23<00:00,  2.16it/s]\n",
      "100%|██████████████████████████████| 360/360 [00:02<00:00, 174.78it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation set EER: 0.2774\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch: 30, cce:0.013: 100%|███████████| 51/51 [00:23<00:00,  2.15it/s]\n",
      "100%|██████████████████████████████| 360/360 [00:02<00:00, 175.46it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation set EER: 0.2774\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch: 31, cce:0.013: 100%|███████████| 51/51 [00:23<00:00,  2.16it/s]\n",
      "100%|██████████████████████████████| 360/360 [00:02<00:00, 177.62it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation set EER: 0.27831999999999996\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch: 32, cce:0.013: 100%|███████████| 51/51 [00:23<00:00,  2.16it/s]\n",
      "100%|██████████████████████████████| 360/360 [00:02<00:00, 175.19it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation set EER: 0.27831999999999996\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch: 33, cce:0.012: 100%|███████████| 51/51 [00:23<00:00,  2.15it/s]\n",
      "100%|██████████████████████████████| 360/360 [00:02<00:00, 177.38it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation set EER: 0.27831999999999996\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch: 34, cce:0.012: 100%|███████████| 51/51 [00:23<00:00,  2.16it/s]\n",
      "100%|██████████████████████████████| 360/360 [00:02<00:00, 170.64it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation set EER: 0.27827999999999997\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch: 35, cce:0.012: 100%|███████████| 51/51 [00:23<00:00,  2.16it/s]\n",
      "100%|██████████████████████████████| 360/360 [00:02<00:00, 174.44it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation set EER: 0.27827999999999997\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch: 36, cce:0.011: 100%|███████████| 51/51 [00:23<00:00,  2.15it/s]\n",
      "100%|██████████████████████████████| 360/360 [00:02<00:00, 172.46it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation set EER: 0.27827999999999997\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch: 37, cce:0.011: 100%|███████████| 51/51 [00:25<00:00,  2.00it/s]\n",
      "100%|██████████████████████████████| 360/360 [00:02<00:00, 151.96it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation set EER: 0.27827999999999997\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch: 38, cce:0.011: 100%|███████████| 51/51 [00:24<00:00,  2.11it/s]\n",
      "100%|██████████████████████████████| 360/360 [00:02<00:00, 177.84it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation set EER: 0.27827999999999997\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch: 39, cce:0.011: 100%|███████████| 51/51 [00:24<00:00,  2.12it/s]\n",
      "100%|██████████████████████████████| 360/360 [00:02<00:00, 175.91it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation set EER: 0.27827999999999997\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('venv_498': conda)"
  },
  "interpreter": {
   "hash": "67de50e9e1483ef4227ee85bea71885b09028997b2949254a8c2defc45b36da5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}